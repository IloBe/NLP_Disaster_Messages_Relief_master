{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.<br>\n",
    "\n",
    "My general notes:<br>\n",
    "Have in mind, that we work on a multi-class, multi-output text classification which assigns to each message sample a set of category target classes. The messages are short and an imbalanced data distribution exists. The dataset has 19634 data points with 40 different target categories.\n",
    "\n",
    "During the disaster messages processing, the English text is tokenized, lower cased, lemmatized and the contractions are expanded. Additionally, e.g. spaces, punctuation and English stop words are removed.\n",
    "\n",
    "\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ilona\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ilona\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ilona\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Using TensorFlow backend.\n",
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: UserWarning: once\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# import libraries\n",
    "#\n",
    "\n",
    "# download necessary NLTK data\n",
    "#%pip install nltk\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords'])\n",
    "\n",
    "import random as rn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import pickle\n",
    "from sqlalchemy import create_engine\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#%pip install bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sklearn.neighbors\n",
    "from sklearn.utils import resample\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "\n",
    "# from imblearn.combine import SMOTETomek  -  resampling not possible because of having a multi-class, multi-output task\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# warnings status to show\n",
    "import warnings\n",
    "warnings.warn(\"once\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the code reproducible ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_SEED = 42\n",
    "\n",
    "# The below is necessary for starting NumPy generated random numbers in a well-defined initial state.\n",
    "np.random.seed(FIXED_SEED)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers in a well-defined state.\n",
    "rn.seed(FIXED_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 19634 data points with 40 variables each.\n"
     ]
    }
   ],
   "source": [
    "# load data from database\n",
    "try:\n",
    "    engine = create_engine('sqlite:///Disaster_Messages_engine.db')\n",
    "    df = pd.read_sql_table('Messages_Categories_table', engine)\n",
    "    \n",
    "    # success\n",
    "    print(\"The dataset has {} data points with {} variables each.\".format(*df.shape))\n",
    "except:\n",
    "    print(\"The database 'Disaster_Messages_engine.db' could not be loaded. No ML pipeline activities possible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>lang_code</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Storm at sacred heart of jesus</td>\n",
       "      <td>Cyclone Coeur sacr de jesus</td>\n",
       "      <td>direct</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  \\\n",
       "0  Weather update - a cold front from Cuba that c...   \n",
       "1            Is the Hurricane over or is it not over   \n",
       "2  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "3  says: west side of Haiti, rest of the country ...   \n",
       "4                     Storm at sacred heart of jesus   \n",
       "\n",
       "                                            original   genre lang_code  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        en   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        en   \n",
       "2  UN reports Leogane 80-90 destroyed. Only Hospi...  direct        en   \n",
       "3  facade ouest d Haiti et le reste du pays aujou...  direct        en   \n",
       "4                        Cyclone Coeur sacr de jesus  direct        en   \n",
       "\n",
       "   related  request  offer  aid_related  medical_help  medical_products  ...  \\\n",
       "0        1        0      0            0             0                 0  ...   \n",
       "1        1        0      0            1             0                 0  ...   \n",
       "2        1        1      0            1             0                 1  ...   \n",
       "3        1        0      0            0             0                 0  ...   \n",
       "4        1        0      0            0             0                 0  ...   \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "1            0                     0                1       0      1     0   \n",
       "2            0                     0                0       0      0     0   \n",
       "3            0                     0                0       0      0     0   \n",
       "4            0                     0                1       0      1     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "1           0     0              0              0  \n",
       "2           0     0              0              0  \n",
       "3           0     0              0              0  \n",
       "4           0     0              0              0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input (X) and output (y) samples, we know that related is always one ...\n",
    "# as input we have to take care about the messages\n",
    "# the categories are the targets of the multi-class, multi-output classification\n",
    "X = df['message']\n",
    "y = df[df.columns[4:]]\n",
    "TARGET_NAMES = y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X datatype: <class 'pandas.core.series.Series'>\n",
      "y datatype: <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(\"X datatype: {}\".format(type(X)))\n",
    "print(\"y datatype: {}\".format(type(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Weather update - a cold front from Cuba that c...\n",
       "1              Is the Hurricane over or is it not over\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>security</th>\n",
       "      <th>military</th>\n",
       "      <th>child_alone</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   related  request  offer  aid_related  medical_help  medical_products  \\\n",
       "0        1        0      0            0             0                 0   \n",
       "1        1        0      0            1             0                 0   \n",
       "2        1        1      0            1             0                 1   \n",
       "3        1        0      0            0             0                 0   \n",
       "4        1        0      0            0             0                 0   \n",
       "\n",
       "   search_and_rescue  security  military  child_alone  ...  aid_centers  \\\n",
       "0                  0         0         0            0  ...            0   \n",
       "1                  0         0         0            0  ...            0   \n",
       "2                  0         0         0            0  ...            0   \n",
       "3                  0         0         0            0  ...            0   \n",
       "4                  0         0         0            0  ...            0   \n",
       "\n",
       "   other_infrastructure  weather_related  floods  storm  fire  earthquake  \\\n",
       "0                     0                0       0      0     0           0   \n",
       "1                     0                1       0      1     0           0   \n",
       "2                     0                0       0      0     0           0   \n",
       "3                     0                0       0      0     0           0   \n",
       "4                     0                1       0      1     0           0   \n",
       "\n",
       "   cold  other_weather  direct_report  \n",
       "0     0              0              0  \n",
       "1     0              0              0  \n",
       "2     0              0              0  \n",
       "3     0              0              0  \n",
       "4     0              0              0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.iloc[0:5,:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'related' includes 19634 x value 1.\n",
      "'request' includes 4374 x value 1.\n",
      "'offer' includes 117 x value 1.\n",
      "'aid_related' includes 10729 x value 1.\n",
      "'medical_help' includes 2066 x value 1.\n",
      "'medical_products' includes 1297 x value 1.\n",
      "'search_and_rescue' includes 718 x value 1.\n",
      "'security' includes 467 x value 1.\n",
      "'military' includes 857 x value 1.\n",
      "'child_alone' includes 19 x value 1.\n",
      "'water' includes 1650 x value 1.\n",
      "'food' includes 2885 x value 1.\n",
      "'shelter' includes 2281 x value 1.\n",
      "'clothing' includes 401 x value 1.\n",
      "'money' includes 598 x value 1.\n",
      "'missing_people' includes 297 x value 1.\n",
      "'refugees' includes 872 x value 1.\n",
      "'death' includes 1187 x value 1.\n",
      "'other_aid' includes 3392 x value 1.\n",
      "'infrastructure_related' includes 1688 x value 1.\n",
      "'transport' includes 1197 x value 1.\n",
      "'buildings' includes 1313 x value 1.\n",
      "'electricity' includes 528 x value 1.\n",
      "'tools' includes 158 x value 1.\n",
      "'hospitals' includes 283 x value 1.\n",
      "'shops' includes 118 x value 1.\n",
      "'aid_centers' includes 308 x value 1.\n",
      "'other_infrastructure' includes 1136 x value 1.\n",
      "'weather_related' includes 7212 x value 1.\n",
      "'floods' includes 2130 x value 1.\n",
      "'storm' includes 2420 x value 1.\n",
      "'fire' includes 282 x value 1.\n",
      "'earthquake' includes 2422 x value 1.\n",
      "'cold' includes 528 x value 1.\n",
      "'other_weather' includes 1366 x value 1.\n",
      "'direct_report' includes 4965 x value 1.\n"
     ]
    }
   ],
   "source": [
    "# for creation of train and test datasets it is important that no column includes only 0 values\n",
    "# stratification will not work properly (errors are thrown)\n",
    "for group in y.columns:\n",
    "    print(\"'{}' includes {} x value 1.\".format(group, y[group].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data\n",
    "\n",
    "During EPL pipeline activities we realised that there are messages which are not useful (e.g. 'nonsense' character sequences, html characters) and there are probably web links included. We have to deal with this in the tokenize() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function from Dipanjan's repository:\n",
    "# https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%\\\n",
    "# 20content/nlp%20proven%20approach/NLP%20Strategy%20I%20-%20Processing%20and%20Understanding%20Text.ipynb\n",
    "\n",
    "def expand_contractions(text, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    \n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.remove('no')\n",
    "stop_words.remove('not')\n",
    "\n",
    "def tokenize(text):\n",
    "    # have in mind that we use this for a web app adding new messages;\n",
    "    # if still html, xml or other undefined parts in the existing messages:\n",
    "    # first remove such metatext from English messages\n",
    "    # see: https://docs.python.org/3.7/library/codecs.html#encodings-and-unicode\n",
    "    # \"To be able to detect the endianness of a UTF-16 or UTF-32 byte sequence,\n",
    "    # there’s the so called BOM (“Byte Order Mark”). [...]\n",
    "    # In UTF-8, the use of the BOM is discouraged and should generally be avoided.\"\n",
    "    # specific ones are e.g. notepad signatures from Microsoft as part of the messages which should be avoided;\n",
    "    # other undefined characters have the coding of the 'replacement character' unicode u\"\\ufffd\"\n",
    "    soup = BeautifulSoup(text, 'html')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    \n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'       \n",
    "    detected_urls = re.findall(url_regex, bom_removed)\n",
    "    for url in detected_urls:\n",
    "        text = bom_removed.replace(url, \"urlplaceholder\")\n",
    "        \n",
    "    # change the negation wordings like don't to do not, won't to will not \n",
    "    # or other contractions like I'd to I would, I'll to I will etc. via dictionary\n",
    "    text = expand_contractions(text, CONTRACTION_MAP)\n",
    "\n",
    "    # remove punctuation [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]\n",
    "    text = text.translate(str.maketrans('','', string.punctuation))\n",
    "    # remove numbers\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # during ETL pipeline we have reduced the dataset on English messages ('en' language coding,\n",
    "    # but there can be some wrong codings\n",
    "    tokens = word_tokenize(letters_only, language='english')\n",
    "    lemmatizer = WordNetLemmatizer()  # for the lexical correctly found word stem (root)\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        # use only lower cases, remove leading and ending spaces\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        # remember: there have been nonsense sentences, so, now some strings could be empty\n",
    "        # toDo: what is the correct length number to use now? Small ones are probably no relevant words ...\n",
    "        # remove English stop words\n",
    "        if (len(clean_tok) > 1) & (clean_tok not in stop_words):\n",
    "            clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of string with some punctuation signs\n"
     ]
    }
   ],
   "source": [
    "# example for unit test to remove punctuation [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]\n",
    "example_str = 'This [is an] example? {of} string. with.? some &punctuation &signs!!??!!'\n",
    "result = example_str.translate(str.maketrans('','', string.punctuation))\n",
    "print(result)\n",
    "# output shall be: This is an example of string with some punctuation signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather update - a cold front from Cuba that could pass over Haiti\n",
      "['weather', 'update', 'cold', 'front', 'cuba', 'could', 'pas', 'haiti'] \n",
      "\n",
      "Is the Hurricane over or is it not over\n",
      "['hurricane', 'not'] \n",
      "\n",
      "UN reports Leogane 80-90 destroyed. Only Hospital St. Croix functioning. Needs supplies desperately.\n",
      "['un', 'report', 'leogane', 'destroyed', 'hospital', 'st', 'croix', 'functioning', 'needs', 'supply', 'desperately'] \n",
      "\n",
      "says: west side of Haiti, rest of the country today and tonight\n",
      "['say', 'west', 'side', 'haiti', 'rest', 'country', 'today', 'tonight'] \n",
      "\n",
      "Storm at sacred heart of jesus\n",
      "['storm', 'sacred', 'heart', 'jesus'] \n",
      "\n",
      "Please, we need tents and water. We are in Silo, Thank you!\n",
      "['please', 'need', 'tent', 'water', 'silo', 'thank'] \n",
      "\n",
      "I am in Croix-des-Bouquets. We have health issues. They ( workers ) are in Santo 15. ( an area in Croix-des-Bouquets )\n",
      "['croixdesbouquets', 'health', 'issue', 'worker', 'santo', 'area', 'croixdesbouquets'] \n",
      "\n",
      "There's nothing to eat and water, we starving and thirsty.\n",
      "['nothing', 'eat', 'water', 'starving', 'thirsty'] \n",
      "\n",
      "I am in Thomassin number 32, in the area named Pyron. I would like to have some water. Thank God we are fine, but we desperately need water. Thanks\n",
      "['thomassin', 'number', 'area', 'named', 'pyron', 'would', 'like', 'water', 'thank', 'god', 'fine', 'desperately', 'need', 'water', 'thanks'] \n",
      "\n",
      "Let's do it together, need food in Delma 75, in didine area\n",
      "['let', 'together', 'need', 'food', 'delma', 'didine', 'area'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test tokenize\n",
    "for message in X[:10]:\n",
    "    tokens = tokenize(message)\n",
    "    print(message)\n",
    "    print(tokens, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "Notes:\n",
    "- Regarding the class default parameters, for this Python implementation scikit-learn version 0.21.2 anbd scikit-multilearn version 0.2.0 are used.\n",
    "- We use np.random.seed() too beside of random_state/random_seed parameters ([reason](https://stackoverflow.com/questions/47923258/random-seed-on-svm-sklearn-produces-different-results))\n",
    "- For the pipeline workflow a `FeatureUnion`instance concatenates results of multiple transformer objects\n",
    "\n",
    "Remember, we are dealing with an imbalanced dataset, therefore not all models can be used. One machine learning classifier could be more biased towards the majority class, causing bad classification of the minority class compared to other model types. Therefore we have to take care and to evaluate some of them.\n",
    "\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other remaining target categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables.\n",
    "\n",
    "According scikit-learn [documentation](https://scikit-learn.org/stable/modules/multiclass.html) we can choose only specific classifier using this meta-estimator. We start with `RandomForestClassier`.<br>\n",
    "Its default parameter values are:<br>\n",
    "<i>RandomForestClassifier</i>(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None).\n",
    "\n",
    "For our classifiation task, most important parameters are <i>n_estimators</i> and <i>max_features</i>. As stated in the scikit-learn documentation \"using a random subset of size sqrt(n_features)) for classification tasks (where n_features is the number of features in the data)\" is in general the best for the prediction results. This is the case with max_features='auto', therefore, we will not change this parameter.\n",
    "\n",
    "<i>n_jos=1</i> is used because all other values throw errors and the training task crashed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            \n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize, ngram_range=(1,2))),\n",
    "                ('tfidf', TfidfTransformer(sublinear_tf=True)),\n",
    "            ]))\n",
    "            \n",
    "        ])),\n",
    "    \n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier(n_estimators=100, class_weight='balanced',\n",
    "                                                             n_jobs=1, random_state=FIXED_SEED)))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle is by default set on True,\n",
    "# usage of stratify param leads to stratify split technique for this imbalanced dataset,\n",
    "# having both would be a StratifiedShuffleSplit algorithm in the background,\n",
    "# but\n",
    "# stratify=y leads to a ValueError: The least populated class in y has only 1 member, which is too few.\n",
    "# The minimum number of groups for any class cannot be less than 2.\n",
    "# ToDo: clarify why => solution, must be: stratify=y.iloc[:,:] but that throws errors;\n",
    "# wrong coding with y.iloc[:,1] for getting the rest to run (wrong results with and after training)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, stratify=y.iloc[:,1],\n",
    "#                                                    test_size=0.2, random_state=FIXED_SEED)\n",
    "\n",
    "# therefore: creation of X and y with scikit-multilearn iterative stratifier,\n",
    "# works only because 'child_alone' target class has been mapped to some messages\n",
    "# if this would be still 0 on all rows ValueError would be thrown\n",
    "test_size = 0.2\n",
    "stratifier = IterativeStratification(n_splits=2, order=1,\n",
    "                                     sample_distribution_per_fold=[test_size, 1.0-test_size],\n",
    "                                     random_state=FIXED_SEED)\n",
    "train_indexes, test_indexes = next(stratifier.split(X, y))\n",
    "\n",
    "# y slicing with iloc because y is a dataframe, X is a series;\n",
    "# by adding values to X and y we create numpy arrays\n",
    "X_train, y_train = X[train_indexes].values, y.iloc[train_indexes, :].values  \n",
    "X_test, y_test = X[test_indexes].values, y.iloc[test_indexes, :].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15707,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15707, 36)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train datatype: <class 'numpy.ndarray'>\n",
      "y_train datatype: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train datatype: {}\".format(type(X_train)))\n",
    "print(\"y_train datatype: {}\".format(type(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. numpy.ndarray element is: [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0]\n",
      "{0, 1}\n",
      "1. numpy.ndarray element is: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "2. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "3. numpy.ndarray element is: [1 1 0 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "4. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "5. numpy.ndarray element is: [1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "6. numpy.ndarray element is: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0]\n",
      "{0, 1}\n",
      "7. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "8. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "9. numpy.ndarray element is: [1 1 0 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "10. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "11. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "12. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "13. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "14. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "15. numpy.ndarray element is: [1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "16. numpy.ndarray element is: [1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0]\n",
      "{0, 1}\n",
      "17. numpy.ndarray element is: [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "18. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "19. numpy.ndarray element is: [1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "20. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "21. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "22. numpy.ndarray element is: [1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0]\n",
      "{0, 1}\n",
      "23. numpy.ndarray element is: [1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "24. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "25. numpy.ndarray element is: [1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "26. numpy.ndarray element is: [1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "27. numpy.ndarray element is: [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "28. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "29. numpy.ndarray element is: [1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0]\n",
      "{0, 1}\n",
      "30. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "31. numpy.ndarray element is: [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "32. numpy.ndarray element is: [1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "33. numpy.ndarray element is: [1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1]\n",
      "{0, 1}\n",
      "34. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "35. numpy.ndarray element is: [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "for i in range(y_train.shape[1]):\n",
    "    print(\"{}. numpy.ndarray element is: {}\".format(i, y_train[i]))\n",
    "    print(set(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**<br>\n",
    "As we already know, the dataset is an imbalanced one, which will lead to emphasize the majority target classes too much. We want to get a more balanced dataset distribution by duplicating minority class instances of the training set. With this **oversampling** approach some overfitting may appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['related', 'request', 'offer', 'aid_related', 'medical_help',\n",
       "       'medical_products', 'search_and_rescue', 'security', 'military',\n",
       "       'child_alone', 'water', 'food', 'shelter', 'clothing', 'money',\n",
       "       'missing_people', 'refugees', 'death', 'other_aid',\n",
       "       'infrastructure_related', 'transport', 'buildings', 'electricity',\n",
       "       'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure',\n",
       "       'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold',\n",
       "       'other_weather', 'direct_report'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before resampling, shape of X_train: (15707,)\n",
      "Before resampling, shape of y_train: (15707, 36) \n",
      "\n",
      "Before resampling, label counts '1': [15707  2564    95  8493  1775  1055   595   400   825    10  1229  1994\n",
      "  1727   268   517   240   778  1028  2596  1516  1084  1129   461   136\n",
      "   247    99   270  1043  6122  2013  2186   255  1727   490  1251  3152]\n",
      "Before resampling, label counts '0': [    0 13143 15612  7214 13932 14652 15112 15307 14882 15697 14478 13713\n",
      " 13980 15439 15190 15467 14929 14679 13111 14191 14623 14578 15246 15571\n",
      " 15460 15608 15437 14664  9585 13694 13521 15452 13980 15217 14456 12555] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# datayptes are class 'numpy.ndarray'\n",
    "\n",
    "print('Before resampling, shape of X_train: {}'.format(X_train.shape))\n",
    "print('Before resampling, shape of y_train: {} \\n'.format(y_train.shape))\n",
    "\n",
    "print(\"Before resampling, label counts '1': {}\".format(sum(y_train==1)))\n",
    "print(\"Before resampling, label counts '0': {} \\n\".format(sum(y_train==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampling with scikit-learn utils package\n",
    "X_train_res, y_train_res = resample(X_train, y_train, n_samples=7000, random_state=FIXED_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After resampling, shape of X_train_res: (7000,)\n",
      "After resampling, shape of y_train_res: (7000, 36) \n",
      "\n",
      "After resampling, label counts '1': [7000 1137   36 3860  812  490  304  177  353    2  555  940  791  118\n",
      "  249  130  361  468 1180  653  489  539  225   62  103   35  128  456\n",
      " 2670  875  941  116  769  199  568 1487]\n",
      "After resampling, label counts '0': [   0 5863 6964 3140 6188 6510 6696 6823 6647 6998 6445 6060 6209 6882\n",
      " 6751 6870 6639 6532 5820 6347 6511 6461 6775 6938 6897 6965 6872 6544\n",
      " 4330 6125 6059 6884 6231 6801 6432 5513]\n"
     ]
    }
   ],
   "source": [
    "print('After resampling, shape of X_train_res: {}'.format(X_train_res.shape))\n",
    "print('After resampling, shape of y_train_res: {} \\n'.format(y_train_res.shape))\n",
    "\n",
    "print(\"After resampling, label counts '1': {}\".format(sum(y_train_res==1)))\n",
    "print(\"After resampling, label counts '0': {}\".format(sum(y_train_res==0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train the pipeline, first with the original training set afterwards with the resampled one ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('features',\n",
       "                 FeatureUnion(n_jobs=None,\n",
       "                              transformer_list=[('text_pipeline',\n",
       "                                                 Pipeline(memory=None,\n",
       "                                                          steps=[('vect',\n",
       "                                                                  CountVectorizer(analyzer='word',\n",
       "                                                                                  binary=False,\n",
       "                                                                                  decode_error='strict',\n",
       "                                                                                  dtype=<class 'numpy.int64'>,\n",
       "                                                                                  encoding='utf-8',\n",
       "                                                                                  input='content',\n",
       "                                                                                  lowercase=True,\n",
       "                                                                                  max_df=1.0,\n",
       "                                                                                  max_features=None,\n",
       "                                                                                  min_df=1,\n",
       "                                                                                  ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  preprocessor=Non...\n",
       "                 MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                                        class_weight='balanced',\n",
       "                                                                        criterion='gini',\n",
       "                                                                        max_depth=None,\n",
       "                                                                        max_features='auto',\n",
       "                                                                        max_leaf_nodes=None,\n",
       "                                                                        min_impurity_decrease=0.0,\n",
       "                                                                        min_impurity_split=None,\n",
       "                                                                        min_samples_leaf=1,\n",
       "                                                                        min_samples_split=2,\n",
       "                                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                                        n_estimators=100,\n",
       "                                                                        n_jobs=1,\n",
       "                                                                        oob_score=False,\n",
       "                                                                        random_state=42,\n",
       "                                                                        verbose=0,\n",
       "                                                                        warm_start=False),\n",
       "                                       n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And calculate the model prediction for our original training and testing data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rfc_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do the same thing with the resampled dataset ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('features',\n",
       "                 FeatureUnion(n_jobs=None,\n",
       "                              transformer_list=[('text_pipeline',\n",
       "                                                 Pipeline(memory=None,\n",
       "                                                          steps=[('vect',\n",
       "                                                                  CountVectorizer(analyzer='word',\n",
       "                                                                                  binary=False,\n",
       "                                                                                  decode_error='strict',\n",
       "                                                                                  dtype=<class 'numpy.int64'>,\n",
       "                                                                                  encoding='utf-8',\n",
       "                                                                                  input='content',\n",
       "                                                                                  lowercase=True,\n",
       "                                                                                  max_df=1.0,\n",
       "                                                                                  max_features=None,\n",
       "                                                                                  min_df=1,\n",
       "                                                                                  ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  preprocessor=Non...\n",
       "                 MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                                        class_weight='balanced',\n",
       "                                                                        criterion='gini',\n",
       "                                                                        max_depth=None,\n",
       "                                                                        max_features='auto',\n",
       "                                                                        max_leaf_nodes=None,\n",
       "                                                                        min_impurity_decrease=0.0,\n",
       "                                                                        min_impurity_split=None,\n",
       "                                                                        min_samples_leaf=1,\n",
       "                                                                        min_samples_split=2,\n",
       "                                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                                        n_estimators=100,\n",
       "                                                                        n_jobs=1,\n",
       "                                                                        oob_score=False,\n",
       "                                                                        random_state=42,\n",
       "                                                                        verbose=0,\n",
       "                                                                        warm_start=False),\n",
       "                                       n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rfc_pred_res = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "For evaluation:<br>\n",
    "Report accuracy score, f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each, where:\n",
    "\n",
    "TP = TruePositive; FP = FalsePositive; TN = TrueNegative; FN = FalseNegative.\n",
    "\n",
    "**Accuracy Score** is a classification score. It is the number of correct predictions made divided by the total number of predictions made. In a multilabel classification task it computes subset accuracy. \n",
    "  \n",
    "Furthermore, beside accuracy, we add additional metrics to compare the model performance having an originally imbalanced dataset. Accuracy would focus too much on the majority classes. Because of this overfitting of the majority classes, its value would be too good and therefore misleading.\n",
    "\n",
    "**Precision** quantifies the binary precision. In other words, a measure of a classifiers exactness. It is a ratio of true positives (messages correctly classified to their categories)) to all positives (all messages classified to categories, irrespective of whether that was the correct classification), in other words it is the ratio of\n",
    "\n",
    "TP / (TP + FP)\n",
    "\n",
    "**Recall** tells us what proportion of messages that actually were classified to specific categories were classified by us as this categories. Means, a measure of a classifiers completeness. It is a ratio of true positives to all the correctly category classified messages that were actually disaster messages, in other words it is the ratio of\n",
    "\n",
    "TP / (TP + FN)\n",
    "\n",
    "A model's ability to precisely predict those that are correctly categoriesed disaster messages is more important than the model's ability to recall those individuals. \n",
    "\n",
    "We can use **F-beta score** as a metric that considers both precision and recall. According scikit-learn, the F-beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0. F – Measure is nothing but the harmonic mean of Precision and Recall.\n",
    "\n",
    "Fβ=(1 + β2)  (precision⋅recall / ((β2⋅precision) + recall))\n",
    "\n",
    "In particular, when β=0.5, more emphasis is placed on precision. And when β=1.0 recall and precision are equally important.\n",
    "\n",
    "According scikit-learn: \"The **F1 score** ... reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "In the multi-class and multi-label case, this is the average of the F1 score of each class with weighting depending on the average parameter.\"\n",
    "\n",
    "From scikit-learn documentation for the classification report:<br>\n",
    "The classification_report() function returns an additional value: **Support** - the number of occurrences of each label in y_true.<br>\n",
    "The reported averages include macro average (averaging the unweighted mean per label), weighted average (averaging the support-weighted mean per label), sample average (only for multilabel classification) and micro average (averaging the total true positives, false negatives and false positives) it is only shown for multi-label or multi-class with a subset of classes because it is accuracy otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(target_names, y_test, y_pred, cv=None, parameters=None):\n",
    "   \n",
    "    # text summary of the overall accuracy, precision, recall, F1 score for each class   \n",
    "    print(\"\\nFirst: overall accuracy score: {:5f}\".format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "    # shows F1_score, precision and recall\n",
    "    class_report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "    print(\"Classification Report for each target class:\\n\", class_report)\n",
    "    \n",
    "    if cv != None:\n",
    "        print(\"\\n\\n---- Best Parameters: ----\\n\")\n",
    "        print(\"Best score: {:3f}\".format(cv.best_score_))\n",
    "        print(\"Best estimators parameters set:\")\n",
    "        best_parameters = cv.best_estimator_.get_params()\n",
    "        for param_name in sorted(parameters.keys()):\n",
    "            print(\"\\t {}: {}\".format(param_name, best_parameters[param_name]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the metric results for our original data without resampling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First: overall accuracy score: 0.065190\n",
      "Classification Report for each target class:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "               related       1.00      1.00      1.00      3927\n",
      "               request       0.56      0.40      0.46      1810\n",
      "                 offer       0.00      0.00      0.00        22\n",
      "           aid_related       0.59      0.71      0.64      2236\n",
      "          medical_help       0.07      0.00      0.01       291\n",
      "      medical_products       0.20      0.01      0.02       242\n",
      "     search_and_rescue       0.00      0.00      0.00       123\n",
      "              security       0.00      0.00      0.00        67\n",
      "              military       0.00      0.00      0.00        32\n",
      "           child_alone       0.00      0.00      0.00         9\n",
      "                 water       0.25      0.01      0.01       421\n",
      "                  food       0.31      0.02      0.03       891\n",
      "               shelter       0.06      0.00      0.01       554\n",
      "              clothing       0.29      0.02      0.03       133\n",
      "                 money       0.14      0.01      0.02        81\n",
      "        missing_people       0.00      0.00      0.00        57\n",
      "              refugees       0.00      0.00      0.00        94\n",
      "                 death       0.00      0.00      0.00       159\n",
      "             other_aid       0.16      0.01      0.02       796\n",
      "infrastructure_related       0.00      0.00      0.00       172\n",
      "             transport       0.00      0.00      0.00       113\n",
      "             buildings       0.00      0.00      0.00       184\n",
      "           electricity       0.00      0.00      0.00        67\n",
      "                 tools       0.00      0.00      0.00        22\n",
      "             hospitals       0.00      0.00      0.00        36\n",
      "                 shops       0.00      0.00      0.00        19\n",
      "           aid_centers       0.00      0.00      0.00        38\n",
      "  other_infrastructure       0.00      0.00      0.00        93\n",
      "       weather_related       0.58      0.26      0.36      1090\n",
      "                floods       0.00      0.00      0.00       117\n",
      "                 storm       0.21      0.03      0.05       234\n",
      "                  fire       0.00      0.00      0.00        27\n",
      "            earthquake       0.77      0.26      0.39       695\n",
      "                  cold       0.00      0.00      0.00        38\n",
      "         other_weather       0.10      0.01      0.02       115\n",
      "         direct_report       0.54      0.40      0.46      1813\n",
      "\n",
      "             micro avg       0.73      0.44      0.55     16818\n",
      "             macro avg       0.16      0.09      0.10     16818\n",
      "          weighted avg       0.54      0.44      0.46     16818\n",
      "           samples avg       0.78      0.59      0.57     16818\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "display_results(TARGET_NAMES, y_test, y_rfc_pred, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such kind of behaviour has been expected because having an imbalanced dataset and in the output vectors for each message, most of the target label values are set to 0 - only few are set to 1. So, the vector is not a dense one.<br>\n",
    "The accuracy metric is not an appropriate measure to evaluate model performance of such kind of dataset. It could classify all instances as part of the majority class and classifies the minority class targets as noise. It is not able to evaluate the model performance of a multi-class dataset with multi-output vectors.<br>\n",
    "Additionally in this classification report, often the metrics are not reliable because of being set to 0.0 according calculation rules. If values are available, precision is often higher than recall, in other words, we have a high rate of false negatives (all items wrongly classified as not being part of the specific target classes). A hugh amount of the token inputs are noise features, not associated with the target response class features.<br>\n",
    "Mainly for support values >1000 appropriate F1-score values exists (except earthquake, score >10%). This appeared for the following target features: request, aid_related, wheather related, earthquake and direct_report.<br>\n",
    "And as we know from the ETL pipeline, some target features are correlated.\n",
    "\n",
    "In other words, we start to improve the model by using cross-validated hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the metric results for our resampled data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First: overall accuracy score: 0.050675\n",
      "Classification Report for each target class:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "               related       1.00      1.00      1.00      3927\n",
      "               request       0.57      0.36      0.44      1810\n",
      "                 offer       0.00      0.00      0.00        22\n",
      "           aid_related       0.58      0.77      0.66      2236\n",
      "          medical_help       0.33      0.00      0.01       291\n",
      "      medical_products       0.00      0.00      0.00       242\n",
      "     search_and_rescue       0.00      0.00      0.00       123\n",
      "              security       0.00      0.00      0.00        67\n",
      "              military       0.00      0.00      0.00        32\n",
      "           child_alone       0.00      0.00      0.00         9\n",
      "                 water       0.30      0.01      0.01       421\n",
      "                  food       0.26      0.01      0.02       891\n",
      "               shelter       0.40      0.00      0.01       554\n",
      "              clothing       0.00      0.00      0.00       133\n",
      "                 money       0.00      0.00      0.00        81\n",
      "        missing_people       0.00      0.00      0.00        57\n",
      "              refugees       0.00      0.00      0.00        94\n",
      "                 death       0.00      0.00      0.00       159\n",
      "             other_aid       0.23      0.01      0.03       796\n",
      "infrastructure_related       0.00      0.00      0.00       172\n",
      "             transport       0.00      0.00      0.00       113\n",
      "             buildings       0.00      0.00      0.00       184\n",
      "           electricity       0.00      0.00      0.00        67\n",
      "                 tools       0.00      0.00      0.00        22\n",
      "             hospitals       0.00      0.00      0.00        36\n",
      "                 shops       0.00      0.00      0.00        19\n",
      "           aid_centers       0.00      0.00      0.00        38\n",
      "  other_infrastructure       0.00      0.00      0.00        93\n",
      "       weather_related       0.62      0.25      0.35      1090\n",
      "                floods       0.00      0.00      0.00       117\n",
      "                 storm       0.20      0.00      0.01       234\n",
      "                  fire       0.00      0.00      0.00        27\n",
      "            earthquake       0.79      0.27      0.41       695\n",
      "                  cold       0.00      0.00      0.00        38\n",
      "         other_weather       0.00      0.00      0.00       115\n",
      "         direct_report       0.52      0.43      0.47      1813\n",
      "\n",
      "             micro avg       0.73      0.45      0.56     16818\n",
      "             macro avg       0.16      0.09      0.09     16818\n",
      "          weighted avg       0.55      0.45      0.46     16818\n",
      "           samples avg       0.77      0.59      0.57     16818\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "display_results(TARGET_NAMES, y_test, y_rfc_pred_res, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the F1 score values for each class of both trained models leads to the conclusion that for this dataset the calculated oversampling is no improvement. After having done the resampling the label counts 0 and 1 for each target class still looks being imbalanced and there are still target features having a very low score.\n",
    "\n",
    "The idea behind resampling was, that a hybrid method of doing resampling first and then using an ensemble classification model, would be less prone to imbalanced data and would lead to better prediction results. So, this one oversampling calculation is not good, but there are better resampling methods which are possible to get the desired result. We are using some in the next chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "We use grid search to find better parameters for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None, 'steps': [('features', FeatureUnion(n_jobs=None,\n",
       "                transformer_list=[('text_pipeline',\n",
       "                                   Pipeline(memory=None,\n",
       "                                            steps=[('vect',\n",
       "                                                    CountVectorizer(analyzer='word',\n",
       "                                                                    binary=False,\n",
       "                                                                    decode_error='strict',\n",
       "                                                                    dtype=<class 'numpy.int64'>,\n",
       "                                                                    encoding='utf-8',\n",
       "                                                                    input='content',\n",
       "                                                                    lowercase=True,\n",
       "                                                                    max_df=1.0,\n",
       "                                                                    max_features=None,\n",
       "                                                                    min_df=1,\n",
       "                                                                    ngram_range=(1,\n",
       "                                                                                 2),\n",
       "                                                                    preprocessor=None,\n",
       "                                                                    stop_words=None,\n",
       "                                                                    strip_accents=None,\n",
       "                                                                    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                                    tokenizer=<function tokenize at 0x000000789273AC80>,\n",
       "                                                                    vocabulary=None)),\n",
       "                                                   ('tfidf',\n",
       "                                                    TfidfTransformer(norm='l2',\n",
       "                                                                     smooth_idf=True,\n",
       "                                                                     sublinear_tf=True,\n",
       "                                                                     use_idf=True))],\n",
       "                                            verbose=False))],\n",
       "                transformer_weights=None, verbose=False)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                          class_weight='balanced',\n",
       "                                                          criterion='gini',\n",
       "                                                          max_depth=None,\n",
       "                                                          max_features='auto',\n",
       "                                                          max_leaf_nodes=None,\n",
       "                                                          min_impurity_decrease=0.0,\n",
       "                                                          min_impurity_split=None,\n",
       "                                                          min_samples_leaf=1,\n",
       "                                                          min_samples_split=2,\n",
       "                                                          min_weight_fraction_leaf=0.0,\n",
       "                                                          n_estimators=100,\n",
       "                                                          n_jobs=1,\n",
       "                                                          oob_score=False,\n",
       "                                                          random_state=42,\n",
       "                                                          verbose=0,\n",
       "                                                          warm_start=False),\n",
       "                         n_jobs=None))], 'verbose': False, 'features': FeatureUnion(n_jobs=None,\n",
       "              transformer_list=[('text_pipeline',\n",
       "                                 Pipeline(memory=None,\n",
       "                                          steps=[('vect',\n",
       "                                                  CountVectorizer(analyzer='word',\n",
       "                                                                  binary=False,\n",
       "                                                                  decode_error='strict',\n",
       "                                                                  dtype=<class 'numpy.int64'>,\n",
       "                                                                  encoding='utf-8',\n",
       "                                                                  input='content',\n",
       "                                                                  lowercase=True,\n",
       "                                                                  max_df=1.0,\n",
       "                                                                  max_features=None,\n",
       "                                                                  min_df=1,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               2),\n",
       "                                                                  preprocessor=None,\n",
       "                                                                  stop_words=None,\n",
       "                                                                  strip_accents=None,\n",
       "                                                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                                  tokenizer=<function tokenize at 0x000000789273AC80>,\n",
       "                                                                  vocabulary=None)),\n",
       "                                                 ('tfidf',\n",
       "                                                  TfidfTransformer(norm='l2',\n",
       "                                                                   smooth_idf=True,\n",
       "                                                                   sublinear_tf=True,\n",
       "                                                                   use_idf=True))],\n",
       "                                          verbose=False))],\n",
       "              transformer_weights=None, verbose=False), 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                        class_weight='balanced',\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=None,\n",
       "                                                        max_features='auto',\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        n_estimators=100,\n",
       "                                                        n_jobs=1,\n",
       "                                                        oob_score=False,\n",
       "                                                        random_state=42,\n",
       "                                                        verbose=0,\n",
       "                                                        warm_start=False),\n",
       "                       n_jobs=None), 'features__n_jobs': None, 'features__transformer_list': [('text_pipeline',\n",
       "   Pipeline(memory=None,\n",
       "            steps=[('vect',\n",
       "                    CountVectorizer(analyzer='word', binary=False,\n",
       "                                    decode_error='strict',\n",
       "                                    dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                    input='content', lowercase=True, max_df=1.0,\n",
       "                                    max_features=None, min_df=1,\n",
       "                                    ngram_range=(1, 2), preprocessor=None,\n",
       "                                    stop_words=None, strip_accents=None,\n",
       "                                    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                    tokenizer=<function tokenize at 0x000000789273AC80>,\n",
       "                                    vocabulary=None)),\n",
       "                   ('tfidf',\n",
       "                    TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=True,\n",
       "                                     use_idf=True))],\n",
       "            verbose=False))], 'features__transformer_weights': None, 'features__verbose': False, 'features__text_pipeline': Pipeline(memory=None,\n",
       "          steps=[('vect',\n",
       "                  CountVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                  input='content', lowercase=True, max_df=1.0,\n",
       "                                  max_features=None, min_df=1,\n",
       "                                  ngram_range=(1, 2), preprocessor=None,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=<function tokenize at 0x000000789273AC80>,\n",
       "                                  vocabulary=None)),\n",
       "                 ('tfidf',\n",
       "                  TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=True,\n",
       "                                   use_idf=True))],\n",
       "          verbose=False), 'features__text_pipeline__memory': None, 'features__text_pipeline__steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function tokenize at 0x000000789273AC80>,\n",
       "                   vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=True, use_idf=True))], 'features__text_pipeline__verbose': False, 'features__text_pipeline__vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function tokenize at 0x000000789273AC80>,\n",
       "                 vocabulary=None), 'features__text_pipeline__tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=True, use_idf=True), 'features__text_pipeline__vect__analyzer': 'word', 'features__text_pipeline__vect__binary': False, 'features__text_pipeline__vect__decode_error': 'strict', 'features__text_pipeline__vect__dtype': numpy.int64, 'features__text_pipeline__vect__encoding': 'utf-8', 'features__text_pipeline__vect__input': 'content', 'features__text_pipeline__vect__lowercase': True, 'features__text_pipeline__vect__max_df': 1.0, 'features__text_pipeline__vect__max_features': None, 'features__text_pipeline__vect__min_df': 1, 'features__text_pipeline__vect__ngram_range': (1,\n",
       "  2), 'features__text_pipeline__vect__preprocessor': None, 'features__text_pipeline__vect__stop_words': None, 'features__text_pipeline__vect__strip_accents': None, 'features__text_pipeline__vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'features__text_pipeline__vect__tokenizer': <function __main__.tokenize(text)>, 'features__text_pipeline__vect__vocabulary': None, 'features__text_pipeline__tfidf__norm': 'l2', 'features__text_pipeline__tfidf__smooth_idf': True, 'features__text_pipeline__tfidf__sublinear_tf': True, 'features__text_pipeline__tfidf__use_idf': True, 'clf__estimator__bootstrap': True, 'clf__estimator__class_weight': 'balanced', 'clf__estimator__criterion': 'gini', 'clf__estimator__max_depth': None, 'clf__estimator__max_features': 'auto', 'clf__estimator__max_leaf_nodes': None, 'clf__estimator__min_impurity_decrease': 0.0, 'clf__estimator__min_impurity_split': None, 'clf__estimator__min_samples_leaf': 1, 'clf__estimator__min_samples_split': 2, 'clf__estimator__min_weight_fraction_leaf': 0.0, 'clf__estimator__n_estimators': 100, 'clf__estimator__n_jobs': 1, 'clf__estimator__oob_score': False, 'clf__estimator__random_state': 42, 'clf__estimator__verbose': 0, 'clf__estimator__warm_start': False, 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=100, n_jobs=1, oob_score=False,\n",
       "                        random_state=42, verbose=0, warm_start=False), 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters for grid search\n",
    "rfc_param_grid = {\n",
    "    'features__text_pipeline__vect__ngram_range': [(1,2), (1,3)],\n",
    "    'clf__estimator__n_estimators': [200, 500, 1000],\n",
    "    'clf__estimator__max_depth': [10, 20],\n",
    "    'clf__estimator__class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# create grid search object\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV\n",
    "# cv not higher than 5 buckets, training needs days with cv=10 if e.g. amazon AWS EC2 service is not available\n",
    "# n_jobs set to 1 because cloud service throws TerminatedWorkerError if > 1\n",
    "# for scoring and refit see: https://stackoverflow.com/questions/57591311/combination-of-gridsearchcvs-refit-and-scorer-unclear\n",
    "#scoring = {'f1': make_scorer(f1_score, average=\"samples\"), 'Accuracy': make_scorer(accuracy_score)}\n",
    "grid_cv = GridSearchCV(pipeline, param_grid=rfc_param_grid, n_jobs=-1, cv=5,\n",
    "                       return_train_score=True, verbose=2)# scoring = scoring, refit='f1', return_train_score=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, recall and F-score of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 122.3min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 331.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('features',\n",
       "                                        FeatureUnion(n_jobs=None,\n",
       "                                                     transformer_list=[('text_pipeline',\n",
       "                                                                        Pipeline(memory=None,\n",
       "                                                                                 steps=[('vect',\n",
       "                                                                                         CountVectorizer(analyzer='word',\n",
       "                                                                                                         binary=False,\n",
       "                                                                                                         decode_error='strict',\n",
       "                                                                                                         dtype=<class 'numpy.int64'>,\n",
       "                                                                                                         encoding='utf-8',\n",
       "                                                                                                         input='content',\n",
       "                                                                                                         lowercase=True,\n",
       "                                                                                                         max_df=1.0,\n",
       "                                                                                                         m...\n",
       "                                                                                               verbose=0,\n",
       "                                                                                               warm_start=False),\n",
       "                                                              n_jobs=None))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'clf__estimator__class_weight': ['balanced'],\n",
       "                         'clf__estimator__max_depth': [10, 20],\n",
       "                         'clf__estimator__n_estimators': [200, 500, 1000],\n",
       "                         'features__text_pipeline__vect__ngram_range': [(1, 2),\n",
       "                                                                        (1,\n",
       "                                                                         3)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = cv\n",
    "grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 1],\n",
       "       [1, 1, 0, ..., 0, 0, 1],\n",
       "       [1, 0, 0, ..., 0, 0, 1],\n",
       "       ...,\n",
       "       [1, 1, 0, ..., 0, 0, 1],\n",
       "       [1, 0, 0, ..., 0, 0, 1],\n",
       "       [1, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_rfc_pred2 = grid_cv.predict(X_test)\n",
    "y_rfc_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV results:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mean_fit_time',\n",
       " 'mean_score_time',\n",
       " 'mean_test_score',\n",
       " 'mean_train_score',\n",
       " 'param_clf__estimator__class_weight',\n",
       " 'param_clf__estimator__max_depth',\n",
       " 'param_clf__estimator__n_estimators',\n",
       " 'param_features__text_pipeline__vect__ngram_range',\n",
       " 'params',\n",
       " 'rank_test_score',\n",
       " 'split0_test_score',\n",
       " 'split0_train_score',\n",
       " 'split1_test_score',\n",
       " 'split1_train_score',\n",
       " 'split2_test_score',\n",
       " 'split2_train_score',\n",
       " 'split3_test_score',\n",
       " 'split3_train_score',\n",
       " 'split4_test_score',\n",
       " 'split4_train_score',\n",
       " 'std_fit_time',\n",
       " 'std_score_time',\n",
       " 'std_test_score',\n",
       " 'std_train_score']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"CV results:\")\n",
    "sorted(grid_cv.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_fit_time = [ 143.32499766  222.84364619  420.73699727  584.24256101  832.77612128\n",
      " 1144.81913681  420.15419774  548.97682629  880.09615598 1191.90661998\n",
      " 1714.74533973 2363.58026776] \n",
      "\n",
      "std_fit_time = [  8.79552331  29.07888301  28.34485122  45.49333393  44.03554115\n",
      "  80.26441434  20.05158935  27.73019138  32.80056408  78.54556821\n",
      "  87.63899992 135.97262028] \n",
      "\n",
      "mean_score_time = [ 22.0082262   44.34183908  54.73562617 101.05241728 106.72635608\n",
      " 190.67582159  30.54257665  44.96658883  57.79411554  99.36488132\n",
      " 102.66963129 193.37914872] \n",
      "\n",
      "std_score_time = [ 2.27988077  3.9070024   3.51014375 12.98181257 18.30584527 17.27878492\n",
      "  1.17855691  2.26240257  5.69602013 15.49061136  1.33642001 54.79085304] \n",
      "\n",
      "param_clf__estimator__class_weight = ['balanced' 'balanced' 'balanced' 'balanced' 'balanced' 'balanced'\n",
      " 'balanced' 'balanced' 'balanced' 'balanced' 'balanced' 'balanced'] \n",
      "\n",
      "param_clf__estimator__max_depth = [10 10 10 10 10 10 20 20 20 20 20 20] \n",
      "\n",
      "param_clf__estimator__n_estimators = [200 200 500 500 1000 1000 200 200 500 500 1000 1000] \n",
      "\n",
      "param_features__text_pipeline__vect__ngram_range = [(1, 2) (1, 3) (1, 2) (1, 3) (1, 2) (1, 3) (1, 2) (1, 3) (1, 2) (1, 3)\n",
      " (1, 2) (1, 3)] \n",
      "\n",
      "params = [{'clf__estimator__class_weight': 'balanced', 'clf__estimator__max_depth': 10, 'clf__estimator__n_estimators': 200, 'features__text_pipeline__vect__ngram_range': (1, 2)}, {'clf__estimator__class_weight': 'balanced', 'clf__estimator__max_depth': 10, 'clf__estimator__n_estimators': 200, 'features__text_pipeline__vect__ngram_range': (1, 3)}, {'clf__estimator__class_weight': 'balanced', 'clf__estimator__max_depth': 10, 'clf__estimator__n_estimators': 500, 'features__text_pipeline__vect__ngram_range': (1, 2)}, {'clf__estimator__class_weight': 'balanced', 'clf__estimator__max_depth': 10, 'clf__estimator__n_estimators': 500, 'features__text_pipeline__vect__ngram_range': (1, 3)}, {'clf__estimator__class_weight': 'balanced', 'clf__estimator__max_depth': 10, 'clf__estimator__n_estimators': 1000, 'features__text_pipeline__vect__ngram_range': (1, 2)}, {'clf__estimator__class_weight': 'balanced', 'clf__estimator__max_depth': 10, 'clf__estimator__n_estimators': 1000, 'features__text_pipeline__vect__ngram_range': (1, 3)}, {'clf__estimator__class_weight': 'balanced', 'clf__estimator__max_depth': 20, 'clf__estimator__n_estimators': 200, 'features__text_pipeline__vect__ngram_range': (1, 2)}, {'clf__estimator__class_weight': 'balanced', 'clf__estimator__max_depth': 20, 'clf__estimator__n_estimators': 200, 'features__text_pipeline__vect__ngram_range': (1, 3)}, {'clf__estimator__class_weight': 'balanced', 'clf__estimator__max_depth': 20, 'clf__estimator__n_estimators': 500, 'features__text_pipeline__vect__ngram_range': (1, 2)}, {'clf__estimator__class_weight': 'balanced', 'clf__estimator__max_depth': 20, 'clf__estimator__n_estimators': 500, 'features__text_pipeline__vect__ngram_range': (1, 3)}, {'clf__estimator__class_weight': 'balanced', 'clf__estimator__max_depth': 20, 'clf__estimator__n_estimators': 1000, 'features__text_pipeline__vect__ngram_range': (1, 2)}, {'clf__estimator__class_weight': 'balanced', 'clf__estimator__max_depth': 20, 'clf__estimator__n_estimators': 1000, 'features__text_pipeline__vect__ngram_range': (1, 3)}] \n",
      "\n",
      "split0_test_score = [0.03596435 0.04328453 0.04423934 0.06970083 0.04774029 0.07447486\n",
      " 0.05315086 0.06906429 0.05410567 0.07352005 0.06047104 0.07988542] \n",
      "\n",
      "split1_test_score = [0.02036919 0.02896244 0.03119032 0.02928071 0.03405474 0.03405474\n",
      " 0.02768937 0.0334182  0.03500955 0.0324634  0.03723743 0.03405474] \n",
      "\n",
      "split2_test_score = [0.02387775 0.03088188 0.0296084  0.04011461 0.03533906 0.04138809\n",
      " 0.0391595  0.04457179 0.0423432  0.04616364 0.04043298 0.04743712] \n",
      "\n",
      "split3_test_score = [0.03565743 0.05062082 0.04266157 0.05539637 0.04966571 0.055078\n",
      " 0.04998408 0.06080866 0.0487106  0.06017192 0.05571474 0.05921681] \n",
      "\n",
      "split4_test_score = [0.03311047 0.05093919 0.04043298 0.06431073 0.04489016 0.06240051\n",
      " 0.05093919 0.06590258 0.0582617  0.06399236 0.05475963 0.0646291 ] \n",
      "\n",
      "mean_test_score = [0.02979563 0.04093716 0.03762654 0.05176036 0.04233781 0.05347934\n",
      " 0.04418412 0.05475266 0.04768575 0.05526199 0.04972305 0.05704463] \n",
      "\n",
      "std_test_score = [0.00643887 0.00942144 0.00604444 0.01505769 0.00643422 0.01446786\n",
      " 0.00956086 0.01359941 0.00828693 0.01439708 0.00915441 0.01552395] \n",
      "\n",
      "rank_test_score = [12 10 11  5  9  4  8  3  7  2  6  1] \n",
      "\n",
      "split0_train_score = [0.19689614 0.19625945 0.22888977 0.23016315 0.24568245 0.23844011\n",
      " 0.29088739 0.27043374 0.31890171 0.29749304 0.32725826 0.30465579] \n",
      "\n",
      "split1_train_score = [0.16283327 0.18782332 0.21504178 0.23270991 0.23294867 0.24735376\n",
      " 0.2822125  0.28245125 0.32304019 0.31301234 0.33083963 0.32160764] \n",
      "\n",
      "split2_train_score = [0.15000796 0.18112367 0.1928219  0.21120484 0.21104568 0.22521089\n",
      " 0.27478911 0.26754735 0.29818558 0.28211046 0.30423365 0.29341079] \n",
      "\n",
      "split3_train_score = [0.12096132 0.14340283 0.15637434 0.17555308 0.17109661 0.18653509\n",
      " 0.23006526 0.23157727 0.25839567 0.2559287  0.2720834  0.2700939 ] \n",
      "\n",
      "split4_train_score = [0.1336941  0.15327073 0.15884132 0.17881585 0.17229031 0.19027535\n",
      " 0.23953525 0.24088811 0.26730861 0.26635365 0.27837021 0.27431163] \n",
      "\n",
      "mean_train_score = [0.15287856 0.172376   0.19039382 0.20568937 0.20661274 0.21756304\n",
      " 0.2634979  0.25857955 0.29316635 0.28297964 0.30255703 0.29281595] \n",
      "\n",
      "std_train_score = [0.02619785 0.02044513 0.02914873 0.02445354 0.03059117 0.02485631\n",
      " 0.02416552 0.01914634 0.02629666 0.02059871 0.02419302 0.01912171] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for param_name, param_value in zip(grid_cv.cv_results_.keys(), grid_cv.cv_results_.values()):\n",
    "    print(param_name, \"=\", param_value, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.pipeline.Pipeline"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(grid_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for the 5 buckets cross validation tuned 'RandomForestClassifier' estimator:\n",
      "\n",
      "First: overall accuracy score: 0.035905\n",
      "Classification Report for each target class:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "               related       1.00      1.00      1.00      3927\n",
      "               request       0.51      0.89      0.65      1810\n",
      "                 offer       0.00      0.00      0.00        22\n",
      "           aid_related       0.60      0.70      0.65      2236\n",
      "          medical_help       0.00      0.00      0.00       291\n",
      "      medical_products       1.00      0.01      0.02       242\n",
      "     search_and_rescue       0.00      0.00      0.00       123\n",
      "              security       0.00      0.00      0.00        67\n",
      "              military       0.00      0.00      0.00        32\n",
      "           child_alone       0.00      0.00      0.00         9\n",
      "                 water       0.16      0.08      0.11       421\n",
      "                  food       0.26      0.59      0.36       891\n",
      "               shelter       0.16      0.18      0.17       554\n",
      "              clothing       1.00      0.02      0.03       133\n",
      "                 money       0.03      0.01      0.02        81\n",
      "        missing_people       0.00      0.00      0.00        57\n",
      "              refugees       0.00      0.00      0.00        94\n",
      "                 death       0.00      0.00      0.00       159\n",
      "             other_aid       0.21      0.38      0.28       796\n",
      "infrastructure_related       0.00      0.00      0.00       172\n",
      "             transport       0.00      0.00      0.00       113\n",
      "             buildings       0.00      0.00      0.00       184\n",
      "           electricity       0.00      0.00      0.00        67\n",
      "                 tools       0.00      0.00      0.00        22\n",
      "             hospitals       0.00      0.00      0.00        36\n",
      "                 shops       0.00      0.00      0.00        19\n",
      "           aid_centers       0.00      0.00      0.00        38\n",
      "  other_infrastructure       0.00      0.00      0.00        93\n",
      "       weather_related       0.60      0.28      0.38      1090\n",
      "                floods       1.00      0.01      0.02       117\n",
      "                 storm       0.23      0.09      0.13       234\n",
      "                  fire       0.00      0.00      0.00        27\n",
      "            earthquake       0.57      0.39      0.46       695\n",
      "                  cold       0.00      0.00      0.00        38\n",
      "         other_weather       0.00      0.00      0.00       115\n",
      "         direct_report       0.49      0.92      0.64      1813\n",
      "\n",
      "             micro avg       0.56      0.62      0.59     16818\n",
      "             macro avg       0.22      0.15      0.14     16818\n",
      "          weighted avg       0.55      0.62      0.55     16818\n",
      "           samples avg       0.59      0.72      0.57     16818\n",
      "\n",
      "\n",
      "\n",
      "---- Best Parameters: ----\n",
      "\n",
      "Best score: 0.057045\n",
      "Best estimators parameters set:\n",
      "\t clf__estimator__class_weight: balanced\n",
      "\t clf__estimator__max_depth: 20\n",
      "\t clf__estimator__n_estimators: 1000\n",
      "\t features__text_pipeline__vect__ngram_range: (1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation results for the 5 buckets cross validation tuned 'RandomForestClassifier' estimator:\")\n",
    "display_results(TARGET_NAMES, y_test, y_rfc_pred2, grid_cv, rfc_param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation result of the RandomForestClassifier with tuned hyperparameters is better, even there are still a lot of categories set to 0.0. Some recall values of specific target features are better. If the recall of minority target classes is very less, it proves that the model is still more biased towards majority classes. This issue is reduced as well, but still this is not the best model.\n",
    "\n",
    "With this approach target features for support values round about >400 appropriate F1-score values exists (>10%). This appeared for the following target features: request, direct_report, aid_related, earthquake, wheather related, food, other aid, shelter, storm and water. Additionally, the weighted avg F1 value is  (now 55%), the samples F1 avg value is still the same (57%).\n",
    "\n",
    "Furthermore, have in mind that some target features are not disaster related, they are document type related, like 'direct_report' or 'request'. Other target features deliver no value for the prediction task: 'related' is always set to 1 being a disaster message or 'child_alone' which is set originally to 0 for all - means no message has been labelled to this target and the existing training examples are changed manually during the ETL pipeline activities. Nevertheless, there are not enough data sets for this target making a good prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First**, we try out other machine learning algorithms which are tuned by cross validation to compare their prediction results. Other estimator models for the requested `MultiOutputClassifier` are:\n",
    "- `KNeighborsClassifier` with its default parameters: (n_neighbors=5, weights=’uniform’, algorithm=’auto’, leaf_size=30, p=2, metric=’minkowski’, metric_params=None, n_jobs=None, **kwargs)<br>\n",
    "According [KNN with TF-IDF Based Framework for Text Categorization](https://core.ac.uk/download/pdf/82438337.pdf) from Bruno Trstenjak, Sasa Mikac and Dzenana Donko in '24th DAAAM International Symposium on Intelligent Manufacturing and Automation, 2013', \"The algorithm assumes that it is possible to classify documents in the Euclidean space as points. Euclidean  distance is the distance between two points in Euclidean space.\"<br>\n",
    "But in [Effects of Distance Measure Choice on KNN Classifier Performance - A Review](https://arxiv.org/pdf/1708.04321.pdf) from V. B. Surya Prasath et al., 29.Sept.2019, in chapter '2.1. Brief overview of KNN classifier' 4 disadvantages of the KNN are mentioned. To determine a proper distance metric is one of them. Because a particular distance metric is problem and dataset dependent, we first try the euclidian default of the KNN classifier and afterwards other ones.  \n",
    "- `AdaBoostClassifier` default values are: class sklearn.ensemble.AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None).\n",
    "\n",
    "As stated in the scikit-learn [documentation](https://scikit-learn.org/stable/modules/neighbors.html#classification) \"scikit-learn implements two different nearest neighbors classifiers. One of them is the <i>KNeighborsClassifier</i> implements learning based on the nearest neighbors of each query point, where is an integer value specified by the user.\n",
    "    \n",
    "**Second**, because it is an imbalanced dataset we could do a balancing before classification. The categority classes with low numbers of observations are outnumbered. So, the dataset is highly skewed. To create a balanced dataset several strategies exists:\n",
    "- Undersampling the majority classes\n",
    "- Oversampling the minority classes\n",
    "- Combining over- and under-sampling\n",
    "- Create ensemble balanced sets\n",
    "\n",
    "But have in mind, that minority class oversampling could result in overfitting problems doing it before cross-validation. Therefore we tried to use the 'imbalanced-learn' package to modify our dataset being more balanced.\n",
    "\n",
    "Note:<br>\n",
    "Doing balancing activities the specific scikit package 'imbalanced-learn' is imported.<br>\n",
    "For combining the strategies we implement a naive random oversampling of the minority classes.<br>\n",
    "For undersampling the package can be used as well to create the pipeline with `PipelineImb`. The pipeline itself includes the class `RandomUnderSampler` directly before the MultiOutputClassifier to equalize the number of samples in all the classes before the training. Another possible approach is using the `SMOTETomek` class directly on the training dataset before classification.\n",
    "\n",
    "But using such package throws the following ValueError: 'Imbalanced-learn currently supports binary, multiclass and binarized encoded multiclasss targets. Multilabel and multioutput targets are not supported.' So, the associated package classes do not support the multi-target classification with multiple outputs as we need for our project. Therefore this coding is removed after such experiment.\n",
    "\n",
    "Another resampling technique is `cross-validation`, a method repeatingly creating additional training samples from the original training dataset to obtain additional fit information from the selected model. It creates an additional model validation set. The prediction model fits on the remaining training set and afterwards is doing its predictions on the validation set. This calculated validation error rate is an estimation of the datasets test error rate. Specific cross validation strategies exist, we are using the `k-fold cross-validation`, that divides the training set in k non-overlapping groups - called folders -. One of this folders acts as a validation set and the rest is used for training. This process is repeated k times, each time a different validation set is selected out of the group. The k-fold cross validation estimate is calculated by averaging the single k times estimation results. For k we use 5 because of time consuming calculations and not 10.\n",
    "\n",
    "According the [paper](https://arxiv.org/ftp/arxiv/papers/1810/1810.11612.pdf) <i>Handling Imbalanced Dataset in Multi-label Text Categorization using Bagging and Adaptive Boosting</i> of 27 October 2018 from Genta Indra Winata and Masayu Leylia Khodra, regarding new data, it is more appropriate to balance the dataset on the algorithm level instead of the data level to avoid overfitting. The algorithm \"approach modifies algorithm by adjusting weight or cost of various classes.\"<br>\n",
    "So, the `AdaBoostClassifier` is an ensemble method using boosting process to optimise weights. We will try this estimator as well for the <i>MultiOutputClassifier</i>. The <i>AdaBoostClassifier</i> is using the <i>DecisionTreeClassifier</i> as its own base estimator. The tree parameters are changed in the parameter grid to improve the imbalanced data situation. Weak learners are boosted to be stronger learners and the results are aggregated at the end.\n",
    "\n",
    "If the usage of the mentioned specific library is not possible for our task, what could we do instead having an appropriate input for the data classifier model? We do feature engineering.<br>\n",
    "Another option is a `feature-selection` approach which can be done after the feature extraction of the `TfidfVectorizer`, which is creating [feature vectors](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer).\n",
    "\n",
    "Additionally, scikit-learn offers the package [feature decomposition](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition) to reduce the complexity of features. With its help a subsampling is added:\n",
    "- For the sparse matrix delivered from the `TfidfVectorizer` instance we use 3000 most frequent text features, each feature token shall appear at least 2 times and n-gram wording during grid search hyperparameter tuning. The  importance of the token is increased proportionally to the number of appearing in the disaster messages.\n",
    "- Feature relationship of the sparse matrix is handled with `TruncatedSVD` for latent semantic analysis (LSA). There, a component relationship parameter is evaluated via grid search hyperparameter tuning. Afterwards we have to normalise again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This resampling with imbalanced package is not possible:\n",
    "# The following ValueError is thrown:\n",
    "# Imbalanced-learn currently supports binary, multiclass and binarized encoded multiclasss targets.\n",
    "# Multilabel and multioutput targets are not supported.\n",
    "\n",
    "# smote_tomek = SMOTETomek(random_state=FIXED_SEED)\n",
    "# X_train_res, y_train_res = smote_tomek.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('After resampling, shape of train_X: {}'.format(X_train_res.shape))\n",
    "#print('After resampling, shape of train_y: {} \\n'.format(y_train_res.shape))\n",
    "\n",
    "#print(\"After resampling, label counts '1': {}\".format(sum(y_train_res==1)))\n",
    "#print(\"After resampling, label counts '0': {}\".format(sum(y_train_res==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_type, params):\n",
    "    ''' \n",
    "    input:\n",
    "    model_type - the estimator model used for the MultiOutputClassifier\n",
    "    params - the estimator model parameter grid used for the GridSearchCV \n",
    "    ''' \n",
    "    \n",
    "    # TfidfVectorizer, by default: use_idf=True, norm=’l2’\n",
    "    # TruncatedSVD: for SLA n_components of 100 is recommended, but it is stated:\n",
    "    # Desired dimensionality of output data. Must be strictly less than the number of features.\n",
    "    # We have 36 target categories. Some of them are 'useless'. We want to know the prio list of all.\n",
    "    # The max features are 3000 tokens, so we use a smaller value as n_compontents for LSA.\n",
    "    # A token is part of the result if it appears at least 2 times\n",
    "    #\n",
    "    # For RandomizedSearchCV: for RandomForestClassifier, we have 8 parameters => n_iter=8\n",
    "    pipeline2 = Pipeline([\n",
    "        ('features', FeatureUnion([            \n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(tokenizer=tokenize, sublinear_tf=True,                                   \n",
    "                                          max_features=3000, min_df=2)),\n",
    "                ('best', TruncatedSVD(random_state=FIXED_SEED)),\n",
    "                ('normalizer', Normalizer(copy=False))\n",
    "            ]))           \n",
    "        ])),\n",
    "\n",
    "        ('clf', MultiOutputClassifier(model_type))\n",
    "    ])\n",
    "    \n",
    "    # the higher the verbose number the more information is thrown\n",
    "    cv = GridSearchCV(pipeline2, param_grid=params, return_train_score=True, n_jobs=1, cv=5, verbose=2)\n",
    "    \n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_randomcv(model_type, params, cv_iter):\n",
    "    ''' \n",
    "    input:\n",
    "    model_type - the estimator model used for the MultiOutputClassifier\n",
    "    params - the estimator model parameter grid used for the GridSearchCV \n",
    "    ''' \n",
    "    \n",
    "    # TfidfVectorizer, by default: use_idf=True, norm=’l2’\n",
    "    # TruncatedSVD: for SLA n_components of 100 is recommended, but it is stated:\n",
    "    # Desired dimensionality of output data. Must be strictly less than the number of features.\n",
    "    # We have 36 target categories. Some of them are 'useless'. We want to know the prio list of all.\n",
    "    # The max features are 3000 tokens, so we use a smaller value as n_compontents for LSA.\n",
    "    # A token is part of the result if it appears at least 2 times\n",
    "    #\n",
    "    # For RandomizedSearchCV:\n",
    "    # RandomForestClassifier: we have 8 parameters => n_iter=8\n",
    "    # AdaBoostClassifier: we have  parameters => n_iter=\n",
    "    pipeline2 = Pipeline([\n",
    "        ('features', FeatureUnion([            \n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(tokenizer=tokenize, sublinear_tf=True,                                   \n",
    "                                          max_features=3000, min_df=2)),\n",
    "                ('best', TruncatedSVD(random_state=FIXED_SEED)),\n",
    "                ('normalizer', Normalizer(copy=False))\n",
    "            ]))           \n",
    "        ])),\n",
    "\n",
    "        ('clf', MultiOutputClassifier(model_type))\n",
    "    ])\n",
    "    \n",
    "    # the higher the verbose number the more information is thrown\n",
    "    cv = RandomizedSearchCV(pipeline2, param_distributions=params, n_jobs=1, cv=5, n_iter=cv_iter,\n",
    "                            return_train_score=True, verbose=2, random_state=FIXED_SEED)\n",
    "    \n",
    "    return cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try this new pipeline including feature selection and decomposition first with the other mentioned classifiers and afterwards with an additionally tuned RandomForestClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple `KNN` parameter grid needs a long time for calculation, means the computational time cost is high. As stated in the mentioned KNN paper from Sept. 2019, Euclidian distance is not an appropriate metric if the feature dimension is high. This is the case with a high n_components value of >=1000. So, we try 'best' n_components=100 and 500 instead of 1000 or higher (note: in the scikit-learn documentation 100 is proposed for LSA tasks) and do other parameter modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['braycurtis',\n",
       " 'canberra',\n",
       " 'chebyshev',\n",
       " 'cityblock',\n",
       " 'correlation',\n",
       " 'cosine',\n",
       " 'cosine',\n",
       " 'dice',\n",
       " 'euclidean',\n",
       " 'hamming',\n",
       " 'haversine',\n",
       " 'jaccard',\n",
       " 'kulsinski',\n",
       " 'l1',\n",
       " 'l2',\n",
       " 'mahalanobis',\n",
       " 'manhattan',\n",
       " 'matching',\n",
       " 'minkowski',\n",
       " 'precomputed',\n",
       " 'rogerstanimoto',\n",
       " 'russellrao',\n",
       " 'seuclidean',\n",
       " 'sokalmichener',\n",
       " 'sokalsneath',\n",
       " 'sqeuclidean',\n",
       " 'wminkowski',\n",
       " 'yule']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sklearn.neighbors.VALID_METRICS['brute'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create param grids for the models\n",
    "\n",
    "# KNeighborsClassifier\n",
    "# according http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.24.5135&rep=rep1&type=pdf\n",
    "# cosine distance metric is commonly used,\n",
    "# compared are the cosine angles between two documents/vectors\n",
    "# (the term frequencies in different documents collected as metrics).\n",
    "# This particular metric is used when the magnitude between vectors does not matter but the orientation.\n",
    "# \n",
    "# The hamming distance tells us about the differences of compared strings of equal length.\n",
    "# It is defined as the amount of positions having different characters or symbols.\n",
    "\n",
    "knn_param_grid  = {\n",
    "    'features__text_pipeline__tfidf__ngram_range': [(1, 2), (1,3)],\n",
    "    'features__text_pipeline__best__n_components':[100, 500],\n",
    "    'clf__estimator__n_neighbors': [1, 3],\n",
    "    'clf__estimator__metric': ['euclidean', 'cosine', 'hamming'],\n",
    "    'clf__estimator__weights': ['uniform', 'distance']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- KNeighborsClassifier with feature engineering -----\n",
      "Build best model: ...\n",
      "Train model: ...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 1.5min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.6min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.5min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.4min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.6min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.6min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.5min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.5min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.6min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.5min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.3min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.1min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.2min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.3min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.2min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.3min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.1min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.2min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.6min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.0min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.2min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.4min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.1min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.3min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 1.3min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.4min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.3min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.4min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.2min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.3min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.2min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.3min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.0min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.1min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.1min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.1min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.1min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.1min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.9min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.1min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.7min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.8min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.0min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.7min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.7min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.9min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.7min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.9min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 1.6min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.8min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.6min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.6min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.7min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.5min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.5min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 4.0min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.8min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.4min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.2min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.3min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.3min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.4min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.5min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.5min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.5min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.7min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.6min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.8min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.7min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.7min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.3min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.2min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.4min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.5min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.4min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.3min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.2min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.2min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.5min\n",
      "[CV] clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=euclidean, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.4min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.3min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.2min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.0min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.2min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.3min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.1min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.0min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.1min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 1.9min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.2min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.8min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.7min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.0min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.9min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.8min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.9min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.7min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.9min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.7min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.7min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 1.8min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 1.9min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.0min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 1.8min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 1.9min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 1.8min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 1.6min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 1.9min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 1.7min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.5min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.3min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.6min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.5min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.9min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.4min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.3min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.5min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.6min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.5min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.6min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.2min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.5min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.5min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.3min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 1.8min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.6min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.6min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.5min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.6min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.6min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.4min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.0min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.2min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.3min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.2min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.4min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.3min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.1min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.1min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.1min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.1min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.2min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.4min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.4min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.2min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.4min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.4min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.3min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.1min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.6min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.2min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.2min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.0min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.2min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.8min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.9min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 3.2min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.9min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.8min\n",
      "[CV] clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=cosine, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 2.8min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 7.6min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 7.6min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 7.5min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 7.7min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 7.6min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 7.5min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 7.8min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 7.4min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 7.5min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 7.8min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total=26.7min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total=27.9min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total=27.8min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total=27.9min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total=28.0min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total=27.8min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total=28.3min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total=27.7min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total=27.4min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total=27.8min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 7.0min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 7.0min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 7.0min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 7.3min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 7.1min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 7.1min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 7.2min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 7.4min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 7.1min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 7.0min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total=26.6min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total=26.7min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total=25.6min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total=25.5min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total=24.9min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total=26.1min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total=25.3min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total=26.9min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total=25.8min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=1, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total=26.5min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 7.7min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 7.1min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 6.9min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 6.9min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 6.5min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 6.5min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 6.7min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 7.2min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 6.8min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 6.7min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total=25.2min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total=26.1min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total=21.6min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total=23.2min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total=24.0min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total=23.4min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total=22.6min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total=26.5min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total=23.7min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total=22.0min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 6.5min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 6.1min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 6.4min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 6.3min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 5.9min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 6.2min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 6.3min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 7.1min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 5.7min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 3), total= 6.7min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total=22.8min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total=23.0min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total=23.5min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total=22.7min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 2), total=24.1min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total=25.0min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total=23.9min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total=23.4min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total=22.3min\n",
      "[CV] clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3) \n",
      "[CV]  clf__estimator__metric=hamming, clf__estimator__n_neighbors=3, clf__estimator__weights=distance, features__text_pipeline__best__n_components=500, features__text_pipeline__tfidf__ngram_range=(1, 3), total=23.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed: 7668.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('features',\n",
       "                                        FeatureUnion(n_jobs=None,\n",
       "                                                     transformer_list=[('text_pipeline',\n",
       "                                                                        Pipeline(memory=None,\n",
       "                                                                                 steps=[('tfidf',\n",
       "                                                                                         TfidfVectorizer(analyzer='word',\n",
       "                                                                                                         binary=False,\n",
       "                                                                                                         decode_error='strict',\n",
       "                                                                                                         dtype=<class 'numpy.float64'>,\n",
       "                                                                                                         encoding='utf-8',\n",
       "                                                                                                         input='content',\n",
       "                                                                                                         lowercase=True,\n",
       "                                                                                                         max_df=1....\n",
       "             param_grid={'clf__estimator__metric': ['euclidean', 'cosine',\n",
       "                                                    'hamming'],\n",
       "                         'clf__estimator__n_neighbors': [1, 3],\n",
       "                         'clf__estimator__weights': ['uniform', 'distance'],\n",
       "                         'features__text_pipeline__best__n_components': [100,\n",
       "                                                                         500],\n",
       "                         'features__text_pipeline__tfidf__ngram_range': [(1, 2),\n",
       "                                                                         (1,\n",
       "                                                                          3)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# according scikitlearn: we have a sparse matrix therefore use algorithm 'brute'\n",
    "print(\"\\n----- KNeighborsClassifier with feature engineering -----\")\n",
    "print(\"Build best model: ...\")\n",
    "cv_knn_model = build_model(KNeighborsClassifier(n_jobs=1, algorithm='brute'), knn_param_grid)\n",
    "print(\"Train model: ...\")\n",
    "cv_knn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_knn_pred = cv_knn_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.pipeline.Pipeline"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cv_knn_model.estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.pipeline.FeatureUnion"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cv_knn_model.estimator['features']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cv_knn_model.estimator['features'].get_params()['transformer_list'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.pipeline.Pipeline"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cv_knn_model.estimator['features'].get_params()['transformer_list'][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=3000,\n",
       "                                 min_df=2, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=True,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenize at 0x000000789273AC80>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('best',\n",
       "                 TruncatedSVD(algorithm='randomized', n_components=2, n_iter=5,\n",
       "                              random_state=42, tol=0.0)),\n",
       "                ('normalizer', Normalizer(copy=False, norm='l2'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_knn_model.estimator['features'].get_params()['transformer_list'][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.076\n",
      "Best parameters set:\n",
      "\tclf__estimator__metric: 'euclidean'\n",
      "\tclf__estimator__n_neighbors: 3\n",
      "\tclf__estimator__weights: 'distance'\n",
      "\tfeatures__text_pipeline__best__n_components: 100\n",
      "\tfeatures__text_pipeline__tfidf__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: %0.3f\" % cv_knn_model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = cv_knn_model.best_estimator_.get_params()\n",
    "for param_name in sorted(knn_param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model evaluation on tuned KNeighborsClassifier ...\n",
      "\n",
      "First: overall accuracy score: 0.089127\n",
      "Classification Report for each target class:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "               related       1.00      1.00      1.00      3927\n",
      "               request       0.53      0.43      0.47      1810\n",
      "                 offer       0.00      0.00      0.00        22\n",
      "           aid_related       0.58      0.60      0.59      2236\n",
      "          medical_help       0.10      0.03      0.04       291\n",
      "      medical_products       0.15      0.04      0.06       242\n",
      "     search_and_rescue       0.00      0.00      0.00       123\n",
      "              security       0.00      0.00      0.00        67\n",
      "              military       0.00      0.00      0.00        32\n",
      "           child_alone       0.00      0.00      0.00         9\n",
      "                 water       0.11      0.03      0.05       421\n",
      "                  food       0.24      0.12      0.16       891\n",
      "               shelter       0.13      0.05      0.07       554\n",
      "              clothing       0.07      0.01      0.01       133\n",
      "                 money       0.06      0.02      0.04        81\n",
      "        missing_people       0.00      0.00      0.00        57\n",
      "              refugees       0.00      0.00      0.00        94\n",
      "                 death       0.06      0.01      0.02       159\n",
      "             other_aid       0.19      0.12      0.15       796\n",
      "infrastructure_related       0.04      0.01      0.02       172\n",
      "             transport       0.04      0.01      0.01       113\n",
      "             buildings       0.02      0.01      0.01       184\n",
      "           electricity       0.00      0.00      0.00        67\n",
      "                 tools       0.00      0.00      0.00        22\n",
      "             hospitals       0.00      0.00      0.00        36\n",
      "                 shops       0.00      0.00      0.00        19\n",
      "           aid_centers       0.00      0.00      0.00        38\n",
      "  other_infrastructure       0.00      0.00      0.00        93\n",
      "       weather_related       0.43      0.35      0.38      1090\n",
      "                floods       0.03      0.01      0.01       117\n",
      "                 storm       0.15      0.09      0.11       234\n",
      "                  fire       0.00      0.00      0.00        27\n",
      "            earthquake       0.53      0.34      0.41       695\n",
      "                  cold       0.00      0.00      0.00        38\n",
      "         other_weather       0.04      0.01      0.01       115\n",
      "         direct_report       0.50      0.41      0.45      1813\n",
      "\n",
      "             micro avg       0.62      0.46      0.53     16818\n",
      "             macro avg       0.14      0.10      0.11     16818\n",
      "          weighted avg       0.51      0.46      0.48     16818\n",
      "           samples avg       0.71      0.60      0.54     16818\n",
      "\n",
      "\n",
      "\n",
      "---- Best Parameters: ----\n",
      "\n",
      "Best score: 0.076208\n",
      "Best estimators parameters set:\n",
      "\t clf__estimator__metric: euclidean\n",
      "\t clf__estimator__n_neighbors: 3\n",
      "\t clf__estimator__weights: distance\n",
      "\t features__text_pipeline__best__n_components: 100\n",
      "\t features__text_pipeline__tfidf__ngram_range: (1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel evaluation on tuned KNeighborsClassifier ...\")\n",
    "display_results(TARGET_NAMES, y_test, y_knn_pred, cv_knn_model, knn_param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we improve the hyperparameter settings for the KNN classifier? By default with p=2 euclidian metric is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_knn_param_grid  = {\n",
    "    'features__text_pipeline__tfidf__ngram_range': [(1, 2)],\n",
    "    'features__text_pipeline__best__n_components':[35, 50, 100],\n",
    "    'clf__estimator__n_neighbors': [5, 7],\n",
    "    'clf__estimator__weights': ['distance', 'uniform']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- KNeighborsClassifier with feature engineering, better param grid -----\n",
      "Build best model: ...\n",
      "Train model: ...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.7min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  9.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.2min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.1min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.5min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.3min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.8min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.6min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.3min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.2min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.4min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.5min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.2min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.2min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.6min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.6min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.6min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.4min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.1min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.6min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.4min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.9min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.6min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.8min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.5min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.8min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.5min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 3.3min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.9min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.4min\n",
      "[CV] clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=5, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.5min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.2min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.2min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.7min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.2min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.8min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.4min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.4min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.2min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.3min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.2min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.4min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.3min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.6min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.9min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=distance, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.3min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.4min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.6min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.3min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.2min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=35, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.5min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.5min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.4min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.2min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.1min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=50, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.4min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.7min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.4min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.3min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.6min\n",
      "[CV] clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=7, clf__estimator__weights=uniform, features__text_pipeline__best__n_components=100, features__text_pipeline__tfidf__ngram_range=(1, 2), total= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed: 538.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('features',\n",
       "                                        FeatureUnion(n_jobs=None,\n",
       "                                                     transformer_list=[('text_pipeline',\n",
       "                                                                        Pipeline(memory=None,\n",
       "                                                                                 steps=[('tfidf',\n",
       "                                                                                         TfidfVectorizer(analyzer='word',\n",
       "                                                                                                         binary=False,\n",
       "                                                                                                         decode_error='strict',\n",
       "                                                                                                         dtype=<class 'numpy.float64'>,\n",
       "                                                                                                         encoding='utf-8',\n",
       "                                                                                                         input='content',\n",
       "                                                                                                         lowercase=True,\n",
       "                                                                                                         max_df=1....\n",
       "                                                                                             weights='uniform'),\n",
       "                                                              n_jobs=None))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=1,\n",
       "             param_grid={'clf__estimator__n_neighbors': [5, 7],\n",
       "                         'clf__estimator__weights': ['distance', 'uniform'],\n",
       "                         'features__text_pipeline__best__n_components': [35, 50,\n",
       "                                                                         100],\n",
       "                         'features__text_pipeline__tfidf__ngram_range': [(1,\n",
       "                                                                          2)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# according scikitlearn: we have a sparse matrix therefore use algorithm 'brute'\n",
    "print(\"\\n----- KNeighborsClassifier with feature engineering, better param grid -----\")\n",
    "print(\"Build best model: ...\")\n",
    "better_cv_knn_model = build_model(KNeighborsClassifier(n_jobs=1, algorithm='brute'), better_knn_param_grid)\n",
    "print(\"Train model: ...\")\n",
    "better_cv_knn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_y_knn_pred = better_cv_knn_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model evaluation on second better tuned KNeighborsClassifier ...\n",
      "\n",
      "First: overall accuracy score: 0.082506\n",
      "Classification Report for each target class:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "               related       1.00      1.00      1.00      3927\n",
      "               request       0.54      0.42      0.47      1810\n",
      "                 offer       0.00      0.00      0.00        22\n",
      "           aid_related       0.58      0.64      0.61      2236\n",
      "          medical_help       0.20      0.01      0.03       291\n",
      "      medical_products       0.18      0.01      0.02       242\n",
      "     search_and_rescue       0.00      0.00      0.00       123\n",
      "              security       0.00      0.00      0.00        67\n",
      "              military       0.00      0.00      0.00        32\n",
      "           child_alone       0.00      0.00      0.00         9\n",
      "                 water       0.14      0.01      0.01       421\n",
      "                  food       0.24      0.07      0.11       891\n",
      "               shelter       0.10      0.01      0.03       554\n",
      "              clothing       0.00      0.00      0.00       133\n",
      "                 money       0.00      0.00      0.00        81\n",
      "        missing_people       0.00      0.00      0.00        57\n",
      "              refugees       0.00      0.00      0.00        94\n",
      "                 death       0.22      0.01      0.02       159\n",
      "             other_aid       0.19      0.06      0.09       796\n",
      "infrastructure_related       0.00      0.00      0.00       172\n",
      "             transport       0.00      0.00      0.00       113\n",
      "             buildings       0.00      0.00      0.00       184\n",
      "           electricity       0.00      0.00      0.00        67\n",
      "                 tools       0.00      0.00      0.00        22\n",
      "             hospitals       0.00      0.00      0.00        36\n",
      "                 shops       0.00      0.00      0.00        19\n",
      "           aid_centers       0.00      0.00      0.00        38\n",
      "  other_infrastructure       0.00      0.00      0.00        93\n",
      "       weather_related       0.46      0.32      0.38      1090\n",
      "                floods       0.00      0.00      0.00       117\n",
      "                 storm       0.17      0.06      0.08       234\n",
      "                  fire       0.00      0.00      0.00        27\n",
      "            earthquake       0.67      0.32      0.44       695\n",
      "                  cold       0.00      0.00      0.00        38\n",
      "         other_weather       0.00      0.00      0.00       115\n",
      "         direct_report       0.51      0.39      0.44      1813\n",
      "\n",
      "             micro avg       0.68      0.45      0.54     16818\n",
      "             macro avg       0.14      0.09      0.10     16818\n",
      "          weighted avg       0.52      0.45      0.47     16818\n",
      "           samples avg       0.75      0.59      0.56     16818\n",
      "\n",
      "\n",
      "\n",
      "---- Best Parameters: ----\n",
      "\n",
      "Best score: 0.076781\n",
      "Best estimators parameters set:\n",
      "\t clf__estimator__n_neighbors: 5\n",
      "\t clf__estimator__weights: uniform\n",
      "\t features__text_pipeline__best__n_components: 100\n",
      "\t features__text_pipeline__tfidf__ngram_range: (1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel evaluation on second better tuned KNeighborsClassifier ...\")\n",
    "display_results(TARGET_NAMES, y_test, better_y_knn_pred, better_cv_knn_model, better_knn_param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this KNN training and prediction is still not good for the single categories. Only the categories with highest amount of samples are predicted properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now**, we try the other ensemble model for prediction - the `AdaBoostClassifier`. AdaBoost is an iterative ensemble method. AdaBoost classifier builds a strong classifier by combining multiple poorly performing classifiers to get high accuracy by using classifier weights and with them optimising the training data samples in each iteration by minimising training error. Therefore it deals with imbalanced datasets more appropriate compared to e.g. KNN. So, we expect to have better prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble model AdaBoostClassifier\n",
    "# class sklearn.ensemble.AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0,\n",
    "# algorithm='SAMME.R', random_state=None)\n",
    "# base_estimator is by default DecisionTreeClassifier(max_depth=1), changed it\n",
    "ada_param_grid = {\n",
    "    'features__text_pipeline__tfidf__ngram_range': [(1,2), (1,3)],\n",
    "    'features__text_pipeline__best__n_components':[35, 50, 100],\n",
    "    'clf__estimator__base_estimator__max_depth': [1, 3],\n",
    "    'clf__estimator__n_estimators': [50, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- AdaBoostClassifier with feature engineering -----\n",
      "Build best model: ...\n",
      "Train model: ...\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total= 8.8min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 10.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total= 8.8min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total= 8.8min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total= 8.6min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total= 8.5min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total= 9.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total= 9.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total= 9.1min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total= 8.9min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total= 8.8min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total=11.7min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total=11.7min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total=11.7min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total=11.6min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total=10.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total=11.8min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total=11.7min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total=11.5min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total=11.5min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total=11.6min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total=21.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total=21.1min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total=21.2min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total=21.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total=20.9min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total=20.9min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total=21.1min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total=18.9min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total=20.8min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=1, total=20.5min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=10.5min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=12.1min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=16.2min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=16.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=16.1min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=16.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=16.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=16.2min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=16.2min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=16.1min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=19.7min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=21.9min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=21.8min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=21.9min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=22.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=21.9min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=22.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=21.6min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=21.9min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=21.8min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=38.6min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=40.9min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=40.8min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=41.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=40.8min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=41.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=39.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=40.8min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=40.8min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=1, total=40.6min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=20.3min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=20.4min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=20.3min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=18.5min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=20.5min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=19.8min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=20.6min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=20.5min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=20.4min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=20.5min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=27.8min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=28.7min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=28.5min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=26.1min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=28.5min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=28.2min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=28.7min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=28.6min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=28.3min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=28.2min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=51.1min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=52.8min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=47.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=54.9min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=54.8min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=51.3min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=55.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=54.9min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=55.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=50, clf__estimator__base_estimator__max_depth=3, total=53.5min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=38.5min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=40.1min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=39.9min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=40.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=40.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=37.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=39.9min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=40.1min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=39.9min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=40.2min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=52.4min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=56.1min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=55.5min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=55.1min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=56.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=52.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=55.7min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=55.8min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=56.1min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=53.4min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=97.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=108.6min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=109.3min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=108.5min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 2), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=107.2min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=106.3min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=107.1min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=109.2min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=107.2min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3 \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=100, clf__estimator__base_estimator__max_depth=3, total=109.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed: 4398.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('features',\n",
       "                                              FeatureUnion(n_jobs=None,\n",
       "                                                           transformer_list=[('text_pipeline',\n",
       "                                                                              Pipeline(memory=None,\n",
       "                                                                                       steps=[('tfidf',\n",
       "                                                                                               TfidfVectorizer(analyzer='word',\n",
       "                                                                                                               binary=False,\n",
       "                                                                                                               decode_error='strict',\n",
       "                                                                                                               dtype=<class 'numpy.float64'>,\n",
       "                                                                                                               encoding='utf-8',\n",
       "                                                                                                               input='content',\n",
       "                                                                                                               lowercase=True,\n",
       "                                                                                                               max...\n",
       "                   iid='warn', n_iter=24, n_jobs=1,\n",
       "                   param_distributions={'clf__estimator__base_estimator__max_depth': [1,\n",
       "                                                                                      3],\n",
       "                                        'clf__estimator__n_estimators': [50,\n",
       "                                                                         100],\n",
       "                                        'features__text_pipeline__best__n_components': [35,\n",
       "                                                                                        50,\n",
       "                                                                                        100],\n",
       "                                        'features__text_pipeline__tfidf__ngram_range': [(1,\n",
       "                                                                                         2),\n",
       "                                                                                        (1,\n",
       "                                                                                         3)]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=True, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n----- AdaBoostClassifier with feature engineering -----\")\n",
    "print(\"Build best model: ...\")\n",
    "cv_ada_model = build_model_randomcv(model_type=AdaBoostClassifier(\n",
    "                                                base_estimator=DecisionTreeClassifier(class_weight='balanced',\n",
    "                                                                                      random_state=FIXED_SEED),\n",
    "                                                random_state=FIXED_SEED),\n",
    "                                    params=ada_param_grid, cv_iter=24)\n",
    "print(\"Train model: ...\")\n",
    "cv_ada_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ada_pred = cv_ada_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model evaluation on tuned AdaBoostClassifier ...\n",
      "\n",
      "First: overall accuracy score: 0.005093\n",
      "Classification Report for each target class:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "               related       1.00      1.00      1.00      3927\n",
      "               request       0.50      0.68      0.58      1810\n",
      "                 offer       0.00      0.00      0.00        22\n",
      "           aid_related       0.59      0.60      0.60      2236\n",
      "          medical_help       0.07      0.17      0.10       291\n",
      "      medical_products       0.07      0.12      0.09       242\n",
      "     search_and_rescue       0.04      0.06      0.05       123\n",
      "              security       0.01      0.01      0.01        67\n",
      "              military       0.00      0.00      0.00        32\n",
      "           child_alone       0.00      0.00      0.00         9\n",
      "                 water       0.11      0.24      0.15       421\n",
      "                  food       0.25      0.43      0.31       891\n",
      "               shelter       0.14      0.26      0.18       554\n",
      "              clothing       0.10      0.05      0.06       133\n",
      "                 money       0.03      0.06      0.04        81\n",
      "        missing_people       0.00      0.00      0.00        57\n",
      "              refugees       0.02      0.05      0.03        94\n",
      "                 death       0.06      0.15      0.08       159\n",
      "             other_aid       0.21      0.38      0.27       796\n",
      "infrastructure_related       0.04      0.11      0.06       172\n",
      "             transport       0.03      0.09      0.05       113\n",
      "             buildings       0.04      0.12      0.06       184\n",
      "           electricity       0.02      0.03      0.02        67\n",
      "                 tools       0.00      0.00      0.00        22\n",
      "             hospitals       0.00      0.00      0.00        36\n",
      "                 shops       0.00      0.00      0.00        19\n",
      "           aid_centers       0.00      0.00      0.00        38\n",
      "  other_infrastructure       0.02      0.06      0.04        93\n",
      "       weather_related       0.39      0.43      0.41      1090\n",
      "                floods       0.03      0.08      0.04       117\n",
      "                 storm       0.11      0.24      0.15       234\n",
      "                  fire       0.00      0.00      0.00        27\n",
      "            earthquake       0.31      0.50      0.38       695\n",
      "                  cold       0.03      0.05      0.03        38\n",
      "         other_weather       0.03      0.06      0.04       115\n",
      "         direct_report       0.49      0.68      0.57      1813\n",
      "\n",
      "             micro avg       0.41      0.58      0.48     16818\n",
      "             macro avg       0.13      0.19      0.15     16818\n",
      "          weighted avg       0.50      0.58      0.53     16818\n",
      "           samples avg       0.43      0.69      0.47     16818\n",
      "\n",
      "\n",
      "\n",
      "---- Best Parameters: ----\n",
      "\n",
      "Best score: 0.004266\n",
      "Best estimators parameters set:\n",
      "\t clf__estimator__base_estimator__max_depth: 3\n",
      "\t clf__estimator__n_estimators: 100\n",
      "\t features__text_pipeline__best__n_components: 100\n",
      "\t features__text_pipeline__tfidf__ngram_range: (1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel evaluation on tuned AdaBoostClassifier ...\")\n",
    "display_results(TARGET_NAMES, y_test, y_ada_pred, cv_ada_model, ada_param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_fit_time = [ 500.81215472  518.36387935  661.96901188  678.1040729  1236.76648321\n",
      " 1201.19012208  827.04902592  939.91503806 1260.20814085 1284.69999075\n",
      " 2389.56403565 2387.29037957 1182.47269912 1201.14438825 1652.31215382\n",
      " 1686.02609415 3106.19282417 3212.97365623 2355.07406769 2338.73415484\n",
      " 3277.80501637 3249.80948391 6335.19864755 6429.43141322] \n",
      "\n",
      "std_fit_time = [  8.47003324   2.52748568  42.09369518   9.42296901   6.65352007\n",
      "  48.74900499 139.98581277   6.2688438   52.92003108   5.07312582\n",
      "  54.70592671  42.07549276  43.46590703  16.46598529  59.09347479\n",
      "  13.28271674 177.09868614  80.95115605  35.47786403  72.78984239\n",
      "  74.35818212  95.04932708 279.46561469  69.63976451] \n",
      "\n",
      "mean_score_time = [21.3091783  18.82001257 19.09625058 18.72874808 24.22245455 24.28735037\n",
      " 23.99989381 25.65693498 26.947825   25.40314484 35.97902799 37.59176879\n",
      " 18.39714007 21.13625941 22.5374536  18.29763508 22.71581182 24.37856922\n",
      " 26.40464034 25.89849601 23.85839944 26.32019887 31.92602406 38.29589491] \n",
      "\n",
      "std_score_time = [1.89919869 3.62561075 3.16507513 3.57525825 4.2688432  2.29873204\n",
      " 4.25342429 3.03839586 2.05738499 4.84414961 2.48898048 2.43463747\n",
      " 3.71156766 1.54657689 1.51780125 3.81152132 6.14831158 4.9139368\n",
      " 2.48049165 1.64306652 7.65718654 2.87478328 9.67506405 3.04638813] \n",
      "\n",
      "param_features__text_pipeline__tfidf__ngram_range = [(1, 2) (1, 3) (1, 2) (1, 3) (1, 2) (1, 3) (1, 2) (1, 3) (1, 2) (1, 3)\n",
      " (1, 2) (1, 3) (1, 2) (1, 3) (1, 2) (1, 3) (1, 2) (1, 3) (1, 2) (1, 3)\n",
      " (1, 2) (1, 3) (1, 2) (1, 3)] \n",
      "\n",
      "param_features__text_pipeline__best__n_components = [35 35 50 50 100 100 35 35 50 50 100 100 35 35 50 50 100 100 35 35 50 50\n",
      " 100 100] \n",
      "\n",
      "param_clf__estimator__n_estimators = [50 50 50 50 50 50 100 100 100 100 100 100 50 50 50 50 50 50 100 100 100\n",
      " 100 100 100] \n",
      "\n",
      "param_clf__estimator__base_estimator__max_depth = [1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3] \n",
      "\n",
      "params = [{'features__text_pipeline__tfidf__ngram_range': (1, 2), 'features__text_pipeline__best__n_components': 35, 'clf__estimator__n_estimators': 50, 'clf__estimator__base_estimator__max_depth': 1}, {'features__text_pipeline__tfidf__ngram_range': (1, 3), 'features__text_pipeline__best__n_components': 35, 'clf__estimator__n_estimators': 50, 'clf__estimator__base_estimator__max_depth': 1}, {'features__text_pipeline__tfidf__ngram_range': (1, 2), 'features__text_pipeline__best__n_components': 50, 'clf__estimator__n_estimators': 50, 'clf__estimator__base_estimator__max_depth': 1}, {'features__text_pipeline__tfidf__ngram_range': (1, 3), 'features__text_pipeline__best__n_components': 50, 'clf__estimator__n_estimators': 50, 'clf__estimator__base_estimator__max_depth': 1}, {'features__text_pipeline__tfidf__ngram_range': (1, 2), 'features__text_pipeline__best__n_components': 100, 'clf__estimator__n_estimators': 50, 'clf__estimator__base_estimator__max_depth': 1}, {'features__text_pipeline__tfidf__ngram_range': (1, 3), 'features__text_pipeline__best__n_components': 100, 'clf__estimator__n_estimators': 50, 'clf__estimator__base_estimator__max_depth': 1}, {'features__text_pipeline__tfidf__ngram_range': (1, 2), 'features__text_pipeline__best__n_components': 35, 'clf__estimator__n_estimators': 100, 'clf__estimator__base_estimator__max_depth': 1}, {'features__text_pipeline__tfidf__ngram_range': (1, 3), 'features__text_pipeline__best__n_components': 35, 'clf__estimator__n_estimators': 100, 'clf__estimator__base_estimator__max_depth': 1}, {'features__text_pipeline__tfidf__ngram_range': (1, 2), 'features__text_pipeline__best__n_components': 50, 'clf__estimator__n_estimators': 100, 'clf__estimator__base_estimator__max_depth': 1}, {'features__text_pipeline__tfidf__ngram_range': (1, 3), 'features__text_pipeline__best__n_components': 50, 'clf__estimator__n_estimators': 100, 'clf__estimator__base_estimator__max_depth': 1}, {'features__text_pipeline__tfidf__ngram_range': (1, 2), 'features__text_pipeline__best__n_components': 100, 'clf__estimator__n_estimators': 100, 'clf__estimator__base_estimator__max_depth': 1}, {'features__text_pipeline__tfidf__ngram_range': (1, 3), 'features__text_pipeline__best__n_components': 100, 'clf__estimator__n_estimators': 100, 'clf__estimator__base_estimator__max_depth': 1}, {'features__text_pipeline__tfidf__ngram_range': (1, 2), 'features__text_pipeline__best__n_components': 35, 'clf__estimator__n_estimators': 50, 'clf__estimator__base_estimator__max_depth': 3}, {'features__text_pipeline__tfidf__ngram_range': (1, 3), 'features__text_pipeline__best__n_components': 35, 'clf__estimator__n_estimators': 50, 'clf__estimator__base_estimator__max_depth': 3}, {'features__text_pipeline__tfidf__ngram_range': (1, 2), 'features__text_pipeline__best__n_components': 50, 'clf__estimator__n_estimators': 50, 'clf__estimator__base_estimator__max_depth': 3}, {'features__text_pipeline__tfidf__ngram_range': (1, 3), 'features__text_pipeline__best__n_components': 50, 'clf__estimator__n_estimators': 50, 'clf__estimator__base_estimator__max_depth': 3}, {'features__text_pipeline__tfidf__ngram_range': (1, 2), 'features__text_pipeline__best__n_components': 100, 'clf__estimator__n_estimators': 50, 'clf__estimator__base_estimator__max_depth': 3}, {'features__text_pipeline__tfidf__ngram_range': (1, 3), 'features__text_pipeline__best__n_components': 100, 'clf__estimator__n_estimators': 50, 'clf__estimator__base_estimator__max_depth': 3}, {'features__text_pipeline__tfidf__ngram_range': (1, 2), 'features__text_pipeline__best__n_components': 35, 'clf__estimator__n_estimators': 100, 'clf__estimator__base_estimator__max_depth': 3}, {'features__text_pipeline__tfidf__ngram_range': (1, 3), 'features__text_pipeline__best__n_components': 35, 'clf__estimator__n_estimators': 100, 'clf__estimator__base_estimator__max_depth': 3}, {'features__text_pipeline__tfidf__ngram_range': (1, 2), 'features__text_pipeline__best__n_components': 50, 'clf__estimator__n_estimators': 100, 'clf__estimator__base_estimator__max_depth': 3}, {'features__text_pipeline__tfidf__ngram_range': (1, 3), 'features__text_pipeline__best__n_components': 50, 'clf__estimator__n_estimators': 100, 'clf__estimator__base_estimator__max_depth': 3}, {'features__text_pipeline__tfidf__ngram_range': (1, 2), 'features__text_pipeline__best__n_components': 100, 'clf__estimator__n_estimators': 100, 'clf__estimator__base_estimator__max_depth': 3}, {'features__text_pipeline__tfidf__ngram_range': (1, 3), 'features__text_pipeline__best__n_components': 100, 'clf__estimator__n_estimators': 100, 'clf__estimator__base_estimator__max_depth': 3}] \n",
      "\n",
      "split0_test_score = [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00031827 0.         0.00031827 0.00063654 0.00063654\n",
      " 0.00095481 0.00095481 0.00413749 0.00381922 0.0050923  0.00318269] \n",
      "\n",
      "split1_test_score = [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00031827 0.         0.         0.00095481 0.00063654 0.00095481\n",
      " 0.00159134 0.00031827 0.00222788 0.00254615 0.00318269 0.00413749] \n",
      "\n",
      "split2_test_score = [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00031837 0.         0.         0.         0.00031837\n",
      " 0.00095511 0.00191022 0.00286533 0.00286533 0.00159185 0.00286533] \n",
      "\n",
      "split3_test_score = [0.         0.         0.         0.         0.00031837 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.00127348 0.00031837 0.00095511 0.00159185\n",
      " 0.00254696 0.00222859 0.00095511 0.00413881 0.0031837  0.00477555] \n",
      "\n",
      "split4_test_score = [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00063674 0.         0.00095511 0.00063674 0.00063674 0.00063674\n",
      " 0.00350207 0.00095511 0.00254696 0.0031837  0.00509392 0.0063674 ] \n",
      "\n",
      "mean_test_score = [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 6.36658815e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 1.90997644e-04 1.27331763e-04 4.45661170e-04 4.45661170e-04\n",
      " 5.72992933e-04 8.27656459e-04 1.90997644e-03 1.27331763e-03\n",
      " 2.54663526e-03 3.31062584e-03 3.62895524e-03 4.26561406e-03] \n",
      "\n",
      "std_test_score = [0.         0.         0.         0.         0.00012734 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00025468 0.00015594 0.00055508 0.00032459 0.0003119  0.00043183\n",
      " 0.00098649 0.00069757 0.00102636 0.0005905  0.00132923 0.00125115] \n",
      "\n",
      "rank_test_score = [14 14 14 14 13 14 14 14 14 14 14 14 11 12  9  9  8  7  5  6  4  3  2  1] \n",
      "\n",
      "split0_train_score = [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 7.95861520e-05 0.00000000e+00 7.95861520e-05 7.95861520e-05\n",
      " 7.95861520e-05 0.00000000e+00 7.95861520e-05 1.59172304e-04\n",
      " 2.22841226e-03 1.91006765e-03 4.45682451e-03 3.02427378e-03\n",
      " 7.56068444e-03 7.71985674e-03 1.80660565e-02 2.10903303e-02\n",
      " 3.26303223e-02 3.28690808e-02 6.17588540e-02 6.42260247e-02] \n",
      "\n",
      "split1_train_score = [7.95861520e-05 0.00000000e+00 1.59172304e-04 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 7.95861520e-05\n",
      " 0.00000000e+00 1.59172304e-04 0.00000000e+00 7.95861520e-05\n",
      " 2.78551532e-03 2.46717071e-03 4.13847990e-03 4.05889375e-03\n",
      " 8.03820135e-03 8.19737366e-03 2.14086749e-02 2.14882610e-02\n",
      " 3.14365300e-02 3.19936331e-02 6.28730601e-02 5.99283725e-02] \n",
      "\n",
      "split2_train_score = [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 1.59159637e-04 0.00000000e+00 7.95798186e-05 0.00000000e+00\n",
      " 7.95798186e-05 7.95798186e-05 7.95798186e-05 7.95798186e-05\n",
      " 3.02403311e-03 3.26277256e-03 4.21773038e-03 4.21773038e-03\n",
      " 1.00270571e-02 1.07432755e-02 2.77733567e-02 2.77733567e-02\n",
      " 3.39805825e-02 3.50151202e-02 6.85182238e-02 7.25767945e-02] \n",
      "\n",
      "split3_train_score = [1.59159637e-04 0.00000000e+00 7.95798186e-05 0.00000000e+00\n",
      " 0.00000000e+00 1.59159637e-04 1.59159637e-04 7.95798186e-05\n",
      " 0.00000000e+00 3.18319274e-04 7.95798186e-04 3.18319274e-04\n",
      " 4.05857075e-03 3.81983129e-03 3.58109184e-03 6.04806621e-03\n",
      " 9.78831768e-03 9.94747732e-03 2.35556263e-02 2.52268025e-02\n",
      " 3.11157091e-02 3.01607512e-02 6.60512494e-02 6.85182238e-02] \n",
      "\n",
      "split4_train_score = [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.59159637e-04\n",
      " 7.95798186e-05 0.00000000e+00 1.59159637e-04 7.95798186e-05\n",
      " 7.95798186e-05 1.59159637e-04 5.57058730e-04 3.97899093e-04\n",
      " 3.58109184e-03 3.66067165e-03 5.80932675e-03 5.80932675e-03\n",
      " 1.11411746e-02 1.06636957e-02 2.12478116e-02 2.42718447e-02\n",
      " 3.53334394e-02 3.56517587e-02 7.19401560e-02 6.62104090e-02] \n",
      "\n",
      "mean_train_score = [4.77491578e-05 0.00000000e+00 4.77504245e-05 3.18319274e-05\n",
      " 6.36651215e-05 3.18319274e-05 9.54970490e-05 6.36663882e-05\n",
      " 4.77491578e-05 1.43246207e-04 3.02404577e-04 2.06911328e-04\n",
      " 3.13552465e-03 3.02410277e-03 4.44069068e-03 4.63165818e-03\n",
      " 9.31108704e-03 9.45433578e-03 2.24103052e-02 2.39701190e-02\n",
      " 3.28993167e-02 3.31380688e-02 6.62283086e-02 6.62919649e-02] \n",
      "\n",
      "std_train_score = [6.36644882e-05 0.00000000e+00 6.36682883e-05 6.36638548e-05\n",
      " 5.95524218e-05 6.36638548e-05 5.95517447e-05 3.18331942e-05\n",
      " 3.89870242e-05 1.05574942e-04 3.15921966e-04 1.29299718e-04\n",
      " 6.33771884e-04 7.27545828e-04 7.42075175e-04 1.13808567e-03\n",
      " 1.32466826e-03 1.26138629e-03 3.20316197e-03 2.47336655e-03\n",
      " 1.58034479e-03 2.00441843e-03 3.71843998e-03 4.22434329e-03] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for param_name, param_value in zip(cv_ada_model.cv_results_.keys(), cv_ada_model.cv_results_.values()):\n",
    "    print(param_name, \"=\", param_value, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the evaluation results of the <i>KNeighborsClassifier</i> model, it is not acceptable comparing the single target features. The hamming distance is not valuable at all, still euclidian metric has been the best. \n",
    "\n",
    "Compared to the KNN model the <i>AdaBoostClassifier</i> model can handle the imbalanced dataset much better and has much more appropriate predictions regarding the metric values of the single target categories. By now, this is the best model we have been evaluated yet.\n",
    "\n",
    "Would the feature selection and decomposition improve the RandomForestClassifier? Because of calculation time range we use the <i>RandomizedSearchCV</i>, knowing that this has a little bit lesser performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the other models 100 best n_components have been the best hyperparameter for TruncatedSVD\n",
    "better_rfc_param_grid = {\n",
    "    'features__text_pipeline__tfidf__ngram_range': [(1,3)],\n",
    "    'features__text_pipeline__best__n_components':[35, 50, 100],\n",
    "    'clf__estimator__n_estimators': [200, 600, 800],\n",
    "    'clf__estimator__max_depth': [20],\n",
    "    'clf__estimator__class_weight': ['balanced']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- RandomForestClassifier with feature engineering and modified param grid -----\n",
      "Build best model: ...\n",
      "Train model: ...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=800, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=800, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=110.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 118.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=800, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=800, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=90.8min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=800, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=800, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=132.6min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=800, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=800, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=133.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=800, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=800, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=130.6min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=32.4min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=33.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=32.3min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=33.1min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=33.3min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=136.7min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=133.8min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=134.5min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=130.4min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=132.7min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=25.1min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=22.6min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=24.4min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=19.8min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=16.9min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=800, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=800, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=188.7min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=800, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=800, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=187.3min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=800, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=800, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=182.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=800, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=800, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=178.5min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=800, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=800, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=177.9min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=45.2min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=45.1min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=44.9min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=42.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=100, clf__estimator__n_estimators=200, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=44.8min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=101.0min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=96.3min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=84.2min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=98.3min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=50, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=95.5min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=77.2min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=74.4min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=69.3min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n",
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=72.6min\n",
      "[CV] features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__text_pipeline__tfidf__ngram_range=(1, 3), features__text_pipeline__best__n_components=35, clf__estimator__n_estimators=600, clf__estimator__max_depth=20, clf__estimator__class_weight=balanced, total=72.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed: 3687.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('features',\n",
       "                                              FeatureUnion(n_jobs=None,\n",
       "                                                           transformer_list=[('text_pipeline',\n",
       "                                                                              Pipeline(memory=None,\n",
       "                                                                                       steps=[('tfidf',\n",
       "                                                                                               TfidfVectorizer(analyzer='word',\n",
       "                                                                                                               binary=False,\n",
       "                                                                                                               decode_error='strict',\n",
       "                                                                                                               dtype=<class 'numpy.float64'>,\n",
       "                                                                                                               encoding='utf-8',\n",
       "                                                                                                               input='content',\n",
       "                                                                                                               lowercase=True,\n",
       "                                                                                                               max...\n",
       "                   param_distributions={'clf__estimator__class_weight': ['balanced'],\n",
       "                                        'clf__estimator__max_depth': [20],\n",
       "                                        'clf__estimator__n_estimators': [200,\n",
       "                                                                         600,\n",
       "                                                                         800],\n",
       "                                        'features__text_pipeline__best__n_components': [35,\n",
       "                                                                                        50,\n",
       "                                                                                        100],\n",
       "                                        'features__text_pipeline__tfidf__ngram_range': [(1,\n",
       "                                                                                         3)]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=True, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n----- RandomForestClassifier with feature engineering and modified param grid -----\")\n",
    "print(\"Build best model: ...\")\n",
    "cv_better_rfc_model = build_model_randomcv(model_type=RandomForestClassifier(n_jobs=1, random_state=FIXED_SEED),\n",
    "                                           params=better_rfc_param_grid, cv_iter=8)\n",
    "print(\"Train model: ...\")\n",
    "cv_better_rfc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_better_rfc_pred = cv_better_rfc_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model evaluation on tuned RandomForestClassifier with feature engineering...\n",
      "\n",
      "First: overall accuracy score: 0.048892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for each target class:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "               related       1.00      1.00      1.00      3927\n",
      "               request       0.54      0.54      0.54      1810\n",
      "                 offer       0.00      0.00      0.00        22\n",
      "           aid_related       0.59      0.74      0.66      2236\n",
      "          medical_help       0.00      0.00      0.00       291\n",
      "      medical_products       0.60      0.01      0.02       242\n",
      "     search_and_rescue       0.00      0.00      0.00       123\n",
      "              security       0.00      0.00      0.00        67\n",
      "              military       0.00      0.00      0.00        32\n",
      "           child_alone       0.00      0.00      0.00         9\n",
      "                 water       0.14      0.00      0.01       421\n",
      "                  food       0.29      0.06      0.09       891\n",
      "               shelter       0.17      0.01      0.02       554\n",
      "              clothing       0.20      0.02      0.03       133\n",
      "                 money       0.08      0.01      0.02        81\n",
      "        missing_people       0.00      0.00      0.00        57\n",
      "              refugees       0.00      0.00      0.00        94\n",
      "                 death       0.11      0.01      0.01       159\n",
      "             other_aid       0.20      0.02      0.04       796\n",
      "infrastructure_related       0.00      0.00      0.00       172\n",
      "             transport       0.00      0.00      0.00       113\n",
      "             buildings       0.00      0.00      0.00       184\n",
      "           electricity       0.00      0.00      0.00        67\n",
      "                 tools       0.00      0.00      0.00        22\n",
      "             hospitals       0.00      0.00      0.00        36\n",
      "                 shops       0.00      0.00      0.00        19\n",
      "           aid_centers       0.00      0.00      0.00        38\n",
      "  other_infrastructure       0.00      0.00      0.00        93\n",
      "       weather_related       0.59      0.27      0.37      1090\n",
      "                floods       0.00      0.00      0.00       117\n",
      "                 storm       0.19      0.05      0.08       234\n",
      "                  fire       0.00      0.00      0.00        27\n",
      "            earthquake       0.71      0.28      0.40       695\n",
      "                  cold       0.00      0.00      0.00        38\n",
      "         other_weather       0.00      0.00      0.00       115\n",
      "         direct_report       0.51      0.51      0.51      1813\n",
      "\n",
      "             micro avg       0.70      0.48      0.57     16818\n",
      "             macro avg       0.16      0.10      0.11     16818\n",
      "          weighted avg       0.54      0.48      0.48     16818\n",
      "           samples avg       0.74      0.62      0.57     16818\n",
      "\n",
      "\n",
      "\n",
      "---- Best Parameters: ----\n",
      "\n",
      "Best score: 0.051378\n",
      "Best estimators parameters set:\n",
      "\t clf__estimator__class_weight: balanced\n",
      "\t clf__estimator__max_depth: 20\n",
      "\t clf__estimator__n_estimators: 200\n",
      "\t features__text_pipeline__best__n_components: 35\n",
      "\t features__text_pipeline__tfidf__ngram_range: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel evaluation on tuned RandomForestClassifier with feature engineering...\")\n",
    "display_results(TARGET_NAMES, y_test, y_better_rfc_pred, cv_better_rfc_model, better_rfc_param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_fit_time = [ 6766.61998363  1934.81381388  7879.77318654  1270.81696072\n",
      " 10694.54701014  2624.38108959  5573.72268753  4256.54027619] \n",
      "\n",
      "std_fit_time = [1137.62940298   23.27127936  121.31311604  182.72848141  192.71065327\n",
      "   72.68445393  338.75292779  144.00183303] \n",
      "\n",
      "mean_score_time = [406.60862017  35.02166605 136.52052999  33.98980808 278.84143052\n",
      "  38.55520611 130.07929959 133.5517818 ] \n",
      "\n",
      "std_score_time = [315.68765054   4.12343799   6.40957199   8.13886705  99.33203965\n",
      "   1.16232684   9.0522394   14.06957846] \n",
      "\n",
      "param_features__text_pipeline__tfidf__ngram_range = [(1, 3) (1, 3) (1, 3) (1, 3) (1, 3) (1, 3) (1, 3) (1, 3)] \n",
      "\n",
      "param_features__text_pipeline__best__n_components = [50 50 100 35 100 100 50 35] \n",
      "\n",
      "param_clf__estimator__n_estimators = [800 200 600 200 800 200 600 600] \n",
      "\n",
      "param_clf__estimator__max_depth = [20 20 20 20 20 20 20 20] \n",
      "\n",
      "param_clf__estimator__class_weight = ['balanced' 'balanced' 'balanced' 'balanced' 'balanced' 'balanced'\n",
      " 'balanced' 'balanced'] \n",
      "\n",
      "params = [{'features__text_pipeline__tfidf__ngram_range': (1, 3), 'features__text_pipeline__best__n_components': 50, 'clf__estimator__n_estimators': 800, 'clf__estimator__max_depth': 20, 'clf__estimator__class_weight': 'balanced'}, {'features__text_pipeline__tfidf__ngram_range': (1, 3), 'features__text_pipeline__best__n_components': 50, 'clf__estimator__n_estimators': 200, 'clf__estimator__max_depth': 20, 'clf__estimator__class_weight': 'balanced'}, {'features__text_pipeline__tfidf__ngram_range': (1, 3), 'features__text_pipeline__best__n_components': 100, 'clf__estimator__n_estimators': 600, 'clf__estimator__max_depth': 20, 'clf__estimator__class_weight': 'balanced'}, {'features__text_pipeline__tfidf__ngram_range': (1, 3), 'features__text_pipeline__best__n_components': 35, 'clf__estimator__n_estimators': 200, 'clf__estimator__max_depth': 20, 'clf__estimator__class_weight': 'balanced'}, {'features__text_pipeline__tfidf__ngram_range': (1, 3), 'features__text_pipeline__best__n_components': 100, 'clf__estimator__n_estimators': 800, 'clf__estimator__max_depth': 20, 'clf__estimator__class_weight': 'balanced'}, {'features__text_pipeline__tfidf__ngram_range': (1, 3), 'features__text_pipeline__best__n_components': 100, 'clf__estimator__n_estimators': 200, 'clf__estimator__max_depth': 20, 'clf__estimator__class_weight': 'balanced'}, {'features__text_pipeline__tfidf__ngram_range': (1, 3), 'features__text_pipeline__best__n_components': 50, 'clf__estimator__n_estimators': 600, 'clf__estimator__max_depth': 20, 'clf__estimator__class_weight': 'balanced'}, {'features__text_pipeline__tfidf__ngram_range': (1, 3), 'features__text_pipeline__best__n_components': 35, 'clf__estimator__n_estimators': 600, 'clf__estimator__max_depth': 20, 'clf__estimator__class_weight': 'balanced'}] \n",
      "\n",
      "split0_test_score = [0.06015277 0.07129217 0.06142584 0.06015277 0.06078931 0.06747295\n",
      " 0.06015277 0.06269892] \n",
      "\n",
      "split1_test_score = [0.02991725 0.02864418 0.02259707 0.03723743 0.0222788  0.02705283\n",
      " 0.02991725 0.03150859] \n",
      "\n",
      "split2_test_score = [0.02451449 0.0286533  0.0222859  0.03724928 0.02069405 0.02769819\n",
      " 0.0254696  0.03438395] \n",
      "\n",
      "split3_test_score = [0.04266157 0.04393505 0.04106972 0.05380452 0.03884113 0.05380452\n",
      " 0.04202483 0.04839223] \n",
      "\n",
      "split4_test_score = [0.05666985 0.06049029 0.05762496 0.06844954 0.0582617  0.06494747\n",
      " 0.055078   0.06208214] \n",
      "\n",
      "mean_test_score = [0.04278347 0.04660343 0.04100083 0.05137837 0.04017317 0.04819507\n",
      " 0.04252881 0.04781308] \n",
      "\n",
      "std_test_score = [0.01409863 0.01705493 0.01662865 0.01244096 0.01705105 0.01761207\n",
      " 0.01355316 0.01320418] \n",
      "\n",
      "rank_test_score = [5 4 7 1 8 2 6 3] \n",
      "\n",
      "split0_train_score = [0.42793474 0.42976522 0.49454835 0.39450856 0.4959809  0.4908078\n",
      " 0.42785515 0.39570235] \n",
      "\n",
      "split1_train_score = [0.43828094 0.44027059 0.49279745 0.40031834 0.49502587 0.49208118\n",
      " 0.43836053 0.3965778 ] \n",
      "\n",
      "split2_train_score = [0.46227917 0.45941429 0.54424638 0.44111093 0.54512176 0.53485596\n",
      " 0.46347286 0.44190673] \n",
      "\n",
      "split3_train_score = [0.47039631 0.466099   0.5441668  0.43577909 0.54392806 0.53915327\n",
      " 0.46928219 0.43530161] \n",
      "\n",
      "split4_train_score = [0.45551488 0.45384371 0.54432596 0.43466497 0.54384848 0.54050613\n",
      " 0.45877765 0.43522203] \n",
      "\n",
      "mean_train_score = [0.45088121 0.44987856 0.52401699 0.42127638 0.52478101 0.51948087\n",
      " 0.45154968 0.4209421 ] \n",
      "\n",
      "std_train_score = [0.01560468 0.01316522 0.02478208 0.01969136 0.02391125 0.02297105\n",
      " 0.01577487 0.02039751] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for param_name, param_value in zip(cv_better_rfc_model.cv_results_.keys(),\n",
    "                                   cv_better_rfc_model.cv_results_.values()):\n",
    "    print(param_name, \"=\", param_value, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:<br>\n",
    "For the RandomForestClassifier the usage of the feature selection and decomposition improves the prediction results for the specific target features and the model is much less biased towards the majority classes.\n",
    "\n",
    "Nevertheless, still the `AdaBoostClassifier`can handle the imbalanced dataset much better compared to all other used model types. So, we store it as our pickle file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, having found the best model from our model selection list, we save this model with its best parameters as a pickle file. Pickle is the standard way of serialising objects in Python. With this pickle file we can deserialise our model and use it to make new predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_filepath):\n",
    "    pickle.dump(model, open(model_filepath, \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "    MODEL: classifier.pkl\n",
      "Best trained model saved!\n"
     ]
    }
   ],
   "source": [
    "# see train_classifier.py file\n",
    "model_filepath = \"classifier.pkl\"\n",
    "model = cv_ada_model\n",
    "print('Saving model...\\n    MODEL: {}'.format(model_filepath))\n",
    "save_model(model, model_filepath)\n",
    "\n",
    "print('Best trained model saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train_classifier.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
