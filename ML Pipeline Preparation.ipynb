{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.<br>\n",
    "\n",
    "My general notes:<br>\n",
    "Have in mind, that we work on a multi-class, multi-label text classification which assigns to each message sample a set of category target labels. The messages are short and an imbalanced data distribution exists. The dataset has 19634 data points with 32 different target categories.\n",
    "\n",
    "During the disaster messages processing, the English text is tokenized, lower cased, lemmatized and the contractions are expanded. Additionally, e.g. spaces, punctuation and English stop words are removed.\n",
    "\n",
    "\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: once\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# import libraries\n",
    "#\n",
    "\n",
    "# download necessary NLTK data\n",
    "import nltk\n",
    "#nltk.download(['punkt', 'wordnet', 'stopwords'])\n",
    "\n",
    "# import libraries\n",
    "import random as rn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import pickle\n",
    "from sqlalchemy import create_engine\n",
    "import Contractions\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# warnings status to show\n",
    "import warnings\n",
    "warnings.warn(\"once\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the code reproducible ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_SEED = 42\n",
    "\n",
    "# The below is necessary for starting NumPy generated random numbers in a well-defined initial state.\n",
    "np.random.seed(FIXED_SEED)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers in a well-defined state.\n",
    "rn.seed(FIXED_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 14403 data points with 36 variables each.\n"
     ]
    }
   ],
   "source": [
    "# load data from database\n",
    "try:\n",
    "    engine = create_engine('sqlite:///Disaster_Messages_engine.db')\n",
    "    df = pd.read_sql_table('Messages_Categories_table', engine)\n",
    "    \n",
    "    # success\n",
    "    print(\"The dataset has {} data points with {} variables each.\".format(*df.shape))\n",
    "except:\n",
    "    print(\"The database 'Disaster_Messages_engine.db' could not be loaded. No ML pipeline activities possible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>lang_code</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>tools</th>\n",
       "      <th>hospitals</th>\n",
       "      <th>shops</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Storm at sacred heart of jesus</td>\n",
       "      <td>Cyclone Coeur sacr de jesus</td>\n",
       "      <td>direct</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Please, we need tents and water. We are in Sil...</td>\n",
       "      <td>Tanpri nou bezwen tant avek dlo nou zon silo m...</td>\n",
       "      <td>direct</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am in Croix-des-Bouquets. We have health iss...</td>\n",
       "      <td>Nou kwadebouke, nou gen pwoblem sant m yo nan ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  \\\n",
       "0            Is the Hurricane over or is it not over   \n",
       "1  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "2                     Storm at sacred heart of jesus   \n",
       "3  Please, we need tents and water. We are in Sil...   \n",
       "4  I am in Croix-des-Bouquets. We have health iss...   \n",
       "\n",
       "                                            original   genre lang_code  \\\n",
       "0                 Cyclone nan fini osinon li pa fini  direct        en   \n",
       "1  UN reports Leogane 80-90 destroyed. Only Hospi...  direct        en   \n",
       "2                        Cyclone Coeur sacr de jesus  direct        en   \n",
       "3  Tanpri nou bezwen tant avek dlo nou zon silo m...  direct        en   \n",
       "4  Nou kwadebouke, nou gen pwoblem sant m yo nan ...  direct        en   \n",
       "\n",
       "   related  request  offer  aid_related  medical_help  medical_products  ...  \\\n",
       "0        1        0      0            1             0                 0  ...   \n",
       "1        1        1      0            1             0                 1  ...   \n",
       "2        1        0      0            0             0                 0  ...   \n",
       "3        1        1      0            1             0                 0  ...   \n",
       "4        1        1      0            1             1                 1  ...   \n",
       "\n",
       "   tools  hospitals  shops  aid_centers  weather_related  floods  storm  fire  \\\n",
       "0      0          0      0            0                1       0      1     0   \n",
       "1      0          1      0            0                0       0      0     0   \n",
       "2      0          0      0            0                1       0      1     0   \n",
       "3      0          0      0            0                0       0      0     0   \n",
       "4      0          0      0            0                0       0      0     0   \n",
       "\n",
       "   earthquake  cold  \n",
       "0           0     0  \n",
       "1           0     0  \n",
       "2           0     0  \n",
       "3           0     0  \n",
       "4           0     0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input (X) and output (y) samples, we know that related is always one ...\n",
    "# as input we have to take care about the messages\n",
    "# the categories are the targets of the multi-class, multi-label classification\n",
    "X = df[['message']]\n",
    "y = df[df.columns[4:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message\n",
       "0            Is the Hurricane over or is it not over\n",
       "1  UN reports Leogane 80-90 destroyed. Only Hospi..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>security</th>\n",
       "      <th>military</th>\n",
       "      <th>water</th>\n",
       "      <th>...</th>\n",
       "      <th>tools</th>\n",
       "      <th>hospitals</th>\n",
       "      <th>shops</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14373</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14374</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14375</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14376</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14377</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14378</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14379</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14380</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14381</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14382</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14383</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14384</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14385</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14386</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14387</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14388</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14389</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14390</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14391</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14392</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14393</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14394</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14395</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14396</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14397</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14398</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14399</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14400</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14401</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14402</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14403 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       related  request  offer  aid_related  medical_help  medical_products  \\\n",
       "0            1        0      0            1             0                 0   \n",
       "1            1        1      0            1             0                 1   \n",
       "2            1        0      0            0             0                 0   \n",
       "3            1        1      0            1             0                 0   \n",
       "4            1        1      0            1             1                 1   \n",
       "5            1        1      0            1             1                 1   \n",
       "6            1        1      0            1             0                 0   \n",
       "7            1        1      0            1             0                 0   \n",
       "8            1        1      0            1             0                 1   \n",
       "9            1        1      0            1             1                 0   \n",
       "10           1        0      0            0             0                 0   \n",
       "11           1        1      0            1             0                 0   \n",
       "12           1        1      0            1             0                 0   \n",
       "13           1        1      0            1             0                 0   \n",
       "14           1        1      0            1             1                 1   \n",
       "15           1        1      0            1             1                 0   \n",
       "16           1        1      0            1             0                 0   \n",
       "17           1        1      0            1             0                 0   \n",
       "18           1        1      0            1             0                 0   \n",
       "19           1        1      0            1             0                 0   \n",
       "20           1        1      0            1             0                 0   \n",
       "21           1        1      0            1             0                 0   \n",
       "22           1        1      0            1             0                 0   \n",
       "23           1        0      0            1             1                 0   \n",
       "24           1        0      0            1             0                 0   \n",
       "25           1        0      0            1             0                 0   \n",
       "26           1        0      0            1             0                 1   \n",
       "27           1        0      0            1             0                 0   \n",
       "28           1        1      0            0             0                 0   \n",
       "29           1        1      0            1             0                 0   \n",
       "...        ...      ...    ...          ...           ...               ...   \n",
       "14373        1        0      0            1             0                 0   \n",
       "14374        1        0      0            0             0                 0   \n",
       "14375        1        0      0            1             1                 1   \n",
       "14376        1        0      0            1             0                 0   \n",
       "14377        1        0      0            0             0                 0   \n",
       "14378        1        0      0            1             0                 0   \n",
       "14379        1        0      0            1             0                 0   \n",
       "14380        1        0      0            1             0                 0   \n",
       "14381        1        0      0            1             1                 0   \n",
       "14382        1        1      0            1             1                 1   \n",
       "14383        1        0      0            1             1                 0   \n",
       "14384        1        0      0            1             1                 1   \n",
       "14385        1        0      0            1             0                 0   \n",
       "14386        1        0      0            1             0                 0   \n",
       "14387        1        0      0            1             0                 1   \n",
       "14388        1        0      0            1             1                 0   \n",
       "14389        1        0      0            1             0                 1   \n",
       "14390        1        0      0            1             0                 0   \n",
       "14391        1        0      0            1             0                 0   \n",
       "14392        1        0      0            1             0                 0   \n",
       "14393        1        0      0            0             0                 0   \n",
       "14394        1        0      0            1             0                 0   \n",
       "14395        1        0      0            1             0                 1   \n",
       "14396        1        0      0            1             0                 0   \n",
       "14397        1        0      0            0             0                 0   \n",
       "14398        1        0      0            1             0                 0   \n",
       "14399        1        0      0            1             1                 1   \n",
       "14400        1        0      0            1             0                 0   \n",
       "14401        1        1      0            0             0                 0   \n",
       "14402        1        0      0            1             0                 0   \n",
       "\n",
       "       search_and_rescue  security  military  water  ...  tools  hospitals  \\\n",
       "0                      0         0         0      0  ...      0          0   \n",
       "1                      0         0         0      0  ...      0          1   \n",
       "2                      0         0         0      0  ...      0          0   \n",
       "3                      0         0         0      1  ...      0          0   \n",
       "4                      0         0         0      0  ...      0          0   \n",
       "5                      0         0         0      1  ...      0          0   \n",
       "6                      0         0         0      1  ...      0          0   \n",
       "7                      0         0         0      0  ...      0          0   \n",
       "8                      0         0         0      1  ...      0          0   \n",
       "9                      0         0         0      1  ...      0          0   \n",
       "10                     0         0         0      0  ...      0          0   \n",
       "11                     0         0         0      0  ...      0          0   \n",
       "12                     0         0         0      1  ...      0          0   \n",
       "13                     0         0         0      0  ...      0          0   \n",
       "14                     0         0         0      1  ...      0          0   \n",
       "15                     0         0         0      0  ...      0          0   \n",
       "16                     0         0         0      1  ...      0          0   \n",
       "17                     0         0         0      0  ...      0          0   \n",
       "18                     0         0         0      1  ...      0          0   \n",
       "19                     0         0         0      0  ...      0          0   \n",
       "20                     0         0         0      0  ...      0          0   \n",
       "21                     0         0         0      0  ...      0          0   \n",
       "22                     0         0         0      1  ...      0          0   \n",
       "23                     0         0         1      1  ...      0          0   \n",
       "24                     0         0         0      0  ...      0          0   \n",
       "25                     0         0         0      0  ...      0          0   \n",
       "26                     0         0         0      0  ...      0          0   \n",
       "27                     0         0         0      0  ...      0          0   \n",
       "28                     0         0         0      0  ...      0          0   \n",
       "29                     0         0         0      1  ...      0          0   \n",
       "...                  ...       ...       ...    ...  ...    ...        ...   \n",
       "14373                  0         0         0      0  ...      0          0   \n",
       "14374                  0         0         0      0  ...      0          0   \n",
       "14375                  0         0         0      0  ...      0          0   \n",
       "14376                  0         0         0      0  ...      0          0   \n",
       "14377                  0         0         0      0  ...      0          0   \n",
       "14378                  0         0         0      0  ...      0          0   \n",
       "14379                  0         0         1      0  ...      0          0   \n",
       "14380                  0         0         0      0  ...      0          0   \n",
       "14381                  0         0         0      0  ...      0          0   \n",
       "14382                  0         0         0      0  ...      0          0   \n",
       "14383                  0         0         0      0  ...      0          0   \n",
       "14384                  0         0         0      0  ...      0          0   \n",
       "14385                  0         0         1      0  ...      0          0   \n",
       "14386                  0         0         0      0  ...      0          0   \n",
       "14387                  0         0         0      0  ...      0          0   \n",
       "14388                  0         0         0      0  ...      0          0   \n",
       "14389                  0         0         0      0  ...      0          0   \n",
       "14390                  0         0         0      0  ...      0          0   \n",
       "14391                  0         0         1      0  ...      0          0   \n",
       "14392                  0         0         1      0  ...      0          0   \n",
       "14393                  0         0         0      0  ...      0          0   \n",
       "14394                  1         0         0      1  ...      0          0   \n",
       "14395                  0         0         0      0  ...      0          0   \n",
       "14396                  0         0         0      0  ...      0          0   \n",
       "14397                  0         0         0      0  ...      0          0   \n",
       "14398                  0         0         0      0  ...      0          0   \n",
       "14399                  0         0         0      0  ...      0          1   \n",
       "14400                  0         0         0      0  ...      0          0   \n",
       "14401                  0         0         0      0  ...      0          0   \n",
       "14402                  0         0         1      0  ...      0          0   \n",
       "\n",
       "       shops  aid_centers  weather_related  floods  storm  fire  earthquake  \\\n",
       "0          0            0                1       0      1     0           0   \n",
       "1          0            0                0       0      0     0           0   \n",
       "2          0            0                1       0      1     0           0   \n",
       "3          0            0                0       0      0     0           0   \n",
       "4          0            0                0       0      0     0           0   \n",
       "5          0            0                1       1      0     0           0   \n",
       "6          0            0                0       0      0     0           0   \n",
       "7          0            0                0       0      0     0           0   \n",
       "8          0            0                0       0      0     0           0   \n",
       "9          0            0                0       0      0     0           0   \n",
       "10         0            0                1       0      0     0           1   \n",
       "11         0            0                0       0      0     0           0   \n",
       "12         0            0                0       0      0     0           0   \n",
       "13         0            0                0       0      0     0           0   \n",
       "14         0            0                0       0      0     0           0   \n",
       "15         0            0                1       1      0     0           0   \n",
       "16         0            0                0       0      0     0           0   \n",
       "17         0            0                0       0      0     0           0   \n",
       "18         0            0                0       0      0     0           0   \n",
       "19         0            0                0       0      0     0           0   \n",
       "20         0            0                0       0      0     0           0   \n",
       "21         0            0                0       0      0     0           0   \n",
       "22         0            0                0       0      0     0           0   \n",
       "23         1            0                1       0      0     0           1   \n",
       "24         0            0                0       0      0     0           0   \n",
       "25         0            0                0       0      0     0           0   \n",
       "26         0            0                1       0      0     0           0   \n",
       "27         0            0                1       0      0     0           1   \n",
       "28         0            0                0       0      0     0           0   \n",
       "29         0            0                0       0      0     0           0   \n",
       "...      ...          ...              ...     ...    ...   ...         ...   \n",
       "14373      0            0                1       1      0     0           0   \n",
       "14374      0            0                1       0      0     0           0   \n",
       "14375      0            0                0       0      0     0           0   \n",
       "14376      0            0                1       0      0     0           0   \n",
       "14377      0            0                1       0      0     0           0   \n",
       "14378      0            0                0       0      0     0           0   \n",
       "14379      0            0                0       0      0     0           0   \n",
       "14380      0            0                1       1      0     0           0   \n",
       "14381      0            0                0       0      0     0           0   \n",
       "14382      0            0                0       0      0     0           0   \n",
       "14383      0            1                0       0      0     0           0   \n",
       "14384      0            0                0       0      0     0           0   \n",
       "14385      0            0                0       0      0     0           0   \n",
       "14386      0            0                0       0      0     0           0   \n",
       "14387      0            0                0       0      0     0           0   \n",
       "14388      0            0                0       0      0     0           0   \n",
       "14389      0            0                0       0      0     0           0   \n",
       "14390      0            0                0       0      0     0           0   \n",
       "14391      0            0                0       0      0     0           0   \n",
       "14392      0            0                0       0      0     0           0   \n",
       "14393      0            0                1       1      0     1           0   \n",
       "14394      0            0                0       0      0     0           0   \n",
       "14395      0            0                0       0      0     0           0   \n",
       "14396      0            0                0       0      0     0           0   \n",
       "14397      0            0                1       1      0     0           0   \n",
       "14398      0            0                1       1      0     0           0   \n",
       "14399      0            0                0       0      0     0           0   \n",
       "14400      0            0                0       0      0     0           0   \n",
       "14401      0            0                0       0      0     0           0   \n",
       "14402      0            0                0       0      0     0           0   \n",
       "\n",
       "       cold  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "5         0  \n",
       "6         0  \n",
       "7         0  \n",
       "8         0  \n",
       "9         0  \n",
       "10        0  \n",
       "11        0  \n",
       "12        0  \n",
       "13        0  \n",
       "14        0  \n",
       "15        0  \n",
       "16        0  \n",
       "17        0  \n",
       "18        0  \n",
       "19        0  \n",
       "20        0  \n",
       "21        0  \n",
       "22        0  \n",
       "23        0  \n",
       "24        0  \n",
       "25        0  \n",
       "26        0  \n",
       "27        0  \n",
       "28        0  \n",
       "29        0  \n",
       "...     ...  \n",
       "14373     0  \n",
       "14374     0  \n",
       "14375     0  \n",
       "14376     0  \n",
       "14377     1  \n",
       "14378     0  \n",
       "14379     0  \n",
       "14380     0  \n",
       "14381     0  \n",
       "14382     0  \n",
       "14383     0  \n",
       "14384     0  \n",
       "14385     0  \n",
       "14386     0  \n",
       "14387     0  \n",
       "14388     0  \n",
       "14389     0  \n",
       "14390     0  \n",
       "14391     0  \n",
       "14392     0  \n",
       "14393     0  \n",
       "14394     0  \n",
       "14395     0  \n",
       "14396     0  \n",
       "14397     0  \n",
       "14398     0  \n",
       "14399     0  \n",
       "14400     0  \n",
       "14401     0  \n",
       "14402     0  \n",
       "\n",
       "[14403 rows x 32 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'related' includes 14403 x value 1.\n",
      "'request' includes 4374 x value 1.\n",
      "'offer' includes 117 x value 1.\n",
      "'aid_related' includes 10729 x value 1.\n",
      "'medical_help' includes 2066 x value 1.\n",
      "'medical_products' includes 1297 x value 1.\n",
      "'search_and_rescue' includes 718 x value 1.\n",
      "'security' includes 467 x value 1.\n",
      "'military' includes 857 x value 1.\n",
      "'water' includes 1650 x value 1.\n",
      "'food' includes 2885 x value 1.\n",
      "'shelter' includes 2281 x value 1.\n",
      "'clothing' includes 401 x value 1.\n",
      "'money' includes 598 x value 1.\n",
      "'missing_people' includes 297 x value 1.\n",
      "'refugees' includes 872 x value 1.\n",
      "'death' includes 1187 x value 1.\n",
      "'other_aid' includes 3392 x value 1.\n",
      "'infrastructure_related' includes 1688 x value 1.\n",
      "'transport' includes 1197 x value 1.\n",
      "'buildings' includes 1313 x value 1.\n",
      "'electricity' includes 528 x value 1.\n",
      "'tools' includes 158 x value 1.\n",
      "'hospitals' includes 283 x value 1.\n",
      "'shops' includes 118 x value 1.\n",
      "'aid_centers' includes 308 x value 1.\n",
      "'weather_related' includes 7209 x value 1.\n",
      "'floods' includes 2130 x value 1.\n",
      "'storm' includes 2418 x value 1.\n",
      "'fire' includes 282 x value 1.\n",
      "'earthquake' includes 2421 x value 1.\n",
      "'cold' includes 528 x value 1.\n"
     ]
    }
   ],
   "source": [
    "for group in y.columns:\n",
    "    print(\"'{}' includes {} x value 1.\".format(group, y[group].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     3857\n",
       "4     3232\n",
       "5     2332\n",
       "6     1675\n",
       "7     1084\n",
       "2      742\n",
       "8      661\n",
       "9      356\n",
       "10     176\n",
       "11     125\n",
       "12      57\n",
       "13      43\n",
       "14      22\n",
       "16      11\n",
       "15      10\n",
       "17       5\n",
       "19       4\n",
       "18       4\n",
       "20       3\n",
       "25       1\n",
       "24       1\n",
       "22       1\n",
       "23       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label count 1 shall not exist anymore\n",
    "df[df.columns[4:]].iloc[:,:].sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data\n",
    "\n",
    "During EPL pipeline activities we realised that there are messages which are not useful (e.g. 'nonsense' character sequences, html characters) and there are probably web links included. We have to deal with this in the tokenize() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function from Dipanjan's repository:\n",
    "# https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%\\\n",
    "# 20content/nlp%20proven%20approach/NLP%20Strategy%20I%20-%20Processing%20and%20Understanding%20Text.ipynb\n",
    "\n",
    "def expand_contractions(text, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    \n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # have in mind that we use this for a web app adding new messages;\n",
    "    # if still html, xml or other undefined parts in the existing messages:\n",
    "    # first remove such metatext from English messages\n",
    "    # see: https://docs.python.org/3.7/library/codecs.html#encodings-and-unicode\n",
    "    # \"To be able to detect the endianness of a UTF-16 or UTF-32 byte sequence,\n",
    "    # there’s the so called BOM (“Byte Order Mark”). [...]\n",
    "    # In UTF-8, the use of the BOM is discouraged and should generally be avoided.\"\n",
    "    # specific ones are e.g. notepad signatures from Microsoft as part of the messages which should be avoided;\n",
    "    # other undefined characters have the coding of the 'replacement character' unicode u\"\\ufffd\"\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #stop_words.remove('no')\n",
    "    #stop_words.remove('not')\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'       \n",
    "    detected_urls = re.findall(url_regex, bom_removed)\n",
    "    for url in detected_urls:\n",
    "        text = bom_removed.replace(url, \"urlplaceholder\")\n",
    "        \n",
    "    # change the negation wordings like don't to do not, won't to will not \n",
    "    # or other contractions like I'd to I would, I'll to I will etc. via dictionary\n",
    "    text = expand_contractions(text, CONTRACTION_MAP)\n",
    "\n",
    "    # remove punctuation [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]\n",
    "    text = text.translate(str.maketrans('','', string.punctuation))\n",
    "    # remove numbers\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # during ETL pipeline we have reduced the dataset on English messages ('en' language coding,\n",
    "    # but there can be some wrong codings\n",
    "    tokens = word_tokenize(letters_only, language='english')\n",
    "    lemmatizer = WordNetLemmatizer()  # for the lexical correctly found word stem (root)\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        # use only lower cases, remove leading and ending spaces\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        # remember: there have been nonsense sentences, so, now some strings could be empty\n",
    "        # toDo: what is the correct length number to use now? Small ones are probably no relevant words ...\n",
    "        # remove English stop words\n",
    "        if (len(clean_tok) > 1) & (clean_tok not in stop_words):\n",
    "            clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of string with some punctuation signs\n"
     ]
    }
   ],
   "source": [
    "# example for unit test to remove punctuation [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]\n",
    "example_str = 'This [is an] example? {of} string. with.? some &punctuation &signs!!??!!'\n",
    "result = example_str.translate(str.maketrans('','', string.punctuation))\n",
    "print(result)\n",
    "# output shall be: This is an example of string with some punctuation signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the Hurricane over or is it not over\n",
      "['hurricane'] \n",
      "\n",
      "UN reports Leogane 80-90 destroyed. Only Hospital St. Croix functioning. Needs supplies desperately.\n",
      "['un', 'report', 'leogane', 'destroyed', 'hospital', 'st', 'croix', 'functioning', 'needs', 'supply', 'desperately'] \n",
      "\n",
      "Storm at sacred heart of jesus\n",
      "['storm', 'sacred', 'heart', 'jesus'] \n",
      "\n",
      "Please, we need tents and water. We are in Silo, Thank you!\n",
      "['please', 'need', 'tent', 'water', 'silo', 'thank'] \n",
      "\n",
      "I am in Croix-des-Bouquets. We have health issues. They ( workers ) are in Santo 15. ( an area in Croix-des-Bouquets )\n",
      "['croixdesbouquets', 'health', 'issue', 'worker', 'santo', 'area', 'croixdesbouquets'] \n",
      "\n",
      "There's nothing to eat and water, we starving and thirsty.\n",
      "['nothing', 'eat', 'water', 'starving', 'thirsty'] \n",
      "\n",
      "I am in Thomassin number 32, in the area named Pyron. I would like to have some water. Thank God we are fine, but we desperately need water. Thanks\n",
      "['thomassin', 'number', 'area', 'named', 'pyron', 'would', 'like', 'water', 'thank', 'god', 'fine', 'desperately', 'need', 'water', 'thanks'] \n",
      "\n",
      "Let's do it together, need food in Delma 75, in didine area\n",
      "['let', 'together', 'need', 'food', 'delma', 'didine', 'area'] \n",
      "\n",
      "A Comitee in Delmas 19, Rue ( street ) Janvier, Impasse Charite #2. We have about 500 people in a temporary shelter and we are in dire need of Water, Food, Medications, Tents and Clothes. Please stop by and see us.\n",
      "['comitee', 'delmas', 'rue', 'street', 'janvier', 'impasse', 'charite', 'people', 'temporary', 'shelter', 'dire', 'need', 'water', 'food', 'medications', 'tents', 'clothes', 'please', 'stop', 'see'] \n",
      "\n",
      "We need food and water in Klecin 12. We are dying of hunger. Impasse Chretien Klecin 12 extended ( extension ) We are hungry and sick.\n",
      "['need', 'food', 'water', 'klecin', 'dying', 'hunger', 'impasse', 'chretien', 'klecin', 'extended', 'extension', 'hungry', 'sick'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test tokenize\n",
    "for message in X['message'][:10]:\n",
    "    tokens = tokenize(message)\n",
    "    print(message)\n",
    "    print(tokens, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "Notes:\n",
    "- Regarding the class default parameters, for this Python implementation scikit-learn version 0.21.2 is used.\n",
    "- We use np.random.seed() beside of random_state/random_seed parameters ([reason](https://stackoverflow.com/questions/47923258/random-seed-on-svm-sklearn-produces-different-results))\n",
    "- For the pipeline workflow a `FeatureUnion`instance concatenates results of multiple transformer objects\n",
    "\n",
    "Remember, we are dealing with an imbalanced dataset, therefore not all models can be used. A machine learning classifier could be more biased towards the majority class, causing bad classification of the minority class. Therefore we have to take care. We start with <i>LogisticRegression</i> and use other appropriate models later in this Python implementation. If the metric evaluation of the used models shows issues, we have to change our dataset e.g. doing undersampling or oversampling getting a more balanced dataset.\n",
    "\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other remaining 31 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables.\n",
    "\n",
    "As its first estimator we use [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression). Its default parameter values are:<br>\n",
    "LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’auto’, verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "We have to solve a supervised, multi-class, multi-label problem, therefore some parameters have to be changed:<br>\n",
    "- solver = 'saga'  (handles L1 and L2, according scikit-learn documentation it is often the best choice)\n",
    "- multi_class = 'multinomial'\n",
    "- C = the optimal value for the inverse of regularization strength is going to be set later, in the cross validation optimisation subchapter of this project.\n",
    "\n",
    "\n",
    "Regarding feature extraction:<br>\n",
    "For having a measure of the word frequency of each text term the <i>Term Frequency - Inverse Document Frequency</i> class exists in the library scikit-learn with 2 types - vectoriser and transformer. The used class [TfidTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer) has the default parameters:<br>\n",
    "TfidfTransformer(norm=’l2’, use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "Both pipeline classes <i>TfidTransformer</i> and <i>LogisticRegression</i> use a L2 normalisation for scaling. Therefore, the amount of words has no influence on our result. As it is stated in the scikit-learn [documentation](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.html#sphx-glr-auto-examples-linear-model-plot-sparse-logistic-regression-20newsgroups-py), \"if the goal is to get the best predictive accuracy, it is better to use the non sparsity-inducing l2 penalty instead.\" All feature vectors have an euclidian norm of 1.<br>\n",
    "The text messages are transformed to a number vector representation used to train supervised classifiers able to predict the associated categories of future, new messages.\n",
    "\n",
    "The usage of other machine learning models for imbalanced datasets, like Linear Support Vector Machine or Multinomial Naive Bayes classification model or Random Forest ensemble classification model, as well as model parameter optimisation is part of the project subchapters below. Have in mind that text data are being part of the higher dimensional bag-of-words spaces and there Euclidean distance does not work well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([ \n",
    "            \n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize, ngram_range=(1,2))),\n",
    "                ('tfidf', TfidfTransformer(sublinear_tf=True)),\n",
    "            ]))\n",
    "            \n",
    "        ])),\n",
    "    \n",
    "        ('clf', MultiOutputClassifier(LogisticRegression(solver='saga', multi_class='auto',\n",
    "                                                         max_iter=500, class_weight='balanced', \n",
    "                                                         n_jobs=-1, random_state=FIXED_SEED)))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the pipeline on the whole dataset, do single prediction in default form with its components for an example disaster message of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14403, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14403, 32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle is by default set on True,\n",
    "# usage of stratify param leads to stratify split technique for this imbalanced dataset,\n",
    "# having both would be a StratifiedShuffleSplit algorithm in the background,\n",
    "# but\n",
    "# stratify=y leads to a ValueError: The least populated class in y has only 1 member, which is too few.\n",
    "# The minimum number of groups for any class cannot be less than 2.\n",
    "# ToDo: clarify why => must be: stratify=y.iloc[:,1]\n",
    "# Therefore I added randomised resampling to reduce the probability of getting ValueError's during model fit()\n",
    "# and after clarification comment them out\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y.iloc[:,1],\n",
    "                                                    test_size=0.2, random_state=FIXED_SEED)\n",
    "#X_train = X_train.sample(n = X_train.shape[0], axis=0, random_state=FIXED_SEED) \n",
    "#y_train = y_train.sample(n = y_train.shape[0], axis=0, random_state=FIXED_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11522, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11522, 32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train the pipeline ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 11522]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-58e3b7912133>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    354\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[0;32m    355\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    150\u001b[0m         X, y = check_X_y(X, y,\n\u001b[0;32m    151\u001b[0m                          \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m                          accept_sparse=True)\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 205\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 11522]"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And calculate the LogisticRegression model prediction ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_logr_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_logr_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pred = pd.DataFrame(y_logr_pred, columns=y_test.columns)\n",
    "#df_pred.to_csv(\"y_logr_pred_file.csv\")  \n",
    "#y_test.to_csv(\"y_logr_test_file.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there differences in the test dataFrame and in y_pred, means exists a test item without corresponding prediction\n",
    "#set(y_test) - set(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "For evaluation:<br>\n",
    "Report accuracy score, f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each, where:\n",
    "\n",
    "TP = TruePositive; FP = FalsePositive; TN = TrueNegative; FN = FalseNegative.\n",
    "\n",
    "**Accuracy Score** is a classification score. It is the number of correct predictions made divided by the total number of predictions made. In a multilabel classification task it computes subset accuracy. \n",
    "  \n",
    "Furthermore, beside accuracy, we add additional metrics to compare the model performance having an originally imbalanced dataset. Accuracy would focus too much on the majority classes. Because of this overfitting of the majority classes, its value would be too good and therefore misleading.\n",
    "\n",
    "**Precision** quantifies the binary precision. In other words, a measure of a classifiers exactness. It is a ratio of true positives (messages correctly classified to their categories)) to all positives (all messages classified to categories, irrespective of whether that was the correct classification), in other words it is the ratio of\n",
    "\n",
    "TP / (TP + FP)\n",
    "\n",
    "**Recall** tells us what proportion of messages that actually were classified to specific categories were classified by us as this categories. Means, a measure of a classifiers completeness. It is a ratio of true positives to all the correctly category classified messages that were actually disaster messages, in other words it is the ratio of\n",
    "\n",
    "TP / (TP + FN)\n",
    "\n",
    "A model's ability to precisely predict those that are correctly categoriesed disaster messages is more important than the model's ability to recall those individuals. \n",
    "\n",
    "We can use **F-beta score** as a metric that considers both precision and recall. According scikit-learn, the F-beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0. F – Measure is nothing but the harmonic mean of Precision and Recall.\n",
    "\n",
    "Fβ=(1 + β2)  (precision⋅recall / ((β2⋅precision) + recall))\n",
    "\n",
    "In particular, when β=0.5, more emphasis is placed on precision. And when β=1.0 recall and precision are equally important.\n",
    "\n",
    "According scikit-learn: \"The **F1 score** ... reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "In the multi-class and multi-label case, this is the average of the F1 score of each class with weighting depending on the average parameter.\"\n",
    "\n",
    "From scikit-learn documentation for the classification report:<br>\n",
    "The classification_report() function returns an additional value: **Support** - the number of occurrences of each label in y_true.<br>\n",
    "The reported averages include macro average (averaging the unweighted mean per label), weighted average (averaging the support-weighted mean per label), sample average (only for multilabel classification) and micro average (averaging the total true positives, false negatives and false positives) it is only shown for multi-label or multi-class with a subset of classes because it is accuracy otherwise.\n",
    "\n",
    "Note: Having the imbalanced dataset in mind, Cohen's Kappa and Confusion Matrix are not possible because this is a multi-label classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(y_test, y_pred, cv=None):\n",
    "    # text summary of the overall accuracy, precision, recall, F1 score for each class   \n",
    "    print(\"First: overall accuracy score: {:5f}\".format(accuracy_score(y_test, y_pred)))\n",
    "    \n",
    "    target_names = y_test.columns\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "    # shows F1_score, precision and recall\n",
    "    class_report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "    print(\"Classification Report for each target class:\\n\", class_report)\n",
    "\n",
    "    if cv != None:\n",
    "        print(\"\\n\\n---- Best Parameters: ----\\n{}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results(y_test, y_logr_pred, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such kind of behaviour has been ..... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters for grid search\n",
    "parameters = {\n",
    "    'features__text_pipeline__vect__ngram_range': [(1, 2), (1,3)],\n",
    "    'features__text_pipeline__vect__max_df': [0.5, 0.75, 1.0],\n",
    "    'features__text_pipeline_tfidf_sublinear_tf': [True],\n",
    "    'clf__estimator__classifier__C' : [0.01, 0.1, 1, 10, 50, 100, 200],  \n",
    "    'clf__estimator__classifier__max_iter': [1000, 1500, 3000, 5000]\n",
    "}\n",
    "\n",
    "# create grid search object\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV\n",
    "grid_cv = GridSearchCV(pipeline, param_grid=parameters, n_jobs=-1, cv=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, recall and F-score of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 75.0min\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed: 157.0min\n",
      "[Parallel(n_jobs=4)]: Done 280 out of 280 | elapsed: 442.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('features',\n",
       "                                        FeatureUnion(n_jobs=None,\n",
       "                                                     transformer_list=[('text_pipeline',\n",
       "                                                                        Pipeline(memory=None,\n",
       "                                                                                 steps=[('vect',\n",
       "                                                                                         CountVectorizer(analyzer='word',\n",
       "                                                                                                         binary=False,\n",
       "                                                                                                         decode_error='strict',\n",
       "                                                                                                         dtype=<class 'numpy.int64'>,\n",
       "                                                                                                         encoding='utf-8',\n",
       "                                                                                                         input='content',\n",
       "                                                                                                         lowercase=True,\n",
       "                                                                                                         max_df=1.0,\n",
       "                                                                                                         m...\n",
       "                                                                                           solver='saga',\n",
       "                                                                                           tol=0.0001,\n",
       "                                                                                           verbose=0,\n",
       "                                                                                           warm_start=False),\n",
       "                                                              n_jobs=None))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=4,\n",
       "             param_grid={'clf__estimator__C': [0.001, 0.01, 0.1, 1, 10, 100,\n",
       "                                               1000],\n",
       "                         'clf__estimator__max_iter': [1000, 1500, 3000, 5000],\n",
       "                         'features__text_pipeline__vect__ngram_range': [(1, 1),\n",
       "                                                                        (1,\n",
       "                                                                         2)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = cv\n",
    "grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_logr_pred = grid_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for the cross validation tuned 'Logistic Regression' estimator:\n",
      "First: overall accuracy score: 0.086576\n",
      "Classification Report for each target class:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "               request       0.55      0.47      0.51      1082\n",
      "                 offer       0.00      0.00      0.00        31\n",
      "           aid_related       0.57      0.60      0.58      2700\n",
      "          medical_help       0.20      0.01      0.03       564\n",
      "      medical_products       0.06      0.00      0.01       317\n",
      "     search_and_rescue       0.50      0.01      0.02       178\n",
      "              security       0.00      0.00      0.00       118\n",
      "              military       0.14      0.00      0.01       213\n",
      "           child_alone       0.00      0.00      0.00        10\n",
      "                 water       0.10      0.01      0.02       409\n",
      "                  food       0.26      0.09      0.13       722\n",
      "               shelter       0.15      0.03      0.04       554\n",
      "              clothing       0.33      0.01      0.02       104\n",
      "                 money       0.00      0.00      0.00       127\n",
      "        missing_people       0.00      0.00      0.00        72\n",
      "              refugees       0.10      0.00      0.01       215\n",
      "                 death       0.06      0.00      0.01       304\n",
      "             other_aid       0.20      0.05      0.08       860\n",
      "infrastructure_related       0.16      0.01      0.02       415\n",
      "             transport       0.08      0.00      0.01       305\n",
      "             buildings       0.05      0.00      0.01       339\n",
      "           electricity       0.00      0.00      0.00       138\n",
      "                 tools       0.00      0.00      0.00        34\n",
      "             hospitals       0.00      0.00      0.00        76\n",
      "                 shops       0.00      0.00      0.00        28\n",
      "           aid_centers       0.00      0.00      0.00        72\n",
      "       weather_related       0.52      0.43      0.47      1781\n",
      "                floods       0.20      0.03      0.06       521\n",
      "                 storm       0.43      0.17      0.24       623\n",
      "                  fire       0.00      0.00      0.00        75\n",
      "            earthquake       0.55      0.22      0.31       575\n",
      "                  cold       0.12      0.01      0.02       122\n",
      "\n",
      "             micro avg       0.50      0.24      0.32     13684\n",
      "             macro avg       0.17      0.07      0.08     13684\n",
      "          weighted avg       0.34      0.24      0.26     13684\n",
      "           samples avg       0.40      0.20      0.24     13684\n",
      "\n",
      "\n",
      "\n",
      "---- Best Parameters: ----\n",
      "{'clf__estimator__C': 100, 'clf__estimator__max_iter': 1000, 'features__text_pipeline__vect__ngram_range': (1, 2)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation results for the cross validation tuned 'Logistic Regression' estimator:\")\n",
    "display_results(y_test, y_logr_pred, grid_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation result is a little bit better, but still includes categories with 0.0 values. These are the group of categories with supported observation numbers < 150. So, metrics have not been calculated properly. Additionally, in general it is still the case that the precision values are higher compared to the recall values, means a lot of false negatives. We have to improve the model conditions again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First**, we try out other machine learning algorithms which are tuned by cross validation to compare their prediction results. Further models are:\n",
    "- Naïve Bayes: `MultinomialNB`<br>\n",
    "    The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g. word counts for text classification). Its default setting is: MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None). The parameter 'alpha' is its \"Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\"\n",
    "    \n",
    "- Support Vector Machines (regular `LinearSVC`)<br>\n",
    "  Linear Support Vector Classification default setting is: LinearSVC(penalty=’l2’, loss=’squared_hinge’, dual=True, tol=0.0001, C=1.0, multi_class=’ovr’, fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)<br>\n",
    "  According scikit-learn the parameter multi_class (string ‘ovr’ or ‘crammer_singer’ (default=’ovr’)): Determines the multi-class strategy if y contains more than two classes. \"ovr\" trains n_classes one-vs-rest classifiers, while \"crammer_singer\" optimizes a joint objective over all classes. While crammer_singer is interesting from a theoretical perspective as it is consistent, it is seldom used in practice as it rarely leads to better accuracy and is more expensive to compute. If \"crammer_singer\" is chosen, the options loss, penalty and dual will be ignored.<br>\n",
    "  And the parameter C is the penalty parameter of the error term.\n",
    "\n",
    "- Ensemble Method: `RandomForestClassifier`<br>\n",
    "    A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control overfitting.<br>\n",
    "    Its default setting is: RandomForestClassifier(n_estimators=’warn’, criterion=’gini’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
    "    \n",
    "**Second**, because it is an imbalanced dataset we do a balancing before classification. The categority classes with low numbers of observations are outnumbered. So, the dataset is highly skewed. To create a balanced dataset several strategies exists:\n",
    "- Undersampling the majority classes\n",
    "- Oversampling the minority classes\n",
    "- Combining over- and under-sampling\n",
    "- Create ensemble balanced sets\n",
    "\n",
    "But have in mind, that minority class oversampling could result in overfitting problems doing it before cross-validation. We would link the information of validation data to our training dataset which is forbidden.\n",
    "\n",
    "Note:<br>\n",
    "Doing balancing activities the scikit package 'imbalanced-learn' is imported.<br>\n",
    "For combining the strategies we implement a naive random oversampling of the minority classes.<br>\n",
    "For undersampling the package can be used as well to create the pipeline with `PipelineImb`. The pipeline itself includes the class `RandomUnderSampler()` directly before the MultiOutputClassifier to equalize the number of samples in all the classes before the training.\n",
    "\n",
    "Using such package throws the following ValueError: 'Imbalanced-learn currently supports binary, multiclass and binarized encoded multiclasss targets. Multilabel and multioutput targets are not supported.' So, the associated package classes do not support the multi-target classification we need for our project. Therefore this coding is removed.\n",
    "\n",
    "According the [paper](https://arxiv.org/ftp/arxiv/papers/1810/1810.11612.pdf) <i>Handling Imbalanced Dataset in Multi-label Text Categorization using Bagging and Adaptive Boosting</i> of 27 October 2018 from Genta Indra Winata and Masayu Leylia Khodra, regarding new data, it is more appropriate to balance the dataset on the algorithm level instead of the data level to avoid overfitting. The algorithm \"approach modifies algorithm by adjusting weight or cost of various classes.\"\n",
    "\n",
    "E.g. the `RandomForestClassifier` is an ensemble model estimator in which each tree of the forest will be provided a balanced bootstrap sample if the class weight attribute is set appropriately. We will have a look if this configuration is already enough to get a good model performance.<br>\n",
    "\n",
    "If this is not the case, could we use a variant of the popular method called `SMOTE` (Synthetic Minority Oversampling Technique)? It creates synthetic samples from the minor category classes rather of creating copies like the naive random oversampling method. According Max Kuhn and Kjell Johnson in their book 'Applied Predictive Modeling', chapter 16.7 'Sampling Methods' (as part of chapter 16 'Remedies of Severe Class Imbalance') SMOTE is a sampling procedure using both, up-sampling and down-sampling.\n",
    "\n",
    "Note: To use the 'RandomForestClassifer' has not been enough. The improvement has been too small. And the usage of another meta-estimator ('BoostingClassifier') directly for the other base-estimator models leads to an endless loop of ConvergenceWarning's during fit() action. The ensemble BaggingClassifier, according scikit-learn is a \"meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction.\" It seems as if this is not working for a multi-label classification task.\n",
    "\n",
    "The ideal solution for the drastical differences of target class observations would be to get more data for them, but this is not the case for this project. So, we try the synthetic creation of new data points of the minority classes by SMOTE technique. But according the [research paper](https://docplayer.net/13735758-Preprocessing-imbalanced-dataset-using-oversampling-approach.html) of Dasharath C.Magar and S.M.Rokade called <i>Preprocessing Imbalanced Dataset Using Oversampling Approach</i> we have to take care which SMOTE variant to use, some methods could be inappropriate. They propose three steps to work on:\n",
    "- Selection of  an  appropriate subset of the original minority class samples\n",
    "- Assigning weights to the selected samples according to their importance in the data\n",
    "- Using a clustering approach for generating the useful synthetic minority class samplese\n",
    "\n",
    "So again, could we try to improve the dataset with the Python library ['smote_variants'](https://smote-variants.readthedocs.io/en/latest/index.html) described in the [paper](https://www.sciencedirect.com/science/article/pii/S0925231219311622) <i>Smote-variants: A python implementation of 85 minority oversampling techniques</i> from György Kovács, e.g. running sampling via ('MWMOTE', \"{'proportion': 0.7, 'k1': 5, 'k2': 5, 'k3': 5, 'M': 10, 'cf_th': 5.0, 'cmax': 10.0, 'n_jobs': 4, 'random_state': 42}\") on our 'X' and 'y' datasets? No, we cannot, because this are text data (string data), so, we will get ValueError's like: could not convert string to float: 'Weather update - a cold front from Cuba that could pass over Haiti'.\n",
    "\n",
    "**Finally**, as possible improvement we use other multi-label classifiers from the `scikit-multilearn` package. They transform our task into multiple single-label actions. As a result, existing single-label models can be used. One possible solution is using [binary relevance](http://scikit.ml/api/skmultilearn.problem_transform.br.html#skmultilearn.problem_transform.BinaryRelevance) which is possible because there are no label correlations anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_type, params):\n",
    "    ''' \n",
    "    input:\n",
    "    model_type - the estimator model used for the MultiOutputClassifier\n",
    "    params - the estimator model parameter grid used for the GridSearchCV \n",
    "    ''' \n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([ \n",
    "            \n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "            ]))           \n",
    "        ])),\n",
    "\n",
    "        ('clf', MultiOutputClassifier(model_type))\n",
    "    ])\n",
    "\n",
    "    #return pipeline.get_params()\n",
    "    \n",
    "    # the higher the verbose number the more information is thrown\n",
    "    # cv not higher than 5 buckets, child_alone support has been only 10\n",
    "    cv = GridSearchCV(pipeline, param_grid=params, n_jobs=4, cv=10, verbose=1) \n",
    "    \n",
    "    return cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display the evaluation metric results this time we use the classification report version for imbalanced datasets. Its metrics are: precision/recall/specificity, geometric mean, and index balanced accuracy of the geometric mean. Additionally to that, the 'normal' accuracy score and the best found parameter set for the model are given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create param grids for the models\n",
    "# during former testing with different params, ngrams of (1,2) have been the best compared to (1,1)\n",
    "mnb_param_grid = {\n",
    "    'features__text_pipeline__vect__ngram_range': [(1, 2), (1,3)],\n",
    "    'features__text_pipeline__vect__max_df': [0.5, 0.75, 1.0],\n",
    "    'features__text_pipeline_tfidf_sublinear_tf': [False, True],\n",
    "    'clf__estimator__classifier__alpha': [1e-5, 1e-4, 1e-2, 1e-1, 1]\n",
    "}\n",
    "\n",
    "svm_param_grid = {\n",
    "    'features__text_pipeline__vect__ngram_range': [(1, 2), (1,3)],\n",
    "    'features__text_pipeline__vect__max_df': [0.5, 0.75, 1.0],\n",
    "    'features__text_pipeline_tfidf_sublinear_tf': [False, True],\n",
    "    'clf__estimator__classifier__C': [0.01, 0.1, 1, 1.5, 5],\n",
    "    'clf__estimator__classifier__multi_class': ['ovr', 'crammer_singer'],\n",
    "    'clf__estimator__classifier__max_iter': [1000, 1500, 3000, 5000]\n",
    "}\n",
    "\n",
    "rfc_param_grid = {\n",
    "    'features__text_pipeline__vect__ngram_range': [(1, 2), (1,3)],\n",
    "    'features__text_pipeline__vect__max_df': [0.5, 0.75, 1.0],\n",
    "    'features__text_pipeline_tfidf_sublinear_tf': [False, True],\n",
    "    'clf__estimator__classifier__n_estimators': [10, 100, 500, 1000, 1500],\n",
    "    'clf__estimator__classifier__max_depth': [3, 5, 10],\n",
    "    'clf__estimator__classifier__class_weight': ['balanced', 'balanced_subsample']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n----- MultinomialNB -----\")\n",
    "print(\"Build best model: ...\")\n",
    "cv_mnb_model = build_model(MultinomialNB(random_state=FIXED_SEED), mnb_param_grid)\n",
    "print(\"Train model: ...\")\n",
    "cv_mnb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mnb_pred = cv_mnb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model evaluation: ...\n",
      "First: accuracy score: 0.1901954700589513\n",
      "Classification Report for each target class:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "               related       0.76      1.00      0.86      4880\n",
      "               request       0.00      0.00      0.00      1119\n",
      "                 offer       0.00      0.00      0.00        29\n",
      "           aid_related       0.42      0.02      0.04      2673\n",
      "          medical_help       0.00      0.00      0.00       514\n",
      "      medical_products       0.00      0.00      0.00       340\n",
      "     search_and_rescue       0.00      0.00      0.00       169\n",
      "              security       0.00      0.00      0.00       110\n",
      "              military       0.00      0.00      0.00       205\n",
      "           child_alone       0.00      0.00      0.00         6\n",
      "                 water       0.00      0.00      0.00       406\n",
      "                  food       0.00      0.00      0.00       741\n",
      "               shelter       0.00      0.00      0.00       615\n",
      "              clothing       0.00      0.00      0.00        93\n",
      "                 money       0.00      0.00      0.00       137\n",
      "        missing_people       0.00      0.00      0.00        69\n",
      "              refugees       0.00      0.00      0.00       201\n",
      "                 death       0.00      0.00      0.00       289\n",
      "             other_aid       0.00      0.00      0.00       848\n",
      "infrastructure_related       0.00      0.00      0.00       428\n",
      "             transport       0.00      0.00      0.00       302\n",
      "             buildings       0.00      0.00      0.00       351\n",
      "           electricity       0.00      0.00      0.00       136\n",
      "                 tools       0.00      0.00      0.00        42\n",
      "             hospitals       0.00      0.00      0.00        73\n",
      "                 shops       0.00      0.00      0.00        26\n",
      "           aid_centers       0.00      0.00      0.00        83\n",
      "       weather_related       0.80      0.04      0.08      1779\n",
      "                floods       0.00      0.00      0.00       535\n",
      "                 storm       0.00      0.00      0.00       608\n",
      "                  fire       0.00      0.00      0.00        74\n",
      "            earthquake       0.81      0.02      0.04       593\n",
      "                  cold       0.00      0.00      0.00       127\n",
      "\n",
      "             micro avg       0.75      0.27      0.40     18601\n",
      "             macro avg       0.08      0.03      0.03     18601\n",
      "          weighted avg       0.36      0.27      0.24     18601\n",
      "           samples avg       0.75      0.34      0.41     18601\n",
      "\n",
      "\n",
      "\n",
      "---- Best Parameters: ----\n",
      "{'clf__estimator__alpha': 1, 'features__text_pipeline__vect__ngram_range': (1, 2)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel evaluation on tuned MultinomialNB ...\")\n",
    "display_results_imbalanced(y_test, y_mnb_pred, cv_mnb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- LinearSVC -----\n",
      "Build best model: ...\n",
      "Train model: ...\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed: 51.7min\n",
      "[Parallel(n_jobs=4)]: Done 400 out of 400 | elapsed: 174.4min finished\n",
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('features',\n",
       "                                        FeatureUnion(n_jobs=None,\n",
       "                                                     transformer_list=[('text_pipeline',\n",
       "                                                                        Pipeline(memory=None,\n",
       "                                                                                 steps=[('vect',\n",
       "                                                                                         CountVectorizer(analyzer='word',\n",
       "                                                                                                         binary=False,\n",
       "                                                                                                         decode_error='strict',\n",
       "                                                                                                         dtype=<class 'numpy.int64'>,\n",
       "                                                                                                         encoding='utf-8',\n",
       "                                                                                                         input='content',\n",
       "                                                                                                         lowercase=True,\n",
       "                                                                                                         max_df=1.0,\n",
       "                                                                                                         m...\n",
       "                                                        require_dense=[False,\n",
       "                                                                       True]))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=4,\n",
       "             param_grid={'clf__classifier__C': [0.01, 0.1, 1, 1.5, 5],\n",
       "                         'clf__classifier__max_iter': [1000, 1500, 3000, 5000],\n",
       "                         'clf__classifier__multi_class': ['ovr',\n",
       "                                                          'crammer_singer'],\n",
       "                         'features__text_pipeline__vect__ngram_range': [(1, 2),\n",
       "                                                                        (1,\n",
       "                                                                         3)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n----- LinearSVC -----\")\n",
    "print(\"Build best model: ...\")\n",
    "cv_svm_model = build_model(LinearSVC(random_state=FIXED_SEED), svm_param_grid)\n",
    "print(\"Train model: ...\")\n",
    "cv_svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_svm_pred = cv_svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model evaluation on tuned LinearSVC: ...\n",
      "First: overall accuracy score: 0.088205\n",
      "Classification Report for each target class:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "               request       0.55      0.48      0.51      1082\n",
      "                 offer       0.00      0.00      0.00        31\n",
      "           aid_related       0.57      0.60      0.59      2700\n",
      "          medical_help       0.25      0.01      0.03       564\n",
      "      medical_products       0.10      0.00      0.01       317\n",
      "     search_and_rescue       0.67      0.01      0.02       178\n",
      "              security       0.00      0.00      0.00       118\n",
      "              military       0.11      0.00      0.01       213\n",
      "           child_alone       0.00      0.00      0.00        10\n",
      "                 water       0.05      0.00      0.00       409\n",
      "                  food       0.31      0.08      0.13       722\n",
      "               shelter       0.16      0.02      0.04       554\n",
      "              clothing       0.25      0.01      0.02       104\n",
      "                 money       0.00      0.00      0.00       127\n",
      "        missing_people       0.00      0.00      0.00        72\n",
      "              refugees       0.11      0.00      0.01       215\n",
      "                 death       0.00      0.00      0.00       304\n",
      "             other_aid       0.19      0.04      0.07       860\n",
      "infrastructure_related       0.14      0.01      0.02       415\n",
      "             transport       0.08      0.00      0.01       305\n",
      "             buildings       0.06      0.00      0.01       339\n",
      "           electricity       0.00      0.00      0.00       138\n",
      "                 tools       0.00      0.00      0.00        34\n",
      "             hospitals       0.00      0.00      0.00        76\n",
      "                 shops       0.00      0.00      0.00        28\n",
      "           aid_centers       0.00      0.00      0.00        72\n",
      "       weather_related       0.51      0.45      0.48      1781\n",
      "                floods       0.18      0.03      0.06       521\n",
      "                 storm       0.44      0.17      0.25       623\n",
      "                  fire       0.00      0.00      0.00        75\n",
      "            earthquake       0.57      0.23      0.33       575\n",
      "                  cold       0.12      0.01      0.02       122\n",
      "\n",
      "             micro avg       0.51      0.24      0.33     13684\n",
      "             macro avg       0.17      0.07      0.08     13684\n",
      "          weighted avg       0.35      0.24      0.26     13684\n",
      "           samples avg       0.40      0.20      0.25     13684\n",
      "\n",
      "\n",
      "\n",
      "---- Best Parameters: ----\n",
      "{'clf__classifier__C': 1, 'clf__classifier__max_iter': 1000, 'clf__classifier__multi_class': 'crammer_singer', 'features__text_pipeline__vect__ngram_range': (1, 2)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel evaluation on tuned LinearSVC: ...\")\n",
    "display_results(y_test, y_svm_pred, cv_svm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- RandomForestClassifier -----\n",
      "Build model: ...\n",
      "Train model: ...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed: 187.5min\n",
      "[Parallel(n_jobs=3)]: Done 180 out of 180 | elapsed: 500.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('features',\n",
       "                                        FeatureUnion(n_jobs=None,\n",
       "                                                     transformer_list=[('text_pipeline',\n",
       "                                                                        Pipeline(memory=None,\n",
       "                                                                                 steps=[('vect',\n",
       "                                                                                         CountVectorizer(analyzer='word',\n",
       "                                                                                                         binary=False,\n",
       "                                                                                                         decode_error='strict',\n",
       "                                                                                                         dtype=<class 'numpy.int64'>,\n",
       "                                                                                                         encoding='utf-8',\n",
       "                                                                                                         input='content',\n",
       "                                                                                                         lowercase=True,\n",
       "                                                                                                         max_df=1.0,\n",
       "                                                                                                         m...\n",
       "                                                              n_jobs=None))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=3,\n",
       "             param_grid={'clf__estimator__class_weight': ['balanced',\n",
       "                                                          'balanced_subsample'],\n",
       "                         'clf__estimator__max_depth': [None, 3, 5],\n",
       "                         'clf__estimator__n_estimators': [10, 100, 200],\n",
       "                         'features__text_pipeline__vect__ngram_range': [(1, 1),\n",
       "                                                                        (1,\n",
       "                                                                         2)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n----- RandomForestClassifier -----\")\n",
    "print(\"Build best model: ...\")\n",
    "cv_rfc_model = build_model(RandomForestClassifier(random_state=FIXED_SEED), rfc_param_grid)\n",
    "print(\"Train model: ...\")\n",
    "cv_rfc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rfc_pred = cv_rfc_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model evaluation RandomForestClassifier: ...\n",
      "First: accuracy score: 0.097576\n",
      "Classification Report for each target class:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "               request       0.53      0.36      0.43      1082\n",
      "                 offer       0.00      0.00      0.00        31\n",
      "           aid_related       0.58      0.57      0.57      2700\n",
      "          medical_help       0.40      0.01      0.02       564\n",
      "      medical_products       0.10      0.00      0.01       317\n",
      "     search_and_rescue       0.00      0.00      0.00       178\n",
      "              security       0.00      0.00      0.00       118\n",
      "              military       0.00      0.00      0.00       213\n",
      "           child_alone       0.00      0.00      0.00        10\n",
      "                 water       0.00      0.00      0.00       409\n",
      "                  food       0.29      0.02      0.04       722\n",
      "               shelter       0.11      0.01      0.01       554\n",
      "              clothing       0.50      0.01      0.02       104\n",
      "                 money       0.00      0.00      0.00       127\n",
      "        missing_people       0.00      0.00      0.00        72\n",
      "              refugees       0.00      0.00      0.00       215\n",
      "                 death       0.00      0.00      0.00       304\n",
      "             other_aid       0.17      0.01      0.03       860\n",
      "infrastructure_related       0.00      0.00      0.00       415\n",
      "             transport       0.00      0.00      0.00       305\n",
      "             buildings       0.00      0.00      0.00       339\n",
      "           electricity       0.00      0.00      0.00       138\n",
      "                 tools       0.00      0.00      0.00        34\n",
      "             hospitals       0.00      0.00      0.00        76\n",
      "                 shops       0.00      0.00      0.00        28\n",
      "           aid_centers       0.00      0.00      0.00        72\n",
      "       weather_related       0.55      0.33      0.41      1781\n",
      "                floods       0.11      0.01      0.01       521\n",
      "                 storm       0.52      0.09      0.15       623\n",
      "                  fire       0.00      0.00      0.00        75\n",
      "            earthquake       0.69      0.19      0.30       575\n",
      "                  cold       0.00      0.00      0.00       122\n",
      "\n",
      "             micro avg       0.54      0.20      0.29     13684\n",
      "             macro avg       0.14      0.05      0.06     13684\n",
      "          weighted avg       0.34      0.20      0.23     13684\n",
      "           samples avg       0.40      0.17      0.22     13684\n",
      "\n",
      "\n",
      "\n",
      "---- Best Parameters: ----\n",
      "{'clf__estimator__class_weight': 'balanced_subsample', 'clf__estimator__max_depth': None, 'clf__estimator__n_estimators': 10, 'features__text_pipeline__vect__ngram_range': (1, 1)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel evaluation RandomForestClassifier: ...\")\n",
    "display_results(y_test, y_rfc_pred, cv_rfc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the evaluation results the best model ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ... best evaluated model with its best params ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, having found the best model from our model selection list, we save this model with its best parameters as a pickle file. Pickle is the standard way of serialising objects in Python. With this pickle file we can deserialise our model and use it to make new predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_filepath):\n",
    "    pickle.dump(model, open(model_filepath, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see train_classifier.py file\n",
    "model_filepath = \"classifier.p\"\n",
    "print('Saving model...\\n    MODEL: {}'.format(model_filepath))\n",
    "save_model(model, model_filepath)\n",
    "\n",
    "print('Best trained model saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train_classifier.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
