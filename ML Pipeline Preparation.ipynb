{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.<br>\n",
    "\n",
    "My general notes:<br>\n",
    "Have in mind, that we work on a multi-class, multi-label text classification which assigns to each message sample a set of category target labels. The messages are short and an imbalanced data distribution exists. The dataset has 19634 data points with 32 different target categories.\n",
    "\n",
    "During the disaster messages processing, the English text is tokenized, lower cased, lemmatized and the contractions are expanded. Additionally, e.g. spaces, punctuation and English stop words are removed.\n",
    "\n",
    "\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: once\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# import libraries\n",
    "#\n",
    "\n",
    "# download necessary NLTK data\n",
    "import nltk\n",
    "#nltk.download(['punkt', 'wordnet', 'stopwords'])\n",
    "\n",
    "# import libraries\n",
    "import random as rn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import pickle\n",
    "from sqlalchemy import create_engine\n",
    "import Contractions\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# warnings status to show\n",
    "import warnings\n",
    "warnings.warn(\"once\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the code reproducible ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_SEED = 42\n",
    "\n",
    "# The below is necessary for starting NumPy generated random numbers in a well-defined initial state.\n",
    "np.random.seed(FIXED_SEED)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers in a well-defined state.\n",
    "rn.seed(FIXED_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 19634 data points with 40 variables each.\n"
     ]
    }
   ],
   "source": [
    "# load data from database\n",
    "try:\n",
    "    engine = create_engine('sqlite:///Disaster_Messages_engine.db')\n",
    "    df = pd.read_sql_table('Messages_Categories_table', engine)\n",
    "    \n",
    "    # success\n",
    "    print(\"The dataset has {} data points with {} variables each.\".format(*df.shape))\n",
    "except:\n",
    "    print(\"The database 'Disaster_Messages_engine.db' could not be loaded. No ML pipeline activities possible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>lang_code</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Storm at sacred heart of jesus</td>\n",
       "      <td>Cyclone Coeur sacr de jesus</td>\n",
       "      <td>direct</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  \\\n",
       "0  Weather update - a cold front from Cuba that c...   \n",
       "1            Is the Hurricane over or is it not over   \n",
       "2  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "3  says: west side of Haiti, rest of the country ...   \n",
       "4                     Storm at sacred heart of jesus   \n",
       "\n",
       "                                            original   genre lang_code  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        en   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        en   \n",
       "2  UN reports Leogane 80-90 destroyed. Only Hospi...  direct        en   \n",
       "3  facade ouest d Haiti et le reste du pays aujou...  direct        en   \n",
       "4                        Cyclone Coeur sacr de jesus  direct        en   \n",
       "\n",
       "   related  request  offer  aid_related  medical_help  medical_products  ...  \\\n",
       "0        1        0      0            0             0                 0  ...   \n",
       "1        1        0      0            1             0                 0  ...   \n",
       "2        1        1      0            1             0                 1  ...   \n",
       "3        1        0      0            0             0                 0  ...   \n",
       "4        1        0      0            0             0                 0  ...   \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "1            0                     0                1       0      1     0   \n",
       "2            0                     0                0       0      0     0   \n",
       "3            0                     0                0       0      0     0   \n",
       "4            0                     0                1       0      1     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "1           0     0              0              0  \n",
       "2           0     0              0              0  \n",
       "3           0     0              0              0  \n",
       "4           0     0              0              0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input (X) and output (y) samples, we know that related is always one ...\n",
    "# as input we have to take care about the messages\n",
    "# the categories are the targets of the multi-class, multi-label classification\n",
    "X = df['message']\n",
    "y = df[df.columns[4:]]\n",
    "TARGET_NAMES = y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X datatype: <class 'pandas.core.series.Series'>\n",
      "y datatype: <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(\"X datatype: {}\".format(type(X)))\n",
    "print(\"y datatype: {}\".format(type(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Weather update - a cold front from Cuba that c...\n",
       "1              Is the Hurricane over or is it not over\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>security</th>\n",
       "      <th>military</th>\n",
       "      <th>child_alone</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   related  request  offer  aid_related  medical_help  medical_products  \\\n",
       "0        1        0      0            0             0                 0   \n",
       "1        1        0      0            1             0                 0   \n",
       "2        1        1      0            1             0                 1   \n",
       "3        1        0      0            0             0                 0   \n",
       "4        1        0      0            0             0                 0   \n",
       "\n",
       "   search_and_rescue  security  military  child_alone  ...  aid_centers  \\\n",
       "0                  0         0         0            0  ...            0   \n",
       "1                  0         0         0            0  ...            0   \n",
       "2                  0         0         0            0  ...            0   \n",
       "3                  0         0         0            0  ...            0   \n",
       "4                  0         0         0            0  ...            0   \n",
       "\n",
       "   other_infrastructure  weather_related  floods  storm  fire  earthquake  \\\n",
       "0                     0                0       0      0     0           0   \n",
       "1                     0                1       0      1     0           0   \n",
       "2                     0                0       0      0     0           0   \n",
       "3                     0                0       0      0     0           0   \n",
       "4                     0                1       0      1     0           0   \n",
       "\n",
       "   cold  other_weather  direct_report  \n",
       "0     0              0              0  \n",
       "1     0              0              0  \n",
       "2     0              0              0  \n",
       "3     0              0              0  \n",
       "4     0              0              0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.iloc[0:5,:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'related' includes 19634 x value 1.\n",
      "'request' includes 4374 x value 1.\n",
      "'offer' includes 117 x value 1.\n",
      "'aid_related' includes 10729 x value 1.\n",
      "'medical_help' includes 2066 x value 1.\n",
      "'medical_products' includes 1297 x value 1.\n",
      "'search_and_rescue' includes 718 x value 1.\n",
      "'security' includes 467 x value 1.\n",
      "'military' includes 857 x value 1.\n",
      "'child_alone' includes 0 x value 1.\n",
      "'water' includes 1650 x value 1.\n",
      "'food' includes 2885 x value 1.\n",
      "'shelter' includes 2281 x value 1.\n",
      "'clothing' includes 401 x value 1.\n",
      "'money' includes 598 x value 1.\n",
      "'missing_people' includes 297 x value 1.\n",
      "'refugees' includes 872 x value 1.\n",
      "'death' includes 1187 x value 1.\n",
      "'other_aid' includes 3392 x value 1.\n",
      "'infrastructure_related' includes 1688 x value 1.\n",
      "'transport' includes 1197 x value 1.\n",
      "'buildings' includes 1313 x value 1.\n",
      "'electricity' includes 528 x value 1.\n",
      "'tools' includes 158 x value 1.\n",
      "'hospitals' includes 283 x value 1.\n",
      "'shops' includes 118 x value 1.\n",
      "'aid_centers' includes 308 x value 1.\n",
      "'other_infrastructure' includes 1136 x value 1.\n",
      "'weather_related' includes 7212 x value 1.\n",
      "'floods' includes 2130 x value 1.\n",
      "'storm' includes 2420 x value 1.\n",
      "'fire' includes 282 x value 1.\n",
      "'earthquake' includes 2422 x value 1.\n",
      "'cold' includes 528 x value 1.\n",
      "'other_weather' includes 1366 x value 1.\n",
      "'direct_report' includes 4965 x value 1.\n"
     ]
    }
   ],
   "source": [
    "for group in y.columns:\n",
    "    print(\"'{}' includes {} x value 1.\".format(group, y[group].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data\n",
    "\n",
    "During EPL pipeline activities we realised that there are messages which are not useful (e.g. 'nonsense' character sequences, html characters) and there are probably web links included. We have to deal with this in the tokenize() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function from Dipanjan's repository:\n",
    "# https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%\\\n",
    "# 20content/nlp%20proven%20approach/NLP%20Strategy%20I%20-%20Processing%20and%20Understanding%20Text.ipynb\n",
    "\n",
    "def expand_contractions(text, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    \n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "#stop_words.remove('no')\n",
    "#stop_words.remove('not')\n",
    "\n",
    "def tokenize(text):\n",
    "    # have in mind that we use this for a web app adding new messages;\n",
    "    # if still html, xml or other undefined parts in the existing messages:\n",
    "    # first remove such metatext from English messages\n",
    "    # see: https://docs.python.org/3.7/library/codecs.html#encodings-and-unicode\n",
    "    # \"To be able to detect the endianness of a UTF-16 or UTF-32 byte sequence,\n",
    "    # there’s the so called BOM (“Byte Order Mark”). [...]\n",
    "    # In UTF-8, the use of the BOM is discouraged and should generally be avoided.\"\n",
    "    # specific ones are e.g. notepad signatures from Microsoft as part of the messages which should be avoided;\n",
    "    # other undefined characters have the coding of the 'replacement character' unicode u\"\\ufffd\"\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    \n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'       \n",
    "    detected_urls = re.findall(url_regex, bom_removed)\n",
    "    for url in detected_urls:\n",
    "        text = bom_removed.replace(url, \"urlplaceholder\")\n",
    "        \n",
    "    # change the negation wordings like don't to do not, won't to will not \n",
    "    # or other contractions like I'd to I would, I'll to I will etc. via dictionary\n",
    "    text = expand_contractions(text, CONTRACTION_MAP)\n",
    "\n",
    "    # remove punctuation [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]\n",
    "    text = text.translate(str.maketrans('','', string.punctuation))\n",
    "    # remove numbers\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # during ETL pipeline we have reduced the dataset on English messages ('en' language coding,\n",
    "    # but there can be some wrong codings\n",
    "    tokens = word_tokenize(letters_only, language='english')\n",
    "    lemmatizer = WordNetLemmatizer()  # for the lexical correctly found word stem (root)\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        # use only lower cases, remove leading and ending spaces\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        # remember: there have been nonsense sentences, so, now some strings could be empty\n",
    "        # toDo: what is the correct length number to use now? Small ones are probably no relevant words ...\n",
    "        # remove English stop words\n",
    "        if (len(clean_tok) > 1) & (clean_tok not in stop_words):\n",
    "            clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of string with some punctuation signs\n"
     ]
    }
   ],
   "source": [
    "# example for unit test to remove punctuation [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]\n",
    "example_str = 'This [is an] example? {of} string. with.? some &punctuation &signs!!??!!'\n",
    "result = example_str.translate(str.maketrans('','', string.punctuation))\n",
    "print(result)\n",
    "# output shall be: This is an example of string with some punctuation signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather update - a cold front from Cuba that could pass over Haiti\n",
      "['weather', 'update', 'cold', 'front', 'cuba', 'could', 'pas', 'haiti'] \n",
      "\n",
      "Is the Hurricane over or is it not over\n",
      "['hurricane'] \n",
      "\n",
      "UN reports Leogane 80-90 destroyed. Only Hospital St. Croix functioning. Needs supplies desperately.\n",
      "['un', 'report', 'leogane', 'destroyed', 'hospital', 'st', 'croix', 'functioning', 'needs', 'supply', 'desperately'] \n",
      "\n",
      "says: west side of Haiti, rest of the country today and tonight\n",
      "['say', 'west', 'side', 'haiti', 'rest', 'country', 'today', 'tonight'] \n",
      "\n",
      "Storm at sacred heart of jesus\n",
      "['storm', 'sacred', 'heart', 'jesus'] \n",
      "\n",
      "Please, we need tents and water. We are in Silo, Thank you!\n",
      "['please', 'need', 'tent', 'water', 'silo', 'thank'] \n",
      "\n",
      "I am in Croix-des-Bouquets. We have health issues. They ( workers ) are in Santo 15. ( an area in Croix-des-Bouquets )\n",
      "['croixdesbouquets', 'health', 'issue', 'worker', 'santo', 'area', 'croixdesbouquets'] \n",
      "\n",
      "There's nothing to eat and water, we starving and thirsty.\n",
      "['nothing', 'eat', 'water', 'starving', 'thirsty'] \n",
      "\n",
      "I am in Thomassin number 32, in the area named Pyron. I would like to have some water. Thank God we are fine, but we desperately need water. Thanks\n",
      "['thomassin', 'number', 'area', 'named', 'pyron', 'would', 'like', 'water', 'thank', 'god', 'fine', 'desperately', 'need', 'water', 'thanks'] \n",
      "\n",
      "Let's do it together, need food in Delma 75, in didine area\n",
      "['let', 'together', 'need', 'food', 'delma', 'didine', 'area'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test tokenize\n",
    "for message in X[:10]:\n",
    "    tokens = tokenize(message)\n",
    "    print(message)\n",
    "    print(tokens, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "Notes:\n",
    "- Regarding the class default parameters, for this Python implementation scikit-learn version 0.21.2 is used.\n",
    "- We use np.random.seed() too beside of random_state/random_seed parameters ([reason](https://stackoverflow.com/questions/47923258/random-seed-on-svm-sklearn-produces-different-results))\n",
    "- For the pipeline workflow a `FeatureUnion`instance concatenates results of multiple transformer objects\n",
    "\n",
    "Remember, we are dealing with an imbalanced dataset, therefore not all models can be used. A machine learning classifier could be more biased towards the majority class, causing bad classification of the minority class. Therefore we have to take care.\n",
    "\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other remaining 31 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables.\n",
    "\n",
    "According scikit-learn [documentation](https://scikit-learn.org/stable/modules/multiclass.html) we can choose only specific classifier using this meta-estimator. We start with `RandomForestClassier`.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([ \n",
    "            \n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize, ngram_range=(1,2))),\n",
    "                ('tfidf', TfidfTransformer(sublinear_tf=True)),\n",
    "            ]))\n",
    "            \n",
    "        ])),\n",
    "    \n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier(n_estimators=100, class_weight='balanced',\n",
    "                                                             n_jobs=-1, random_state=FIXED_SEED)))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle is by default set on True,\n",
    "# usage of stratify param leads to stratify split technique for this imbalanced dataset,\n",
    "# having both would be a StratifiedShuffleSplit algorithm in the background,\n",
    "# but\n",
    "# stratify=y leads to a ValueError: The least populated class in y has only 1 member, which is too few.\n",
    "# The minimum number of groups for any class cannot be less than 2.\n",
    "# ToDo: clarify why => must be: stratify=y.iloc[:,1]\n",
    "# Therefore I added randomised resampling to reduce the probability of getting ValueError's during model fit()\n",
    "# and after clarification comment them out\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, stratify=y.iloc[:,1],\n",
    "                                                    test_size=0.2, random_state=FIXED_SEED)\n",
    "#X_train = X_train.sample(n = X_train.shape[0], axis=0, random_state=FIXED_SEED) \n",
    "#y_train = y_train.sample(n = y_train.shape[0], axis=0, random_state=FIXED_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15707,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15707, 36)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train datatype: <class 'numpy.ndarray'>\n",
      "y_train datatype: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train datatype: {}\".format(type(X_train)))\n",
    "print(\"y_train datatype: {}\".format(type(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. numpy.ndarray element is: [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1]\n",
      "{0, 1}\n",
      "1. numpy.ndarray element is: [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0]\n",
      "{0, 1}\n",
      "2. numpy.ndarray element is: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "3. numpy.ndarray element is: [1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "4. numpy.ndarray element is: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "5. numpy.ndarray element is: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "6. numpy.ndarray element is: [1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "7. numpy.ndarray element is: [1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "8. numpy.ndarray element is: [1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "9. numpy.ndarray element is: [1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "10. numpy.ndarray element is: [1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "11. numpy.ndarray element is: [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "12. numpy.ndarray element is: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0]\n",
      "{0, 1}\n",
      "13. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "14. numpy.ndarray element is: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "15. numpy.ndarray element is: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "16. numpy.ndarray element is: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "17. numpy.ndarray element is: [1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "18. numpy.ndarray element is: [1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "19. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "20. numpy.ndarray element is: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0]\n",
      "{0, 1}\n",
      "21. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "22. numpy.ndarray element is: [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0]\n",
      "{0, 1}\n",
      "23. numpy.ndarray element is: [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "24. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "25. numpy.ndarray element is: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0]\n",
      "{0, 1}\n",
      "26. numpy.ndarray element is: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0]\n",
      "{0, 1}\n",
      "27. numpy.ndarray element is: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "28. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "29. numpy.ndarray element is: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "30. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "31. numpy.ndarray element is: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0]\n",
      "{0, 1}\n",
      "32. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "33. numpy.ndarray element is: [1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "{0, 1}\n",
      "34. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n",
      "35. numpy.ndarray element is: [1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "for i in range(y_train.shape[1]):\n",
    "    print(\"{}. numpy.ndarray element is: {}\".format(i, y_train[i]))\n",
    "    print(set(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train the pipeline ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('features',\n",
       "                 FeatureUnion(n_jobs=None,\n",
       "                              transformer_list=[('text_pipeline',\n",
       "                                                 Pipeline(memory=None,\n",
       "                                                          steps=[('vect',\n",
       "                                                                  CountVectorizer(analyzer='word',\n",
       "                                                                                  binary=False,\n",
       "                                                                                  decode_error='strict',\n",
       "                                                                                  dtype=<class 'numpy.int64'>,\n",
       "                                                                                  encoding='utf-8',\n",
       "                                                                                  input='content',\n",
       "                                                                                  lowercase=True,\n",
       "                                                                                  max_df=1.0,\n",
       "                                                                                  max_features=None,\n",
       "                                                                                  min_df=1,\n",
       "                                                                                  ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  preprocessor=Non...\n",
       "                 MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                                        class_weight='balanced',\n",
       "                                                                        criterion='gini',\n",
       "                                                                        max_depth=None,\n",
       "                                                                        max_features='auto',\n",
       "                                                                        max_leaf_nodes=None,\n",
       "                                                                        min_impurity_decrease=0.0,\n",
       "                                                                        min_impurity_split=None,\n",
       "                                                                        min_samples_leaf=1,\n",
       "                                                                        min_samples_split=2,\n",
       "                                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                                        n_estimators=100,\n",
       "                                                                        n_jobs=-1,\n",
       "                                                                        oob_score=False,\n",
       "                                                                        random_state=42,\n",
       "                                                                        verbose=0,\n",
       "                                                                        warm_start=False),\n",
       "                                       n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And calculate the model prediction ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rfc_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "For evaluation:<br>\n",
    "Report accuracy score, f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each, where:\n",
    "\n",
    "TP = TruePositive; FP = FalsePositive; TN = TrueNegative; FN = FalseNegative.\n",
    "\n",
    "**Accuracy Score** is a classification score. It is the number of correct predictions made divided by the total number of predictions made. In a multilabel classification task it computes subset accuracy. \n",
    "  \n",
    "Furthermore, beside accuracy, we add additional metrics to compare the model performance having an originally imbalanced dataset. Accuracy would focus too much on the majority classes. Because of this overfitting of the majority classes, its value would be too good and therefore misleading.\n",
    "\n",
    "**Precision** quantifies the binary precision. In other words, a measure of a classifiers exactness. It is a ratio of true positives (messages correctly classified to their categories)) to all positives (all messages classified to categories, irrespective of whether that was the correct classification), in other words it is the ratio of\n",
    "\n",
    "TP / (TP + FP)\n",
    "\n",
    "**Recall** tells us what proportion of messages that actually were classified to specific categories were classified by us as this categories. Means, a measure of a classifiers completeness. It is a ratio of true positives to all the correctly category classified messages that were actually disaster messages, in other words it is the ratio of\n",
    "\n",
    "TP / (TP + FN)\n",
    "\n",
    "A model's ability to precisely predict those that are correctly categoriesed disaster messages is more important than the model's ability to recall those individuals. \n",
    "\n",
    "We can use **F-beta score** as a metric that considers both precision and recall. According scikit-learn, the F-beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0. F – Measure is nothing but the harmonic mean of Precision and Recall.\n",
    "\n",
    "Fβ=(1 + β2)  (precision⋅recall / ((β2⋅precision) + recall))\n",
    "\n",
    "In particular, when β=0.5, more emphasis is placed on precision. And when β=1.0 recall and precision are equally important.\n",
    "\n",
    "According scikit-learn: \"The **F1 score** ... reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "In the multi-class and multi-label case, this is the average of the F1 score of each class with weighting depending on the average parameter.\"\n",
    "\n",
    "From scikit-learn documentation for the classification report:<br>\n",
    "The classification_report() function returns an additional value: **Support** - the number of occurrences of each label in y_true.<br>\n",
    "The reported averages include macro average (averaging the unweighted mean per label), weighted average (averaging the support-weighted mean per label), sample average (only for multilabel classification) and micro average (averaging the total true positives, false negatives and false positives) it is only shown for multi-label or multi-class with a subset of classes because it is accuracy otherwise.\n",
    "\n",
    "Note: Having the imbalanced dataset in mind, Cohen's Kappa and Confusion Matrix are not possible because this is a multi-label classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(target_names, y_test, y_pred, cv=None):\n",
    "    # text summary of the overall accuracy, precision, recall, F1 score for each class   \n",
    "    print(\"First: overall accuracy score: {:5f}\".format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "    # shows F1_score, precision and recall\n",
    "    class_report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "    print(\"Classification Report for each target class:\\n\", class_report)\n",
    "\n",
    "    if cv != None:\n",
    "        print(\"\\n\\n---- Best Parameters: ----\\n{}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First: overall accuracy score: 0.070283\n",
      "Classification Report for each target class:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "               related       1.00      1.00      1.00      3927\n",
      "               request       0.56      0.39      0.46       875\n",
      "                 offer       0.00      0.00      0.00        29\n",
      "           aid_related       0.58      0.70      0.63      2146\n",
      "          medical_help       0.14      0.00      0.01       386\n",
      "      medical_products       0.18      0.01      0.02       253\n",
      "     search_and_rescue       0.20      0.01      0.01       142\n",
      "              security       0.00      0.00      0.00       103\n",
      "              military       0.00      0.00      0.00       179\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.10      0.00      0.01       332\n",
      "                  food       0.21      0.01      0.01       553\n",
      "               shelter       0.06      0.00      0.00       456\n",
      "              clothing       0.25      0.01      0.02        85\n",
      "                 money       0.00      0.00      0.00       122\n",
      "        missing_people       0.00      0.00      0.00        68\n",
      "              refugees       0.00      0.00      0.00       174\n",
      "                 death       0.00      0.00      0.00       233\n",
      "             other_aid       0.24      0.01      0.02       696\n",
      "infrastructure_related       0.00      0.00      0.00       341\n",
      "             transport       0.00      0.00      0.00       222\n",
      "             buildings       0.00      0.00      0.00       259\n",
      "           electricity       0.00      0.00      0.00        98\n",
      "                 tools       0.00      0.00      0.00        43\n",
      "             hospitals       0.00      0.00      0.00        55\n",
      "                 shops       0.00      0.00      0.00        26\n",
      "           aid_centers       0.00      0.00      0.00        60\n",
      "  other_infrastructure       0.00      0.00      0.00       228\n",
      "       weather_related       0.59      0.30      0.40      1425\n",
      "                floods       0.20      0.00      0.00       430\n",
      "                 storm       0.49      0.04      0.07       493\n",
      "                  fire       0.00      0.00      0.00        66\n",
      "            earthquake       0.66      0.16      0.26       483\n",
      "                  cold       0.09      0.01      0.02        99\n",
      "         other_weather       0.22      0.01      0.02       247\n",
      "         direct_report       0.53      0.34      0.42       986\n",
      "\n",
      "             micro avg       0.75      0.41      0.53     16320\n",
      "             macro avg       0.18      0.08      0.09     16320\n",
      "          weighted avg       0.50      0.41      0.42     16320\n",
      "           samples avg       0.80      0.57      0.57     16320\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\anaconda\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "display_results(TARGET_NAMES, y_test, y_rfc_pred, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such kind of behaviour has been expected because having an imbalanced dataset and in the output vectors for each message, most of the target label values are set to 0 - only few are set to 1.<br>\n",
    "The accuracy metric is not an appropriate measure to evaluate model performance of such kind of dataset. It could classify all instances as part of the majority class and classifies the minority class targets as noise. It is not able to evaluate the model performance of a multi-class dataset with multi-output vectors.<br>\n",
    "Additionally in this classification report, often the metrics are not reliable because of being set to 0.0 according calculation rules. If values are available, precision is in general higher than recall, in other words, we have a high rate of false negatives (all items wrongly classified as not being part of the specific target class). Therefore we start to improve the model by using cross-validated hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None, 'steps': [('features', FeatureUnion(n_jobs=None,\n",
       "                transformer_list=[('text_pipeline',\n",
       "                                   Pipeline(memory=None,\n",
       "                                            steps=[('vect',\n",
       "                                                    CountVectorizer(analyzer='word',\n",
       "                                                                    binary=False,\n",
       "                                                                    decode_error='strict',\n",
       "                                                                    dtype=<class 'numpy.int64'>,\n",
       "                                                                    encoding='utf-8',\n",
       "                                                                    input='content',\n",
       "                                                                    lowercase=True,\n",
       "                                                                    max_df=1.0,\n",
       "                                                                    max_features=None,\n",
       "                                                                    min_df=1,\n",
       "                                                                    ngram_range=(1,\n",
       "                                                                                 2),\n",
       "                                                                    preprocessor=None,\n",
       "                                                                    stop_words=None,\n",
       "                                                                    strip_accents=None,\n",
       "                                                                    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                                    tokenizer=<function tokenize at 0x000000DC22FCE1E0>,\n",
       "                                                                    vocabulary=None)),\n",
       "                                                   ('tfidf',\n",
       "                                                    TfidfTransformer(norm='l2',\n",
       "                                                                     smooth_idf=True,\n",
       "                                                                     sublinear_tf=True,\n",
       "                                                                     use_idf=True))],\n",
       "                                            verbose=False))],\n",
       "                transformer_weights=None, verbose=False)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                          class_weight='balanced',\n",
       "                                                          criterion='gini',\n",
       "                                                          max_depth=None,\n",
       "                                                          max_features='auto',\n",
       "                                                          max_leaf_nodes=None,\n",
       "                                                          min_impurity_decrease=0.0,\n",
       "                                                          min_impurity_split=None,\n",
       "                                                          min_samples_leaf=1,\n",
       "                                                          min_samples_split=2,\n",
       "                                                          min_weight_fraction_leaf=0.0,\n",
       "                                                          n_estimators=100,\n",
       "                                                          n_jobs=-1,\n",
       "                                                          oob_score=False,\n",
       "                                                          random_state=42,\n",
       "                                                          verbose=0,\n",
       "                                                          warm_start=False),\n",
       "                         n_jobs=None))], 'verbose': False, 'features': FeatureUnion(n_jobs=None,\n",
       "              transformer_list=[('text_pipeline',\n",
       "                                 Pipeline(memory=None,\n",
       "                                          steps=[('vect',\n",
       "                                                  CountVectorizer(analyzer='word',\n",
       "                                                                  binary=False,\n",
       "                                                                  decode_error='strict',\n",
       "                                                                  dtype=<class 'numpy.int64'>,\n",
       "                                                                  encoding='utf-8',\n",
       "                                                                  input='content',\n",
       "                                                                  lowercase=True,\n",
       "                                                                  max_df=1.0,\n",
       "                                                                  max_features=None,\n",
       "                                                                  min_df=1,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               2),\n",
       "                                                                  preprocessor=None,\n",
       "                                                                  stop_words=None,\n",
       "                                                                  strip_accents=None,\n",
       "                                                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                                  tokenizer=<function tokenize at 0x000000DC22FCE1E0>,\n",
       "                                                                  vocabulary=None)),\n",
       "                                                 ('tfidf',\n",
       "                                                  TfidfTransformer(norm='l2',\n",
       "                                                                   smooth_idf=True,\n",
       "                                                                   sublinear_tf=True,\n",
       "                                                                   use_idf=True))],\n",
       "                                          verbose=False))],\n",
       "              transformer_weights=None, verbose=False), 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                        class_weight='balanced',\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=None,\n",
       "                                                        max_features='auto',\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        n_estimators=100,\n",
       "                                                        n_jobs=-1,\n",
       "                                                        oob_score=False,\n",
       "                                                        random_state=42,\n",
       "                                                        verbose=0,\n",
       "                                                        warm_start=False),\n",
       "                       n_jobs=None), 'features__n_jobs': None, 'features__transformer_list': [('text_pipeline',\n",
       "   Pipeline(memory=None,\n",
       "            steps=[('vect',\n",
       "                    CountVectorizer(analyzer='word', binary=False,\n",
       "                                    decode_error='strict',\n",
       "                                    dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                    input='content', lowercase=True, max_df=1.0,\n",
       "                                    max_features=None, min_df=1,\n",
       "                                    ngram_range=(1, 2), preprocessor=None,\n",
       "                                    stop_words=None, strip_accents=None,\n",
       "                                    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                    tokenizer=<function tokenize at 0x000000DC22FCE1E0>,\n",
       "                                    vocabulary=None)),\n",
       "                   ('tfidf',\n",
       "                    TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=True,\n",
       "                                     use_idf=True))],\n",
       "            verbose=False))], 'features__transformer_weights': None, 'features__verbose': False, 'features__text_pipeline': Pipeline(memory=None,\n",
       "          steps=[('vect',\n",
       "                  CountVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                  input='content', lowercase=True, max_df=1.0,\n",
       "                                  max_features=None, min_df=1,\n",
       "                                  ngram_range=(1, 2), preprocessor=None,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=<function tokenize at 0x000000DC22FCE1E0>,\n",
       "                                  vocabulary=None)),\n",
       "                 ('tfidf',\n",
       "                  TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=True,\n",
       "                                   use_idf=True))],\n",
       "          verbose=False), 'features__text_pipeline__memory': None, 'features__text_pipeline__steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function tokenize at 0x000000DC22FCE1E0>,\n",
       "                   vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=True, use_idf=True))], 'features__text_pipeline__verbose': False, 'features__text_pipeline__vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function tokenize at 0x000000DC22FCE1E0>,\n",
       "                 vocabulary=None), 'features__text_pipeline__tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=True, use_idf=True), 'features__text_pipeline__vect__analyzer': 'word', 'features__text_pipeline__vect__binary': False, 'features__text_pipeline__vect__decode_error': 'strict', 'features__text_pipeline__vect__dtype': numpy.int64, 'features__text_pipeline__vect__encoding': 'utf-8', 'features__text_pipeline__vect__input': 'content', 'features__text_pipeline__vect__lowercase': True, 'features__text_pipeline__vect__max_df': 1.0, 'features__text_pipeline__vect__max_features': None, 'features__text_pipeline__vect__min_df': 1, 'features__text_pipeline__vect__ngram_range': (1,\n",
       "  2), 'features__text_pipeline__vect__preprocessor': None, 'features__text_pipeline__vect__stop_words': None, 'features__text_pipeline__vect__strip_accents': None, 'features__text_pipeline__vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'features__text_pipeline__vect__tokenizer': <function __main__.tokenize(text)>, 'features__text_pipeline__vect__vocabulary': None, 'features__text_pipeline__tfidf__norm': 'l2', 'features__text_pipeline__tfidf__smooth_idf': True, 'features__text_pipeline__tfidf__sublinear_tf': True, 'features__text_pipeline__tfidf__use_idf': True, 'clf__estimator__bootstrap': True, 'clf__estimator__class_weight': 'balanced', 'clf__estimator__criterion': 'gini', 'clf__estimator__max_depth': None, 'clf__estimator__max_features': 'auto', 'clf__estimator__max_leaf_nodes': None, 'clf__estimator__min_impurity_decrease': 0.0, 'clf__estimator__min_impurity_split': None, 'clf__estimator__min_samples_leaf': 1, 'clf__estimator__min_samples_split': 2, 'clf__estimator__min_weight_fraction_leaf': 0.0, 'clf__estimator__n_estimators': 100, 'clf__estimator__n_jobs': -1, 'clf__estimator__oob_score': False, 'clf__estimator__random_state': 42, 'clf__estimator__verbose': 0, 'clf__estimator__warm_start': False, 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                        min_impurity_split=None, min_samples_leaf=1,\n",
       "                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                        n_estimators=100, n_jobs=-1, oob_score=False,\n",
       "                        random_state=42, verbose=0, warm_start=False), 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters for grid search\n",
    "rfc_param_grid = {\n",
    "    'features__text_pipeline__vect__ngram_range': [(1, 2), (1,3)],\n",
    "    'clf__estimator__n_estimators': [10, 100, 500, 1000],\n",
    "    'clf__estimator__max_depth': [None, 5, 10],\n",
    "    'clf__estimator__class_weight': ['balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "# create grid search object\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV\n",
    "# cv not higher than 5 buckets, training needs days with cv=10 if e.g. amazon AWS EC2 service is not available\n",
    "grid_cv = GridSearchCV(pipeline, param_grid=rfc_param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, recall and F-score of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "# model = cv\n",
    "grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rfc_pred2 = grid_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation results for the cross validation tuned 'RandomForestClassifier' estimator:\")\n",
    "display_results(y_test, y_rfc_pred2, grid_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuned evaluation result is ......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First**, we try out other machine learning algorithms which are tuned by cross validation to compare their prediction results. Other estimator models for the requested `MultiOutputClassifier` are:\n",
    "- `KNeighborsClassifier` with its default parameters: (n_neighbors=5, weights=’uniform’, algorithm=’auto’, leaf_size=30, p=2, metric=’minkowski’, metric_params=None, n_jobs=None, **kwargs)\n",
    "- `RadiusNeighborsClassifier` with its default parameters: (radius=1.0, weights=’uniform’, algorithm=’auto’, leaf_size=30, p=2, metric=’minkowski’, outlier_label=None, metric_params=None, n_jobs=None, **kwargs)\n",
    "\n",
    "As stated in the scikit-learn [documentation](https://scikit-learn.org/stable/modules/neighbors.html#classification) \"scikit-learn implements two different nearest neighbors classifiers: <i>KNeighborsClassifier</i> implements learning based on the nearest neighbors of each query point, where is an integer value specified by the user.<br> <i>RadiusNeighborsClassifier</i> implements learning based on the number of neighbors within a fixed radius of each training point, where is a floating-point value specified by the user.\"\n",
    "    \n",
    "**Second**, because it is an imbalanced dataset we could do a balancing before classification. The categority classes with low numbers of observations are outnumbered. So, the dataset is highly skewed. To create a balanced dataset several strategies exists:\n",
    "- Undersampling the majority classes\n",
    "- Oversampling the minority classes\n",
    "- Combining over- and under-sampling\n",
    "- Create ensemble balanced sets\n",
    "\n",
    "But have in mind, that minority class oversampling could result in overfitting problems doing it before cross-validation. We would link the information of validation data to our training dataset which is forbidden.\n",
    "\n",
    "Note:<br>\n",
    "Doing balancing activities the specific scikit package 'imbalanced-learn' is imported.<br>\n",
    "For combining the strategies we implement a naive random oversampling of the minority classes.<br>\n",
    "For undersampling the package can be used as well to create the pipeline with `PipelineImb`. The pipeline itself includes the class `RandomUnderSampler()` directly before the MultiOutputClassifier to equalize the number of samples in all the classes before the training.\n",
    "\n",
    "Using such package throws the following ValueError: 'Imbalanced-learn currently supports binary, multiclass and binarized encoded multiclasss targets. Multilabel and multioutput targets are not supported.' So, the associated package classes do not support the multi-target classification with multiple outputs as we need for our project. Therefore this coding is removed after such experiment.\n",
    "\n",
    "According the [paper](https://arxiv.org/ftp/arxiv/papers/1810/1810.11612.pdf) <i>Handling Imbalanced Dataset in Multi-label Text Categorization using Bagging and Adaptive Boosting</i> of 27 October 2018 from Genta Indra Winata and Masayu Leylia Khodra, regarding new data, it is more appropriate to balance the dataset on the algorithm level instead of the data level to avoid overfitting. The algorithm \"approach modifies algorithm by adjusting weight or cost of various classes.\"\n",
    "\n",
    "If the usage of the mentioned specific library is not possible for our task, what could we do instead?<br>\n",
    "Another option is a `feature-selection` approach which can be done after the feature extraction of the `TfidfTransformer`, which is creating [feature vectors](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer).\n",
    "\n",
    "Scikit-learn offers the package [feature selection](): \"The classes in the sklearn.feature_selection module can be used for feature selection/dimensionality reduction on sample sets, either to improve estimators’ accuracy scores or to boost their performance on very high-dimensional datasets.\"<br>\n",
    "So, first we have to select the important features to get the meaningful ones and afterwards we can do the prediction with the found highest scoring features (say n ones). We could add this directly in our pipeline by using `SelectFromModel` as proposed by scikit-learn.\n",
    "\n",
    "There are several links for information:<br>\n",
    "- https://scikit-learn.org/stable/modules/feature_selection.html#tree-based-feature-selection\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n",
    "- https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection-as-part-of-a-pipeline\n",
    "- https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_type, params):\n",
    "    ''' \n",
    "    input:\n",
    "    model_type - the estimator model used for the MultiOutputClassifier\n",
    "    params - the estimator model parameter grid used for the GridSearchCV \n",
    "    ''' \n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([ \n",
    "            \n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer(sublinear_tf=True)),\n",
    "            ]))           \n",
    "        ])),\n",
    "\n",
    "        ('feature_selection', SelectFromModel(LinearSVC(multi_class=‘crammer_singer’))),\n",
    "        ('clf', MultiOutputClassifier(model_type))\n",
    "    ])\n",
    "    \n",
    "    # the higher the verbose number the more information is thrown\n",
    "    cv = GridSearchCV(pipeline, param_grid=params, n_jobs=-1, cv=5, verbose=1) \n",
    "    \n",
    "    return cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we try this new pipeline again with the <i>RandomForestClassifier</i> and a reduced parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create param grids for the models\n",
    "rfc_reduced_param_grid = {\n",
    "    'features__text_pipeline__vect__ngram_range': [(1, 2), (1,3)],\n",
    "    'clf__estimator__n_estimators': [10, 100, 500, 1000],\n",
    "    'clf__estimator__max_depth': [None, 5, 10],\n",
    "    'clf__estimator__class_weight': ['balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "knn_param_grid  = {\n",
    "    'features__text_pipeline__vect__ngram_range': [(1, 2), (1,3)],\n",
    "    'clf__estimator__weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "rn_param_grid  = {\n",
    "    'features__text_pipeline__vect__ngram_range': [(1, 2), (1,3)],\n",
    "    'clf__estimator__weights': ['uniform', 'distance']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n----- RandomForestClassifier with SelectFromModel -----\")\n",
    "print(\"Build best model: ...\")\n",
    "cv_rfc_reduced_model = build_model(RandomForestClassifier(random_state=FIXED_SEED), rfc_reduced_param_grid)\n",
    "print(\"Train model: ...\")\n",
    "cv_rfc_reduced_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rfc_reduced_pred = cv_rfc_reduced_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nModel evaluation on second tuned RandomForestClassifier ...\")\n",
    "display_results_imbalanced(y_test, y_rfc_reduced_pred, cv_rfc_reduced_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n----- KNeighborsClassifier with SelectFromModel -----\")\n",
    "print(\"Build best model: ...\")\n",
    "cv_knn_model = build_model(KNeighborsClassifier(random_state=FIXED_SEED), knn_param_grid)\n",
    "print(\"Train model: ...\")\n",
    "cv_knn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_knn_pred = cv_knn_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nModel evaluation on tuned KNeighborsClassifier ...\")\n",
    "display_results_imbalanced(y_test, y_knn_pred, cv_knn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n----- RadiusNeighborsClassifier with SelectFromModel -----\")\n",
    "print(\"Build best model: ...\")\n",
    "cv_rn_model = build_model(RadiusNeighborsClassifier(random_state=FIXED_SEED), rn_param_grid)\n",
    "print(\"Train model: ...\")\n",
    "cv_rn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rn_pred = cv_rn_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nModel evaluation on tuned RadiusNeighborsClassifier ...\")\n",
    "display_results_imbalanced(y_test, y_rn_pred, cv_rn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the evaluation results the best model ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, having found the best model from our model selection list, we save this model with its best parameters as a pickle file. Pickle is the standard way of serialising objects in Python. With this pickle file we can deserialise our model and use it to make new predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_filepath):\n",
    "    pickle.dump(model, open(model_filepath, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see train_classifier.py file\n",
    "model_filepath = \"classifier.p\"\n",
    "print('Saving model...\\n    MODEL: {}'.format(model_filepath))\n",
    "save_model(model, model_filepath)\n",
    "\n",
    "print('Best trained model saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train_classifier.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
